import os
import sys
import cv2
import numpy as np
import torch
import logging
from pathlib import Path
import shutil
from typing import Tuple, Optional
import warnings

# ONNX Runtime ê²½ê³  ë¬´ì‹œ (shape ì •ë³´ ë³‘í•© ê²½ê³ ëŠ” ê¸°ëŠ¥ì— ì˜í–¥ ì—†ìŒ)
warnings.filterwarnings('ignore', category=UserWarning, module='onnxruntime')
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(current_dir, 'src', 'backend'))

try:
    import config
    # coreì™€ fast_face_recognizerëŠ” importí•˜ì§€ ì•ŠìŒ (insightface ì˜ì¡´ì„± ë¬¸ì œ ë°©ì§€)
    from ultralytics import YOLO
    # onnxruntimeì€ ë‚˜ì¤‘ì— ì§ì ‘ import (conda í™˜ê²½ í˜¸í™˜ì„±)
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    print("í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

# ë¡œê¹… ì„¤ì • (ONNX Runtime ê²½ê³  í•„í„°ë§)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
# ONNX Runtime ë¡œê¹… ë ˆë²¨ ì¡°ì •
logging.getLogger('onnxruntime').setLevel(logging.ERROR)  # WARNING ì´ìƒë§Œ í‘œì‹œ

def main():
    print("="*60)
    print("ğŸ”„ ì–¼êµ´ ë°ì´í„°ë² ì´ìŠ¤ ì¬êµ¬ì¶• (YOLOv8-Face + AdaFace)")
    print("="*60)

    # 1. ëª¨ë¸ ë¡œë“œ
    print("1. ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # YOLO Face ëª¨ë¸ (ONNX ìš°ì„ )
    yolo_path = config.Paths.YOLO_FACE_MODEL
    onnx_path = os.path.splitext(yolo_path)[0] + ".onnx"
    
    yolo_face = None
    if os.path.exists(onnx_path):
        try:
            print(f"   - YOLO Face ONNX ëª¨ë¸ ë¡œë”©: {onnx_path}")
            # ONNX ëª¨ë¸ì€ task="pose"ë¡œ ë³€í™˜ë˜ì–´ ëœë“œë§ˆí¬ ì œê³µ
            yolo_face = YOLO(onnx_path, task="pose")
            print(f"   âœ… YOLO Face ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (task=pose, ëœë“œë§ˆí¬ ì§€ì›)")
        except Exception as e:
            print(f"   âš ï¸ ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, PyTorchë¡œ ëŒ€ì²´: {e}")
            if os.path.exists(yolo_path):
                print(f"   - YOLO Face PyTorch ëª¨ë¸ ë¡œë”©: {yolo_path}")
                # PyTorch ëª¨ë¸ë„ pose êµ¬ì¡°ì´ë¯€ë¡œ task="pose" ì‚¬ìš©
                yolo_face = YOLO(yolo_path, task="pose")
                print(f"   âœ… YOLO Face PyTorch ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (task=pose, ëœë“œë§ˆí¬ ì§€ì›)")
            else:
                print(f"âŒ YOLO Face ëª¨ë¸ ì—†ìŒ: {yolo_path}")
                return
    elif os.path.exists(yolo_path):
        print(f"   - YOLO Face PyTorch ëª¨ë¸ ë¡œë”©: {yolo_path}")
        # PyTorch ëª¨ë¸ë„ pose êµ¬ì¡°ì´ë¯€ë¡œ task="pose" ì‚¬ìš©
        yolo_face = YOLO(yolo_path, task="pose")
        print(f"   âœ… YOLO Face PyTorch ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (task=pose, ëœë“œë§ˆí¬ ì§€ì›)")
    else:
        print(f"âŒ YOLO Face ëª¨ë¸ ì—†ìŒ: {yolo_path}")
        return
    
    # AdaFace ëª¨ë¸ (onnxruntime ì§ì ‘ ì‚¬ìš©)
    adaface_path = config.Paths.ADAFACE_MODEL
    if not os.path.exists(adaface_path):
        print(f"âŒ AdaFace ëª¨ë¸ ì—†ìŒ: {adaface_path}")
        return

    # ONNX Runtime ì„¸ì…˜ ìƒì„± (insightface ì—†ì´ ì§ì ‘ ì‚¬ìš©)
    # onnxruntime ëª¨ë“ˆ í™•ì¸ ë° import (conda í™˜ê²½ í˜¸í™˜ì„±)
    try:
        # ì§ì ‘ onnxruntime import ì‹œë„
        import onnxruntime
        from onnxruntime import InferenceSession
        print(f"   âœ… onnxruntime ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ (ë²„ì „: {getattr(onnxruntime, '__version__', 'N/A')})")
    except ImportError as e:
        print(f"âŒ onnxruntime ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print(f"   ì¬ì„¤ì¹˜ ë°©ë²•:")
        print(f"   1. pip uninstall onnxruntime onnxruntime-gpu -y")
        print(f"   2. pip install onnxruntime-gpu")
        print(f"   ë˜ëŠ” conda í™˜ê²½ì¸ ê²½ìš°:")
        print(f"   conda install -c conda-forge onnxruntime-gpu")
        return
    except Exception as e:
        print(f"âŒ onnxruntime ëª¨ë“ˆ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return
    
    gpu_id = 0 if torch.cuda.is_available() else -1
    providers = []
    if gpu_id >= 0:
        try:
            providers.append(('CUDAExecutionProvider', {'device_id': gpu_id}))
        except:
            pass
    providers.append('CPUExecutionProvider')
    
    try:
        adaface_session = InferenceSession(adaface_path, providers=providers)
        adaface_input_name = adaface_session.get_inputs()[0].name
        adaface_output_name = adaface_session.get_outputs()[0].name
        print(f"   âœ… AdaFace ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (GPU: {gpu_id})")
    except Exception as e:
        print(f"âŒ AdaFace ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return

    # 2. FAISS ì´ˆê¸°í™”
    index_path = Path(config.Paths.FAISS_INDEX)
    labels_path = Path(config.Paths.FAISS_LABELS)
    
    if index_path.exists():
        backup_path = index_path.with_suffix('.faiss.bak')
        shutil.copy(index_path, backup_path)
        print(f"â„¹ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ ë°±ì—…ë¨")
    
    if labels_path.exists():
        backup_path = labels_path.with_suffix('.npy.bak')
        shutil.copy(labels_path, backup_path)
        print(f"â„¹ï¸ ê¸°ì¡´ ë ˆì´ë¸” ë°±ì—…ë¨")

    import faiss
    index = faiss.IndexFlatIP(512)
    labels = []
    
    # 3. ì´ë¯¸ì§€ ì²˜ë¦¬
    project_root = Path(config.BASE_DIR).parent.parent
    images_dir = project_root / 'face' / 'data' / 'images'
    
    if not images_dir.exists():
        images_dir = project_root / 'images'
        
    if not images_dir.exists():
        print(f"âŒ ì´ë¯¸ì§€ í´ë” ì—†ìŒ")
        return

    print(f"2. ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘ (í´ë”: {images_dir})")
    
    extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.JPG', '*.JPEG', '*.PNG']
    image_files = []
    for ext in extensions:
        image_files.extend(list(images_dir.rglob(ext)))
    
    print(f"â„¹ï¸ ì²˜ë¦¬ ëŒ€ìƒ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: {len(image_files)}")
    
    success_count = 0
    person_count = 0
    
    # ì–¼êµ´ ì •ë ¬ ë° ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜ (insightface ì—†ì´ ì§ì ‘ êµ¬í˜„)
    def align_face_simple(frame: np.ndarray, kps: np.ndarray) -> np.ndarray:
        """ê°„ë‹¨í•œ ì–¼êµ´ ì •ë ¬ (bbox ê¸°ë°˜)"""
        try:
            # ëœë“œë§ˆí¬ ì¤‘ì‹¬ê³¼ í¬ê¸° ê³„ì‚°
            center_x = np.mean(kps[:, 0])
            center_y = np.mean(kps[:, 1])
            
            # ì–¼êµ´ í¬ê¸° ì¶”ì • (ëˆˆ ì‚¬ì´ ê±°ë¦¬ ê¸°ë°˜)
            if len(kps) >= 2:
                eye_distance = np.linalg.norm(kps[1] - kps[0])
                face_size = int(eye_distance * 2.5)
            else:
                # ëœë“œë§ˆí¬ê°€ ë¶€ì¡±í•˜ë©´ bbox ê¸°ë°˜
                x_coords = kps[:, 0]
                y_coords = kps[:, 1]
                face_size = int((np.max(x_coords) - np.min(x_coords) + np.max(y_coords) - np.min(y_coords)) / 2)
            
            # í¬ë¡­ ì˜ì—­ ê³„ì‚°
            x1 = max(0, int(center_x - face_size // 2))
            y1 = max(0, int(center_y - face_size // 2))
            x2 = min(frame.shape[1], int(center_x + face_size // 2))
            y2 = min(frame.shape[0], int(center_y + face_size // 2))
            
            if x2 <= x1 or y2 <= y1:
                return None
            
            # ì–¼êµ´ í¬ë¡­
            face_crop = frame[y1:y2, x1:x2]
            if face_crop.size == 0:
                return None
            
            # 112x112ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (AdaFace ì…ë ¥ í¬ê¸°)
            aligned_face = cv2.resize(face_crop, (112, 112), interpolation=cv2.INTER_LANCZOS4)
            return aligned_face
        except Exception as e:
            return None
    
    def get_embedding_from_onnx(frame: np.ndarray, kps: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """ì–¼êµ´ ì •ë ¬ ë° ì„ë² ë”© ì¶”ì¶œ (onnxruntime ì§ì ‘ ì‚¬ìš©)"""
        try:
            # ì–¼êµ´ ì •ë ¬
            aligned_face = align_face_simple(frame, kps)
            if aligned_face is None or aligned_face.size == 0:
                return None, None
            
            # í™”ì§ˆ ê°œì„ : CLAHE
            try:
                lab = cv2.cvtColor(aligned_face, cv2.COLOR_BGR2LAB)
                l_channel, a, b = cv2.split(lab)
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                l_channel = clahe.apply(l_channel)
                aligned_face = cv2.merge([l_channel, a, b])
                aligned_face = cv2.cvtColor(aligned_face, cv2.COLOR_LAB2BGR)
            except:
                pass
            
            # AdaFace ì „ì²˜ë¦¬
            np_img = aligned_face.astype(np.float32) / 255.0
            np_img = (np_img - 0.5) / 0.5  # [-1, 1] ì •ê·œí™”
            tensor = np_img.transpose(2, 0, 1)[np.newaxis, :, :, :].astype(np.float32)
            
            # ONNX ì¶”ë¡ 
            outputs = adaface_session.run([adaface_output_name], {adaface_input_name: tensor})
            embedding = outputs[0]
            
            # Flatten
            if embedding.ndim > 1:
                embedding = embedding.flatten()
            
            # ì •ê·œí™” (L2 norm)
            norm_val = np.linalg.norm(embedding)
            if norm_val > 0:
                embedding = embedding / norm_val
            else:
                return None, None
            
            return embedding, aligned_face
        except Exception as e:
            return None, None
    
    # ì¦ê°• í•¨ìˆ˜ ì •ì˜ (CCTV í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜)
    def apply_gaussian_blur(img: np.ndarray, kernel_size: int = 3) -> np.ndarray:
        """ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ (CCTV ëª¨ì…˜ ë¸”ëŸ¬ ì‹œë®¬ë ˆì´ì…˜)"""
        if img is None or img.size == 0:
            return img
        try:
            return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)
        except Exception:
            return img
    
    def apply_downscale_upscale(img: np.ndarray, scale_factor: float = 0.5) -> np.ndarray:
        """ì €í•´ìƒë„ ë‹¤ìš´ìŠ¤ì¼€ì¼ í›„ ì—…ìŠ¤ì¼€ì¼ (CCTV ì €í•´ìƒë„ ì‹œë®¬ë ˆì´ì…˜)"""
        if img is None or img.size == 0:
            return img
        try:
            h, w = img.shape[:2]
            new_h, new_w = int(h * scale_factor), int(w * scale_factor)
            if new_h <= 0 or new_w <= 0: return img
            
            # ë‹¤ìš´ìŠ¤ì¼€ì¼
            downscaled = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
            # ì—…ìŠ¤ì¼€ì¼ (ì›ë˜ í¬ê¸°ë¡œ ë³µì›, í™”ì§ˆ ì†ì‹¤ ë°œìƒ)
            upscaled = cv2.resize(downscaled, (w, h), interpolation=cv2.INTER_LINEAR)
            return upscaled
        except Exception:
            return img
    
    def add_gaussian_noise(img: np.ndarray, mean: float = 0, std: float = 10) -> np.ndarray:
        """ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€ (CCTV ì••ì¶• ë…¸ì´ì¦ˆ ì‹œë®¬ë ˆì´ì…˜)"""
        if img is None or img.size == 0:
            return img
        try:
            noise = np.random.normal(mean, std, img.shape).astype(np.float32)
            noisy = img.astype(np.float32) + noise
            return np.clip(noisy, 0, 255).astype(np.uint8)
        except Exception:
            return img
    
    def adjust_brightness(img: np.ndarray, factor: float) -> np.ndarray:
        """ë°ê¸° ì¡°ì • (factor < 1: ì–´ë‘¡ê²Œ, CCTV ì–´ë‘ìš´ ì¡°ëª… ì‹œë®¬ë ˆì´ì…˜)"""
        if img is None or img.size == 0:
            return img
        try:
            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
            hsv = hsv.astype(np.float32)
            hsv[:, :, 2] = np.clip(hsv[:, :, 2] * factor, 0, 255)
            hsv = hsv.astype(np.uint8)
            return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        except Exception:
            return img
    
    def adjust_contrast(img: np.ndarray, factor: float) -> np.ndarray:
        """ëŒ€ë¹„ ì¡°ì • (factor < 1: ëŒ€ë¹„ ê°ì†Œ, CCTV í™”ì§ˆ ì €í•˜ ì‹œë®¬ë ˆì´ì…˜)"""
        if img is None or img.size == 0:
            return img
        try:
            return cv2.convertScaleAbs(img, alpha=factor, beta=0)
        except Exception:
            return img
    
    def apply_jpeg_compression(img: np.ndarray, quality: int = 60) -> np.ndarray:
        """JPEG ì••ì¶• ì‹œë®¬ë ˆì´ì…˜ (CCTV ì••ì¶• ì•„í‹°íŒ©íŠ¸)"""
        if img is None or img.size == 0:
            return img
        try:
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]
            result, encimg = cv2.imencode('.jpg', img, encode_param)
            if result:
                return cv2.imdecode(encimg, 1)
            return img
        except Exception:
            return img
    
    def reduce_saturation(img: np.ndarray, factor: float = 0.7) -> np.ndarray:
        """ì±„ë„ ê°ì†Œ (CCTV ìƒ‰ìƒ ì €í•˜ ì‹œë®¬ë ˆì´ì…˜)"""
        if img is None or img.size == 0:
            return img
        try:
            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
            hsv = hsv.astype(np.float32)
            hsv[:, :, 1] = np.clip(hsv[:, :, 1] * factor, 0, 255)  # ì±„ë„ ê°ì†Œ
            hsv = hsv.astype(np.uint8)
            return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        except Exception:
            return img
    
    def apply_strong_blur(img: np.ndarray, kernel_size: int = 5) -> np.ndarray:
        """ê°•í•œ ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ (CCTV ëª¨ì…˜ ë¸”ëŸ¬ ê°•í™”)"""
        if img is None or img.size == 0:
            return img
        try:
            return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)
        except Exception:
            return img

    for img_path in image_files:
        try:
            # ID/ì´ë¦„ ì¶”ì¶œ
            filename = img_path.stem
            parts = filename.split('_')
            parent_name = img_path.parent.name
            
            if parent_name != images_dir.name:
                person_id = parent_name
                name = parent_name
            else:
                if len(parts) >= 2:
                    person_id = parts[0]
                    name = parts[1]
                else:
                    person_id = filename
                    name = filename
                
            print(f"   Processing: {img_path.name} (ID: {person_id})", end=" -> ")
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            frame = cv2.imread(str(img_path))
            if frame is None:
                print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
                continue
            
            # YOLO ì¶”ë¡  (keypoints í¬í•¨, NMS timeout ì¦ê°€)
            # ONNX ëª¨ë¸ì€ task='pose'ë¡œ ë³€í™˜ë˜ì–´ ëœë“œë§ˆí¬ ì œê³µ
            # ë§ˆìŠ¤í¬ ì°©ìš© ì´ë¯¸ì§€ë„ ê°ì§€í•˜ê¸° ìœ„í•´ confidence thresholdë¥¼ ë” ë‚®ì¶¤
            results = yolo_face(
                frame, 
                verbose=False,
                task='pose',  # pose taskë¡œ ëœë“œë§ˆí¬ ìœ ì§€ (ONNX ë³€í™˜ ì‹œ task="pose" ì‚¬ìš©)
                conf=0.1,  # confidence threshold ë” ë‚®ì¶¤ (ë§ˆìŠ¤í¬ ì°©ìš© ì–¼êµ´ë„ ê°ì§€)
                iou=0.5,  # NMS IoU
                max_det=10,  # ìµœëŒ€ ê°ì§€ ìˆ˜
                imgsz=832  # ONNX ëª¨ë¸ í•´ìƒë„
            )
            
            if not results:
                print(f"âš ï¸ ê²°ê³¼ ì—†ìŒ")
                continue
            
            result = results[0]
            
            if result.boxes is None or len(result.boxes) == 0:
                # ë§ˆìŠ¤í¬ ì°©ìš© ì´ë¯¸ì§€ì˜ ê²½ìš° ì–¼êµ´ ê°ì§€ê°€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ
                # ë§¤ìš° ë‚®ì€ thresholdë¡œ ì¬ì‹œë„
                results_retry = yolo_face(
                    frame,
                    verbose=False,
                    task='pose',
                    conf=0.05,  # ë§¤ìš° ë‚®ì€ thresholdë¡œ ì¬ì‹œë„ (ë§ˆìŠ¤í¬ ì°©ìš© ì–¼êµ´)
                    iou=0.5,
                    max_det=10,
                    imgsz=832
                )
                if results_retry and len(results_retry) > 0:
                    result_retry = results_retry[0]
                    if result_retry.boxes is not None and len(result_retry.boxes) > 0:
                        result = result_retry
                        print(f"   âœ… ë‚®ì€ threshold ì¬ì‹œë„ ì„±ê³µ")
                    else:
                        print(f"âš ï¸ ì–¼êµ´ ì—†ìŒ (ë§ˆìŠ¤í¬ ì°©ìš© ë˜ëŠ” ì–¼êµ´ì´ ê°€ë ¤ì§„ ì´ë¯¸ì§€)")
                        continue
                else:
                    print(f"âš ï¸ ì–¼êµ´ ì—†ìŒ (ë§ˆìŠ¤í¬ ì°©ìš© ë˜ëŠ” ì–¼êµ´ì´ ê°€ë ¤ì§„ ì´ë¯¸ì§€)")
                    continue
            
            # keypoints í™•ì¸
            if result.keypoints is None:
                print(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ì—†ìŒ (ëª¨ë¸ì´ keypointsë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)")
                # keypoints ì—†ì´ë„ ì§„í–‰ (bboxë§Œ ì‚¬ìš©)
                boxes_xyxy = result.boxes.xyxy.cpu().numpy()
                if len(boxes_xyxy) > 0:
                    # ê°€ì¥ í° ì–¼êµ´ ì‚¬ìš©
                    best_box = boxes_xyxy[0]
                    max_area = (best_box[2] - best_box[0]) * (best_box[3] - best_box[1])
                    for i, box in enumerate(boxes_xyxy[1:], 1):
                        area = (box[2] - box[0]) * (box[3] - box[1])
                        if area > max_area:
                            max_area = area
                            best_box = box
                            best_idx = i
                    
                    # bboxì—ì„œ ëœë“œë§ˆí¬ ì¶”ì • (ê°„ë‹¨í•œ ë°©ë²•)
                    x1, y1, x2, y2 = best_box
                    center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2
                    width, height = x2 - x1, y2 - y1
                    # ê°„ë‹¨í•œ ëœë“œë§ˆí¬ ì¶”ì • (5ê°œ í¬ì¸íŠ¸)
                    kps = np.array([
                        [center_x - width * 0.2, center_y - height * 0.1],  # left_eye
                        [center_x + width * 0.2, center_y - height * 0.1],  # right_eye
                        [center_x, center_y + height * 0.1],  # nose
                        [center_x - width * 0.15, center_y + height * 0.3],  # left_mouth
                        [center_x + width * 0.15, center_y + height * 0.3],  # right_mouth
                    ], dtype=np.float32)
                    print(f"âš ï¸ í‚¤í¬ì¸íŠ¸ ì¶”ì • ì‚¬ìš© (bbox ê¸°ë°˜)")
                else:
                    continue
            else:
                # ê°€ì¥ í° ì–¼êµ´ ì°¾ê¸°
                best_idx = -1
                max_area = 0
                
                boxes_xyxy = result.boxes.xyxy.cpu().numpy()
                for i, box in enumerate(boxes_xyxy):
                    area = (box[2] - box[0]) * (box[3] - box[1])
                    if area > max_area:
                        max_area = area
                        best_idx = i
                
                if best_idx == -1:
                    continue
                    
                # ëœë“œë§ˆí¬ ì¶”ì¶œ
                kps = None
                # (N, 5, 2) í˜•íƒœ
                all_kps = result.keypoints.xy.cpu().numpy()
                if len(all_kps) > best_idx:
                    kps = all_kps[best_idx]
                
                if kps is None or len(kps) < 5:
                    print(f"âš ï¸ ëœë“œë§ˆí¬ ë¶€ì¡± (bbox ê¸°ë°˜ ì¶”ì • ì‚¬ìš©)")
                    # bboxì—ì„œ ëœë“œë§ˆí¬ ì¶”ì •
                    box = boxes_xyxy[best_idx]
                    x1, y1, x2, y2 = box
                    center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2
                    width, height = x2 - x1, y2 - y1
                    kps = np.array([
                        [center_x - width * 0.2, center_y - height * 0.1],
                        [center_x + width * 0.2, center_y - height * 0.1],
                        [center_x, center_y + height * 0.1],
                        [center_x - width * 0.15, center_y + height * 0.3],
                        [center_x + width * 0.15, center_y + height * 0.3],
                    ], dtype=np.float32)

            # --- TTA (Test Time Augmentation) ì ìš© ---
            # CCTV ì €í™”ì§ˆ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜: ê³ í™”ì§ˆ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì €í™”ì§ˆë¡œ ë³€í™˜í•˜ì—¬ ì‹¤ì‹œê°„ í™˜ê²½ê³¼ ë§¤ì¹­
            embeddings_to_add = []
            
            h, w = frame.shape[:2]
            
            # 1. ì›ë³¸ ì´ë¯¸ì§€
            emb_orig, _ = get_embedding_from_onnx(frame, kps)
            if emb_orig is not None:
                embeddings_to_add.append(emb_orig)
            
            # 2. ì¢Œìš° ë°˜ì „
            frame_flip = cv2.flip(frame, 1)
            kps_flip = kps.copy()
            kps_flip[:, 0] = w - kps_flip[:, 0]
            kps_flip[[0, 1]] = kps_flip[[1, 0]]  # left_eye <-> right_eye
            kps_flip[[3, 4]] = kps_flip[[4, 3]]  # left_mouth <-> right_mouth
            emb_flip, _ = get_embedding_from_onnx(frame_flip, kps_flip)
            if emb_flip is not None:
                embeddings_to_add.append(emb_flip)
            
            # 3. ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ (CCTV ëª¨ì…˜ ë¸”ëŸ¬ ì‹œë®¬ë ˆì´ì…˜)
            frame_blur = apply_gaussian_blur(frame, kernel_size=3)
            emb_blur, _ = get_embedding_from_onnx(frame_blur, kps)
            if emb_blur is not None:
                embeddings_to_add.append(emb_blur)
            
            # 4. ì €í•´ìƒë„ ë‹¤ìš´ìŠ¤ì¼€ì¼ í›„ ì—…ìŠ¤ì¼€ì¼ (CCTV ì €í•´ìƒë„ ì‹œë®¬ë ˆì´ì…˜)
            frame_lowres = apply_downscale_upscale(frame, scale_factor=0.6)
            emb_lowres, _ = get_embedding_from_onnx(frame_lowres, kps)
            if emb_lowres is not None:
                embeddings_to_add.append(emb_lowres)
            
            # 5. ë…¸ì´ì¦ˆ ì¶”ê°€ (CCTV ì••ì¶• ë…¸ì´ì¦ˆ ì‹œë®¬ë ˆì´ì…˜)
            frame_noise = add_gaussian_noise(frame, mean=0, std=8)
            emb_noise, _ = get_embedding_from_onnx(frame_noise, kps)
            if emb_noise is not None:
                embeddings_to_add.append(emb_noise)
            
            # 6. ì–´ë‘ìš´ ì¡°ëª… (ë°ê¸° ê°ì†Œ, CCTV ì–´ë‘ìš´ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜)
            frame_dark = adjust_brightness(frame, factor=0.7)
            emb_dark, _ = get_embedding_from_onnx(frame_dark, kps)
            if emb_dark is not None:
                embeddings_to_add.append(emb_dark)
            
            # 7. ëŒ€ë¹„ ê°ì†Œ (CCTV í™”ì§ˆ ì €í•˜ ì‹œë®¬ë ˆì´ì…˜)
            frame_low_contrast = adjust_contrast(frame, factor=0.8)
            emb_low_contrast, _ = get_embedding_from_onnx(frame_low_contrast, kps)
            if emb_low_contrast is not None:
                embeddings_to_add.append(emb_low_contrast)
            
            # 8. ë³µí•© ì €í™”ì§ˆ (ë¸”ëŸ¬ + ì €í•´ìƒë„ + ì–´ë‘ì›€)
            frame_composite = apply_gaussian_blur(frame, kernel_size=3)
            frame_composite = apply_downscale_upscale(frame_composite, scale_factor=0.7)
            frame_composite = adjust_brightness(frame_composite, factor=0.8)
            emb_composite, _ = get_embedding_from_onnx(frame_composite, kps)
            if emb_composite is not None:
                embeddings_to_add.append(emb_composite)
            
            # 9. JPEG ì••ì¶• ì‹œë®¬ë ˆì´ì…˜ (CCTV ì••ì¶• ì•„í‹°íŒ©íŠ¸)
            frame_jpeg = apply_jpeg_compression(frame, quality=55)
            emb_jpeg, _ = get_embedding_from_onnx(frame_jpeg, kps)
            if emb_jpeg is not None:
                embeddings_to_add.append(emb_jpeg)
            
            # 10. ì±„ë„ ê°ì†Œ (CCTV ìƒ‰ìƒ ì €í•˜)
            frame_desat = reduce_saturation(frame, factor=0.6)
            emb_desat, _ = get_embedding_from_onnx(frame_desat, kps)
            if emb_desat is not None:
                embeddings_to_add.append(emb_desat)
            
            # 11. ê°•í•œ ë¸”ëŸ¬ (CCTV ëª¨ì…˜ ë¸”ëŸ¬ ê°•í™”)
            frame_strong_blur = apply_strong_blur(frame, kernel_size=5)
            emb_strong_blur, _ = get_embedding_from_onnx(frame_strong_blur, kps)
            if emb_strong_blur is not None:
                embeddings_to_add.append(emb_strong_blur)
            
            # 12. ê·¹ë‹¨ì  ì €í™”ì§ˆ (ëª¨ë“  íš¨ê³¼ ë³µí•©)
            frame_extreme = apply_strong_blur(frame, kernel_size=5)
            frame_extreme = apply_downscale_upscale(frame_extreme, scale_factor=0.5)
            frame_extreme = adjust_brightness(frame_extreme, factor=0.7)
            frame_extreme = reduce_saturation(frame_extreme, factor=0.6)
            frame_extreme = apply_jpeg_compression(frame_extreme, quality=50)
            frame_extreme = add_gaussian_noise(frame_extreme, mean=0, std=10)
            emb_extreme, _ = get_embedding_from_onnx(frame_extreme, kps)
            if emb_extreme is not None:
                embeddings_to_add.append(emb_extreme)
            
            # DB ì¶”ê°€
            if embeddings_to_add:
                for emb in embeddings_to_add:
                    index.add(np.array([emb], dtype=np.float32))
                    labels.append({'id': person_id, 'name': name})
                success_count += len(embeddings_to_add)
                person_count += 1
                print(f"âœ… ë“±ë¡ ì„±ê³µ ({len(embeddings_to_add)}ê°œ ë²¡í„°)")
            else:
                print(f"âš ï¸ ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

    # 4. ì €ì¥
    print("="*60)
    if success_count > 0:
        faiss.write_index(index, str(index_path))
        np.save(str(labels_path), labels)
        print(f"ğŸ‰ ì™„ë£Œ! ì´ {person_count}ëª… ({success_count}ê°œ ë²¡í„°) ë“±ë¡ë¨.")
    else:
        print("âš ï¸ ë“±ë¡ëœ ì–¼êµ´ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
