"""
ì–¼êµ´ ì¸ì‹ ë””ë²„ê·¸ í…ŒìŠ¤íŠ¸
- ë“±ë¡ ì´ë¯¸ì§€ë¡œ ì¸ì‹ í…ŒìŠ¤íŠ¸
- ì„ê³„ê°’ ë¶„ì„
"""

import os
import sys
import cv2
import numpy as np
import faiss
from insightface.app import FaceAnalysis

# ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
FAISS_INDEX_FILE = os.path.join(PROJECT_ROOT, "face", "data", "face_index.faiss")
FAISS_LABELS_FILE = os.path.join(PROJECT_ROOT, "face", "data", "face_index.faiss.labels.npy")

# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ
TEST_IMAGES = {
    "ìœ ìŠ¹ì›": os.path.join(PROJECT_ROOT, "face", "image", "ìœ ìŠ¹ì›", "ìœ ìŠ¹ì›ì •ë©´.jpg"),
    "ì •ì¤€ì„±": os.path.join(PROJECT_ROOT, "face", "image", "ì •ì¤€ì„±", "ì •ì¤€ì„±ì •ë©´1.jpg"),
    "ì¡°ì´í•œ": os.path.join(PROJECT_ROOT, "face", "image", "ì¡°ì´í•œ", "ì¡°ì´í•œì •ë©´.jpg"),
}


def load_faiss_index():
    """FAISS ì¸ë±ìŠ¤ ë¡œë“œ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š FAISS ì¸ë±ìŠ¤ ë¡œë“œ")
    print("=" * 60)
    
    index = faiss.read_index(FAISS_INDEX_FILE)
    labels = np.load(FAISS_LABELS_FILE, allow_pickle=True)
    
    print(f"âœ… ì¸ë±ìŠ¤ í¬ê¸°: {index.ntotal}")
    print(f"âœ… ì„ë² ë”© ì°¨ì›: {index.d}")
    print(f"âœ… ë ˆì´ë¸” ìˆ˜: {len(labels)}")
    
    # ì¸ë¬¼ë³„ í†µê³„
    unique, counts = np.unique(labels, return_counts=True)
    print("\nğŸ“‹ ì¸ë¬¼ë³„ ì„ë² ë”© ìˆ˜:")
    for name, count in zip(unique, counts):
        print(f"   - {name}: {count}ê°œ")
    
    return index, labels


def test_recognition(face_analyzer, index, labels):
    """ì–¼êµ´ ì¸ì‹ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸ” ì–¼êµ´ ì¸ì‹ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    results = []
    
    for name, img_path in TEST_IMAGES.items():
        print(f"\nğŸ‘¤ í…ŒìŠ¤íŠ¸: {name}")
        print(f"   ì´ë¯¸ì§€: {img_path}")
        
        if not os.path.exists(img_path):
            print(f"   âŒ ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ!")
            continue
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        img = cv2.imdecode(np.fromfile(img_path, dtype=np.uint8), cv2.IMREAD_COLOR)
        if img is None:
            print(f"   âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨!")
            continue
        
        print(f"   ì´ë¯¸ì§€ í¬ê¸°: {img.shape}")
        
        # ì–¼êµ´ ê²€ì¶œ ë° ì„ë² ë”© ì¶”ì¶œ
        faces = face_analyzer.get(img)
        
        if not faces:
            print(f"   âŒ ì–¼êµ´ ë¯¸ê²€ì¶œ!")
            continue
        
        face = faces[0]
        embedding = face.embedding
        
        # ì •ê·œí™”
        embedding = embedding / np.linalg.norm(embedding)
        
        print(f"   âœ… ì–¼êµ´ ê²€ì¶œë¨")
        print(f"   ì„ë² ë”© norm: {np.linalg.norm(embedding):.4f}")
        
        # FAISS ê²€ìƒ‰ (Inner Product = Cosine Similarity for normalized vectors)
        embedding = embedding.reshape(1, -1).astype(np.float32)
        k = 5  # Top-5 ê²€ìƒ‰
        
        distances, indices = index.search(embedding, k)
        
        print(f"\n   ğŸ” Top-5 ê²€ìƒ‰ ê²°ê³¼:")
        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
            matched_name = labels[idx]
            # Inner Product (IP) ê±°ë¦¬ -> ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            # IP = cos(theta) for normalized vectors
            similarity = dist  # ì´ë¯¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            print(f"      {i+1}. {matched_name}: ìœ ì‚¬ë„ {similarity:.4f} (ê±°ë¦¬ {dist:.4f})")
        
        # ìµœì¢… íŒì •
        best_idx = indices[0][0]
        best_dist = distances[0][0]
        best_name = labels[best_idx]
        
        # ì¼ë°˜ì ì¸ ì„ê³„ê°’ë“¤
        thresholds = {
            "ì—„ê²© (0.4)": 0.4,
            "ë³´í†µ (0.35)": 0.35,
            "ëŠìŠ¨ (0.3)": 0.3,
            "ë§¤ìš° ëŠìŠ¨ (0.25)": 0.25,
        }
        
        print(f"\n   ğŸ“Š ì„ê³„ê°’ë³„ íŒì •:")
        for th_name, threshold in thresholds.items():
            if best_dist >= threshold:
                status = f"âœ… {best_name} (ìœ ì‚¬ë„ {best_dist:.4f} >= {threshold})"
            else:
                status = f"âŒ ì•Œìˆ˜ì—†ìŒ (ìœ ì‚¬ë„ {best_dist:.4f} < {threshold})"
            print(f"      {th_name}: {status}")
        
        results.append({
            "expected": name,
            "predicted": best_name,
            "similarity": best_dist,
            "correct": name == best_name
        })
    
    return results


def analyze_database_quality(index, labels):
    """ë°ì´í„°ë² ì´ìŠ¤ í’ˆì§ˆ ë¶„ì„"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ë°ì´í„°ë² ì´ìŠ¤ í’ˆì§ˆ ë¶„ì„")
    print("=" * 60)
    
    unique_labels = list(set(labels))
    
    # ê° ì¸ë¬¼ë³„ ì„ë² ë”© ì¶”ì¶œ ë° ë¶„ì„
    for person in sorted(unique_labels):
        indices = [i for i, l in enumerate(labels) if l == person]
        embeddings = np.array([index.reconstruct(i) for i in indices])
        
        # ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚°
        centroid = np.mean(embeddings, axis=0)
        centroid = centroid / np.linalg.norm(centroid)
        
        # í´ë˜ìŠ¤ ë‚´ ìœ ì‚¬ë„
        similarities = []
        for emb in embeddings:
            emb = emb / np.linalg.norm(emb)
            sim = np.dot(emb, centroid)
            similarities.append(sim)
        
        avg_sim = np.mean(similarities)
        min_sim = np.min(similarities)
        max_sim = np.max(similarities)
        
        print(f"\nğŸ‘¤ {person}:")
        print(f"   ì„ë² ë”© ìˆ˜: {len(indices)}")
        print(f"   ì¤‘ì‹¬ê³¼ì˜ ìœ ì‚¬ë„: í‰ê·  {avg_sim:.4f}, ìµœì†Œ {min_sim:.4f}, ìµœëŒ€ {max_sim:.4f}")
    
    # í´ë˜ìŠ¤ ê°„ ë¶„ë¦¬ë„
    print("\nğŸ” í´ë˜ìŠ¤ ê°„ ì„¼íŠ¸ë¡œì´ë“œ ìœ ì‚¬ë„ (ë‚®ì„ìˆ˜ë¡ ë¶„ë¦¬ ì˜ë¨):")
    centroids = {}
    for person in sorted(unique_labels):
        indices = [i for i, l in enumerate(labels) if l == person]
        embeddings = np.array([index.reconstruct(i) for i in indices])
        centroid = np.mean(embeddings, axis=0)
        centroids[person] = centroid / np.linalg.norm(centroid)
    
    persons = sorted(unique_labels)
    for i, p1 in enumerate(persons):
        for p2 in persons[i+1:]:
            sim = np.dot(centroids[p1], centroids[p2])
            print(f"   {p1} â†” {p2}: {sim:.4f}")


def main():
    print("=" * 60)
    print("ğŸ”¬ ì–¼êµ´ ì¸ì‹ ë””ë²„ê·¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # buffalo_l ëª¨ë¸ ë¡œë“œ
    print("\nğŸ¦¬ buffalo_l ëª¨ë¸ ë¡œë”© ì¤‘...")
    face_analyzer = FaceAnalysis(name='buffalo_l', allowed_modules=['detection', 'recognition'])
    face_analyzer.prepare(ctx_id=-1, det_size=(640, 640))
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
    # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
    index, labels = load_faiss_index()
    
    # ë°ì´í„°ë² ì´ìŠ¤ í’ˆì§ˆ ë¶„ì„
    analyze_database_quality(index, labels)
    
    # ì–¼êµ´ ì¸ì‹ í…ŒìŠ¤íŠ¸
    results = test_recognition(face_analyzer, index, labels)
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    correct = sum(1 for r in results if r["correct"])
    total = len(results)
    
    print(f"\nì •í™•ë„: {correct}/{total} ({100*correct/total:.1f}%)")
    
    for r in results:
        status = "âœ…" if r["correct"] else "âŒ"
        print(f"   {status} {r['expected']}: ì˜ˆì¸¡={r['predicted']}, ìœ ì‚¬ë„={r['similarity']:.4f}")


if __name__ == "__main__":
    main()

