# frame_processor.py - í”„ë ˆì„ ì²˜ë¦¬ ë¡œì§
"""
ë‹¨ì¼ í”„ë ˆì„ ì²˜ë¦¬ ëª¨ë“ˆ
AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
"""
import logging
import time
import torch  # ì¶”ê°€: torch ì„í¬íŠ¸
from typing import Dict, Tuple, Any, List, Optional
from concurrent.futures import as_completed, TimeoutError as FuturesTimeoutError

import cv2
import numpy as np
from ultralytics.engine.results import Keypoints

import utils
import config
from utils import find_best_match_faiss, find_best_matches_faiss_batch, draw_modern_bbox, draw_fast_bbox, calculate_iou_batch
from exceptions import (
    ProcessingError,
    FaceRecognitionError,
    ValidationError
)
import state
from state import (
    safety_system_lock,
    frame_stats,
    frame_stats_lock,
    yolo_executor,
    face_recognition_executor,
    dangerous_behavior_executor,
    frame_processing_executor,
    recent_identity_cache,
    embedding_buffers,
    EMBEDDING_BUFFER_SIZE,
    EMBEDDING_BUFFER_MIN_SIZE,
    MAX_EMBEDDING_BUFFERS_PER_CAM,
    fall_start_times,
    FALL_DURATION_THRESHOLD,
    centroid_cache,
    face_bbox_cache,
    last_render_cache,
    last_face_detection_frame,
    face_detection_lock,
    face_recognition_cooldown_ts,
    frame_buffer,
    MAX_BUFFER_SECONDS,
    frame_buffer_lock,
    model_results_cache,
    results_cache_lock,
    CACHE_TTL
)
from ai_processors import (
    _process_ppe_detection,
    _process_face_recognition,
    _process_dangerous_behavior
)
from pipeline_manager import (
    person_tracker,
    face_recognition_queue,
    get_person_crop,
    should_run_face_detection_for_frame,
    PersonTracker,
    FaceRecognitionQueue
)
from state import (
    track_states,
    track_states_lock,
    new_track_ids,
    NEW_TRACK_THRESHOLD,
    last_face_recognition_by_track,
    FACE_RECOGNITION_INTERVAL_PER_TRACK
)

# ë§ˆì§€ë§‰ ë Œë”ë§ëœ í”„ë ˆì„ ìºì‹œ (ìŠ¤í‚µ í”„ë ˆì„ì—ì„œ ë°”ìš´ë”© ë°•ìŠ¤ ìœ ì§€ìš©)
_last_rendered_frames = {}  # {cam_id: (frame_bytes, result_dict)}


# ========================================
# 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ í—¬í¼ í•¨ìˆ˜
# ========================================

def _update_track_state(cam_id: int, track_id: int, person_box: Tuple[int, int, int, int],
                        has_violation: bool = False, violation_types: List[str] = None) -> Dict:
    """
    Track ID ìƒíƒœ ì—…ë°ì´íŠ¸ (2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ìš©)
    
    Args:
        cam_id: ì¹´ë©”ë¼ ID
        track_id: ì¶”ì  ID
        person_box: ì‚¬ëŒ ë°”ìš´ë”© ë°•ìŠ¤
        has_violation: ìœ„ë°˜ ë°œìƒ ì—¬ë¶€
        violation_types: ìœ„ë°˜ ìœ í˜• ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
    """
    now = time.time()
    
    with track_states_lock:
        if track_id not in track_states[cam_id]:
            # ìƒˆë¡œìš´ Track ID
            track_states[cam_id][track_id] = {
                'name': None,
                'confidence': 0.0,
                'last_recognition': 0.0,
                'violations': [],
                'person_box': person_box,
                'first_seen': now,
                'last_update': now,
                'face_bbox': None,
                'embedding': None
            }
            # ìƒˆë¡œìš´ Track ID ê¸°ë¡
            new_track_ids[cam_id][track_id] = now
        else:
            # ê¸°ì¡´ Track ID ì—…ë°ì´íŠ¸
            track_states[cam_id][track_id]['person_box'] = person_box
            track_states[cam_id][track_id]['last_update'] = now
        
        # ìœ„ë°˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        if has_violation and violation_types:
            track_states[cam_id][track_id]['violations'] = violation_types
        
        return track_states[cam_id][track_id]


def _should_recognize_face_for_track(cam_id: int, track_id: int, 
                                      has_violation: bool = False) -> Tuple[bool, str]:
    """
    í•´ë‹¹ Track IDì— ëŒ€í•´ ì–¼êµ´ ì¸ì‹ì„ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨
    
    ì¡°ê±´:
    - ì¡°ê±´ A: ìœ„ë°˜ ì‚¬í•­ ë°œìƒ ì‹œ (ì“°ëŸ¬ì§ OR ì•ˆì „ì¥ë¹„ ë¯¸ì°©ìš©)
    - ì¡°ê±´ B: ìƒˆë¡œìš´ ì‚¬ëŒ ë“±ì¥ ì‹œ (Track IDê°€ ì²˜ìŒ ìƒì„±ë˜ì—ˆì„ ë•Œ)
    - ì¡°ê±´ C: ì£¼ê¸°ì  í™•ì¸ (í•´ë‹¹ IDì— ëŒ€í•´ 1ì´ˆì— 1ë²ˆë§Œ)
    
    Returns:
        (should_recognize, reason)
    """
    now = time.time()
    
    with track_states_lock:
        if track_id not in track_states[cam_id]:
            return False, "unknown_track"
        
        state = track_states[cam_id][track_id]
        
        # ì¡°ê±´ A: ìœ„ë°˜ ì‚¬í•­ ë°œìƒ ì‹œ (ìµœìš°ì„ )
        if has_violation:
            last_face_recognition_by_track[cam_id][track_id] = now
            return True, "violation_detected"
        
        # ì¡°ê±´ B: ìƒˆë¡œìš´ ì‚¬ëŒ ë“±ì¥ ì‹œ
        if track_id in new_track_ids[cam_id]:
            first_seen = new_track_ids[cam_id][track_id]
            if now - first_seen < NEW_TRACK_THRESHOLD:
                # ìƒˆë¡œìš´ ì‚¬ëŒì´ê³  ì•„ì§ ì¸ì‹ ì•ˆ ë¨
                if state['name'] is None:
                    last_face_recognition_by_track[cam_id][track_id] = now
                    return True, "new_person"
        
        # ì¡°ê±´ C: ì£¼ê¸°ì  í™•ì¸ (1ì´ˆì— 1ë²ˆ)
        last_recognition = last_face_recognition_by_track[cam_id].get(track_id, 0)
        if now - last_recognition >= FACE_RECOGNITION_INTERVAL_PER_TRACK:
            # ì•„ì§ ì¸ì‹ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ
            if state['name'] is None or state['confidence'] < 0.5:
                last_face_recognition_by_track[cam_id][track_id] = now
                return True, "periodic_check"
        
        return False, "no_need"


def _set_track_recognized(cam_id: int, track_id: int, name: str, 
                          confidence: float, face_bbox: Tuple = None,
                          embedding: np.ndarray = None) -> None:
    """
    Track IDì— ì–¼êµ´ ì¸ì‹ ê²°ê³¼ ì €ì¥
    """
    with track_states_lock:
        if track_id in track_states[cam_id]:
            track_states[cam_id][track_id]['name'] = name
            track_states[cam_id][track_id]['confidence'] = confidence
            track_states[cam_id][track_id]['last_recognition'] = time.time()
            if face_bbox:
                track_states[cam_id][track_id]['face_bbox'] = face_bbox
            if embedding is not None:
                track_states[cam_id][track_id]['embedding'] = embedding
            
            # ì¸ì‹ ì„±ê³µ ì‹œ ìƒˆë¡œìš´ ì‚¬ëŒ ëª©ë¡ì—ì„œ ì œê±°
            if name and name != "Unknown" and track_id in new_track_ids[cam_id]:
                del new_track_ids[cam_id][track_id]


def _cleanup_expired_tracks(cam_id: int, expiry_time: float = 30.0) -> None:
    """
    ë§Œë£Œëœ Track ID ì •ë¦¬
    """
    now = time.time()
    
    with track_states_lock:
        expired_tracks = []
        for track_id, state in track_states[cam_id].items():
            if now - state['last_update'] > expiry_time:
                expired_tracks.append(track_id)
        
        for track_id in expired_tracks:
            del track_states[cam_id][track_id]
            if track_id in new_track_ids[cam_id]:
                del new_track_ids[cam_id][track_id]
            if track_id in last_face_recognition_by_track[cam_id]:
                del last_face_recognition_by_track[cam_id][track_id]
        
        if expired_tracks:
            logging.debug(f"[CAM-{cam_id}] ë§Œë£Œëœ Track ID ì •ë¦¬: {len(expired_tracks)}ê°œ")

def render_frame_results(
    frame: np.ndarray,
    recognized_faces: List[Dict],
    violations: List[Dict],
    cam_id: int,
    orig_w: int,
    orig_h: int
) -> np.ndarray:
    """
    í”„ë ˆì„ì— AI ê²°ê³¼ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤.
    (ì–¼êµ´ ë°•ìŠ¤ì™€ person_box í†µí•© ë° ë Œë”ë§)
    """
    processed_frame = frame.copy()
    
    # 1. recognized_facesì™€ violationsë¥¼ í†µí•©
    all_boxes = []
    box_to_info = {}  # box_tuple -> (name, ppe_violations, is_violation)
    
    # recognized_faces ì²˜ë¦¬: ì•ˆì „í•œ ì‚¬ëŒ(ìœ„ë°˜ ì—†ìŒ)ë„ ë°•ìŠ¤ í‘œì‹œ
    face_boxes_info = {}  # ì–¼êµ´ ë°•ìŠ¤ ì •ë³´ ì„ì‹œ ì €ì¥ (person_boxì™€ ë§¤ì¹­ìš©)
    for face in recognized_faces:
        box = face.get("box") or face.get("bbox")
        if box and len(box) == 4:
            box_tuple = tuple(map(int, box))
            name = face.get("name", "Unknown")
            ppe_violations = face.get("ppe_violations", [])
            # ë§ˆìŠ¤í¬ ì œì™¸
            filtered_ppe = [v for v in ppe_violations if v != "ë§ˆìŠ¤í¬"]
            is_violation = face.get("isViolation", False) or len(filtered_ppe) > 0
            
            # ì–¼êµ´ ë°•ìŠ¤ ì •ë³´ ì €ì¥
            face_boxes_info[box_tuple] = {
                'name': name,
                'ppe_violations': filtered_ppe,
                'is_violation': is_violation
            }
            
            # â­ ì•ˆì „í•œ ì‚¬ëŒ(ìœ„ë°˜ ì—†ìŒ)ë„ all_boxesì— ì¶”ê°€ - ì´ˆë¡ ë°•ìŠ¤ í‘œì‹œ!
            if not is_violation:
                if box_tuple not in all_boxes:
                    all_boxes.append(box_tuple)
                    box_to_info[box_tuple] = (name, filtered_ppe, False)  # is_violation=False
    
    # violations ì²˜ë¦¬: person_boxë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì–¼êµ´ ë°•ìŠ¤ì™€ ë§¤ì¹­í•˜ì—¬ ì´ë¦„ í†µí•©
    # â­ ì´ë¯¸ ë§¤ì¹­ëœ ì–¼êµ´ ë°•ìŠ¤ ì¶”ì  (ì¤‘ë³µ ë§¤ì¹­ ë°©ì§€)
    matched_face_boxes = set()
    
    for violation in violations:
        box = violation.get("person_box") or violation.get("bbox") or violation.get("box")
        if box and len(box) == 4:
            box_tuple = tuple(map(int, box))
            # ìœ„ë°˜ ì •ë³´ì—ì„œ ì–¼êµ´ ì¸ì‹ëœ ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (ìš°ì„ ìˆœìœ„: recognized_name > worker)
            recognized_name = violation.get("recognized_name", "Unknown")
            worker = violation.get("worker", "ì•Œ ìˆ˜ ì—†ìŒ")
            # recognized_nameì´ "Unknown"ì´ ì•„ë‹ˆë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ worker ì‚¬ìš©
            violation_name = recognized_name if recognized_name != "Unknown" else (worker if worker != "ì•Œ ìˆ˜ ì—†ìŒ" else "Unknown")
            violations_list = violation.get("violations", [])
            
            # ì–¼êµ´ ë°•ìŠ¤ì™€ ë§¤ì¹­ (ì–¼êµ´ ë°•ìŠ¤ê°€ person_box ë‚´ë¶€ì— ìˆìœ¼ë©´ ë§¤ì¹­)
            # â­ ê°€ì¥ IoUê°€ ë†’ì€ ì–¼êµ´ë§Œ ë§¤ì¹­ (ì´ë¯¸ ë§¤ì¹­ëœ ì–¼êµ´ ì œì™¸)
            matched_face_box = None
            matched_face_info = None
            best_iou = 0.0
            
            for face_box_tuple, face_info in face_boxes_info.items():
                # â­ ì´ë¯¸ ë§¤ì¹­ëœ ì–¼êµ´ì€ ê±´ë„ˆë›°ê¸°
                if face_box_tuple in matched_face_boxes:
                    continue
                
                # IoU ê³„ì‚°
                iou = utils.calculate_iou(box_tuple, face_box_tuple)
                if iou > 0.3 and iou > best_iou:  # IoU 0.3 ì´ìƒì´ê³  ë” ë†’ì€ IoUë©´ ì—…ë°ì´íŠ¸
                    best_iou = iou
                    matched_face_box = face_box_tuple
                    matched_face_info = face_info
                    continue  # ë” ì¢‹ì€ ë§¤ì¹­ì„ ì°¾ê¸° ìœ„í•´ ê³„ì† ê²€ìƒ‰
                
                # ì–¼êµ´ ë°•ìŠ¤ ì¤‘ì‹¬ì ì´ person_box ë‚´ë¶€ì— ìˆëŠ”ì§€ í™•ì¸
                fx1, fy1, fx2, fy2 = face_box_tuple
                v_x1, v_y1, v_x2, v_y2 = box_tuple
                face_center_x = (fx1 + fx2) / 2
                face_center_y = (fy1 + fy2) / 2
                if (v_x1 <= face_center_x <= v_x2 and v_y1 <= face_center_y <= v_y2):
                    # ì–¼êµ´ ë°•ìŠ¤ê°€ person_box ë‚´ë¶€ì— ìˆìœ¼ë©´ ë§¤ì¹­ (IoUë³´ë‹¤ ìš°ì„ )
                    if matched_face_box is None or iou < 0.3:  # IoU ë§¤ì¹­ì´ ì—†ì„ ë•Œë§Œ
                        matched_face_box = face_box_tuple
                        matched_face_info = face_info
            
            # â­ ë§¤ì¹­ëœ ì–¼êµ´ ë°•ìŠ¤ ê¸°ë¡
            if matched_face_box:
                matched_face_boxes.add(matched_face_box)
            
            # ì–¼êµ´ ë°•ìŠ¤ì™€ ë§¤ì¹­ëœ ê²½ìš°: ì´ë¦„ê³¼ ìœ„ë°˜ ì •ë³´ í†µí•©
            if matched_face_info:
                face_name = matched_face_info['name']
                face_ppe = matched_face_info.get('ppe_violations', [])
                # ì–¼êµ´ ì¸ì‹ ê²°ê³¼ì˜ ì´ë¦„ì´ "Unknown"ì´ ì•„ë‹ˆë©´ ìš°ì„  ì‚¬ìš©
                final_name = face_name if face_name != "Unknown" else violation_name
                # ìœ„ë°˜ ì •ë³´ ë³‘í•© (ì¤‘ë³µ ì œê±°) - ë§ˆìŠ¤í¬ ì œì™¸
                merged_ppe = [v for v in list(set(face_ppe + violations_list)) if v != "ë§ˆìŠ¤í¬"]
                # â­ ìœ„ë°˜ì´ ìˆì„ ë•Œë§Œ is_violation=True
                box_to_info[box_tuple] = (final_name, merged_ppe, len(merged_ppe) > 0)
            else:
                # ì–¼êµ´ ë°•ìŠ¤ì™€ ë§¤ì¹­ë˜ì§€ ì•Šì€ ê²½ìš°: ìœ„ë°˜ ì •ë³´ë§Œ ì‚¬ìš©
                # ë§ˆìŠ¤í¬ ì œì™¸
                filtered_violations = [v for v in violations_list if v != "ë§ˆìŠ¤í¬"]
                # â­ ìœ„ë°˜ì´ ìˆì„ ë•Œë§Œ is_violation=True
                box_to_info[box_tuple] = (violation_name, filtered_violations, len(filtered_violations) > 0)
            
            # person_boxëŠ” í•­ìƒ all_boxesì— ì¶”ê°€ (ì–¼êµ´ ë°•ìŠ¤ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
            if box_tuple not in all_boxes:
                all_boxes.append(box_tuple)
    
    # ì–¼êµ´ ë°•ìŠ¤ê°€ person_boxì™€ ë§¤ì¹­ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬ (ì´ë¦„ë§Œ í‘œì‹œ, ìœ„ë°˜ ì—†ìŒ)
    for face_box_tuple, face_info in face_boxes_info.items():
        # ì´ë¯¸ person_boxì™€ ë§¤ì¹­ë˜ì—ˆëŠ”ì§€ í™•ì¸
        is_matched = False
        for person_box_tuple in all_boxes:
            iou = utils.calculate_iou(face_box_tuple, person_box_tuple)
            if iou > 0.3:
                is_matched = True
                break
            # ì–¼êµ´ ë°•ìŠ¤ ì¤‘ì‹¬ì ì´ person_box ë‚´ë¶€ì— ìˆëŠ”ì§€ í™•ì¸
            fx1, fy1, fx2, fy2 = face_box_tuple
            px1, py1, px2, py2 = person_box_tuple
            face_center_x = (fx1 + fx2) / 2
            face_center_y = (fy1 + fy2) / 2
            if (px1 <= face_center_x <= px2 and py1 <= face_center_y <= py2):
                is_matched = True
                break
        
        # ë§¤ì¹­ë˜ì§€ ì•Šì€ ì–¼êµ´ ë°•ìŠ¤ëŠ” ì œì™¸ (ì´ë¦„ë§Œ ìˆëŠ” ê²½ìš°ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ)
        # ìœ„ë°˜ì´ ìˆê±°ë‚˜ ì´ë¦„ì´ "Unknown"ì´ ì•„ë‹ˆë©´ person_boxì™€ ë§¤ì¹­ë˜ì–´ì•¼ í•¨
        if not is_matched:
            # ì–¼êµ´ ë°•ìŠ¤ë§Œ ìˆê³  person_boxê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë¬´ì‹œ (ì´ë¦„ ë¼ë²¨ í†µí•©ì„ ìœ„í•´)
            pass
    
    # IoU ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ê°™ì€ ì‚¬ëŒì— ëŒ€í•œ ì¤‘ë³µ ë°•ìŠ¤ ì œê±°)
    final_boxes = []
    for box_tuple in all_boxes:
        is_duplicate = False
        for final_box in final_boxes:
            iou = utils.calculate_iou(box_tuple, final_box)
            if iou > 0.7:  # IoU 0.7 ì´ìƒì´ë©´ ê°™ì€ ì‚¬ëŒìœ¼ë¡œ ê°„ì£¼ (0.98 -> 0.7ë¡œ ì™„í™”)
                is_duplicate = True
                # ë” í° ë°•ìŠ¤ë¡œ í†µí•©
                box_area = (box_tuple[2] - box_tuple[0]) * (box_tuple[3] - box_tuple[1])
                final_area = (final_box[2] - final_box[0]) * (final_box[3] - final_box[1])
                if box_area > final_area:
                    # ë” í° ë°•ìŠ¤ë¡œ êµì²´
                    final_boxes.remove(final_box)
                    final_boxes.append(box_tuple)
                    # ì •ë³´ë„ ì—…ë°ì´íŠ¸
                    box_to_info[box_tuple] = box_to_info.get(final_box, box_to_info[box_tuple])
                    if final_box in box_to_info:
                        del box_to_info[final_box]
                break
        
        if not is_duplicate:
            final_boxes.append(box_tuple)
    
    # ì¢Œí‘œ ìŠ¤ë¬´ë”©: ì´ì „ í”„ë ˆì„ê³¼ ë§¤ì¹­ëœ ë°•ìŠ¤ë§Œ ì¢Œí‘œ ìŠ¤ë¬´ë”© (ì”ìƒ ë°©ì§€)
    if cam_id in _last_rendered_frames:
        _, last_result = _last_rendered_frames[cam_id]
        last_faces = last_result.get("recognized_faces", [])
        
        # ì´ì „ í”„ë ˆì„ì˜ ë°•ìŠ¤ì™€ ë§¤ì¹­í•˜ì—¬ ì¢Œí‘œ ìŠ¤ë¬´ë”© (í˜„ì¬ í”„ë ˆì„ì— ìˆëŠ” ë°•ìŠ¤ë§Œ)
        for last_face in last_faces:
            last_box = last_face.get("box") or last_face.get("bbox")
            if not last_box or len(last_box) != 4:
                continue
            last_box_tuple = tuple(map(int, last_box))
            
            # í˜„ì¬ ë°•ìŠ¤ì™€ ë§¤ì¹­ (í˜„ì¬ í”„ë ˆì„ì— ìˆëŠ” ë°•ìŠ¤ë§Œ ìŠ¤ë¬´ë”©)
            for i, current_box_tuple in enumerate(final_boxes):
                iou = utils.calculate_iou(last_box_tuple, current_box_tuple)
                if iou > 0.5:  # IoU 0.5 ì´ìƒì´ë©´ ê°™ì€ ì‚¬ëŒ
                    # ì¢Œí‘œ ìŠ¤ë¬´ë”© (95% í˜„ì¬, 5% ì´ì „) - ì •í™•ë„ ìµœìš°ì„ , ë¯¸ì„¸í•œ ë–¨ë¦¼ë§Œ ë°©ì§€
                    smoothed_box = (
                        int(current_box_tuple[0] * 0.95 + last_box_tuple[0] * 0.05),
                        int(current_box_tuple[1] * 0.95 + last_box_tuple[1] * 0.05),
                        int(current_box_tuple[2] * 0.95 + last_box_tuple[2] * 0.05),
                        int(current_box_tuple[3] * 0.95 + last_box_tuple[3] * 0.05)
                    )
                    final_boxes[i] = smoothed_box
                    box_to_info[smoothed_box] = box_to_info.pop(current_box_tuple, box_to_info[current_box_tuple])
                    break
    
    # ë Œë”ë§
    renderer = utils.TextRenderer(frame.shape)
    for box_tuple in final_boxes:
        x1, y1, x2, y2 = box_tuple
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(orig_w, x2), min(orig_h, y2)
        if x2 > x1 and y2 > y1:
            name, ppe_violations, is_violation = box_to_info.get(box_tuple, ("Unknown", [], False))
            
            # ì–¼êµ´ ë°•ìŠ¤ í•„í„°ë§: ë°•ìŠ¤ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì–¼êµ´ ë°•ìŠ¤ë¡œ ê°„ì£¼í•˜ì—¬ ì œì™¸
            box_w = x2 - x1
            box_h = y2 - y1
            box_area = box_w * box_h
            frame_area = orig_w * orig_h
            box_ratio = box_area / frame_area if frame_area > 0 else 0
            
            # ë°•ìŠ¤ê°€ í”„ë ˆì„ì˜ 2% ë¯¸ë§Œì´ë©´ ì–¼êµ´ ë°•ìŠ¤ë¡œ ê°„ì£¼í•˜ì—¬ ì œì™¸ (5% -> 2%ë¡œ ì™„í™”)
            # ë‹¨, ìœ„ë°˜ì´ ìˆìœ¼ë©´ ì‘ì€ ë°•ìŠ¤ë„ í‘œì‹œ
            if box_ratio < 0.02 and not is_violation and len(ppe_violations) == 0:
                # ì–¼êµ´ ë°•ìŠ¤ëŠ” person_boxì™€ í†µí•©ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ë³„ë„ë¡œ ë Œë”ë§í•˜ì§€ ì•ŠìŒ
                logging.debug(f"[CAM-{cam_id}] ì–¼êµ´ ë°•ìŠ¤ ì œì™¸: ë°•ìŠ¤={box_tuple}, ë¹„ìœ¨={box_ratio:.3f}")
                continue
            
            # ëª¨ë“  ì‚¬ëŒì—ê²Œ ë°•ìŠ¤ í‘œì‹œ (ìœ„ë°˜ ì—¬ë¶€ ê´€ê³„ì—†ì´)
            # ë§ˆìŠ¤í¬ ì œì™¸í•œ ìœ„ë°˜ë§Œ ì²´í¬
            filtered_violations = [v for v in ppe_violations if v != "ë§ˆìŠ¤í¬"]
            
            if "ë„˜ì–´ì§" in filtered_violations:
                unified_color = (0, 50, 255)  # ë¹¨ê°„ìƒ‰
                alpha = 0.25
            elif len(filtered_violations) > 0:
                unified_color = (0, 140, 255)  # ì£¼í™©ìƒ‰
                alpha = 0.2
            else:
                unified_color = (50, 255, 50)  # ì´ˆë¡ìƒ‰ (ì•ˆì „!)
                alpha = 0.15
            
            draw_modern_bbox(processed_frame, x1, y1, x2, y2, unified_color, thickness=3, corner_length=35, alpha=alpha)
            
            if name != "Unknown" or is_violation or len(ppe_violations) == 0:
                display_name = name if name != "Unknown" else "ì•Œ ìˆ˜ ì—†ìŒ"
                violation_str = ""
                if ppe_violations:
                    # ë§ˆìŠ¤í¬ ì œì™¸
                    filtered_violations = [v for v in ppe_violations if v != "ë§ˆìŠ¤í¬"]
                    if not filtered_violations:
                        # ë§ˆìŠ¤í¬ë§Œ ìˆì—ˆìœ¼ë©´ ì•ˆì „ìœ¼ë¡œ ì²˜ë¦¬
                        violation_str = "ì•ˆì „"
                    elif "ë„˜ì–´ì§" in filtered_violations:
                        other_violations = [v for v in filtered_violations if v != "ë„˜ì–´ì§"]
                        if other_violations:
                            violation_str = f"ë„˜ì–´ì§! {', '.join(other_violations)} ë¯¸ì°©ìš©"
                        else:
                            violation_str = "ë„˜ì–´ì§!"
                    else:
                        violation_str = f"{', '.join(filtered_violations)} ë¯¸ì°©ìš©"
                else:
                    # ìœ„ë°˜ ì—†ìœ¼ë©´ ì•ˆì „
                    violation_str = "ì•ˆì „"
                status_text = f"{display_name}: {violation_str}"
                # ë””ë²„ê¹…: ì‹¤ì œ í‘œì‹œë˜ëŠ” í…ìŠ¤íŠ¸ í™•ì¸
                if name != "Unknown":
                    logging.debug(f"[CAM-{cam_id}] ë¼ë²¨ í‘œì‹œ: {status_text}")
                renderer.add_text(status_text, (x1, y1 - 10), unified_color)
    
    return renderer.render_on(processed_frame)

def _submit_models_background_simple(
    frame: np.ndarray,
    resized_frame: np.ndarray,
    cam_id: int,
    timestamp: float,
    safety_system: Any,
    violation_future: Any,
    pose_future: Any,
    fall_future: Optional[Any],
    face_detection_future: Optional[Any],
    violation_kwargs: Dict,
    pose_kwargs: Dict,
    face_model: Optional[Any],
    face_analyzer: Optional[Any],  # ğŸ¦¬ buffalo_l ì¶”ê°€!
    fast_recognizer: Optional[Any],
    face_database: Optional[Any],
    orig_w: int,
    orig_h: int,
    w_scale: float,
    h_scale: float
):
    """
    í”„ë ˆì„ ë³´ì¥ ë°©ì‹: ëª¨ë“  ëª¨ë¸ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
    """
    # ê³µìœ  ë³€ìˆ˜: Violationê³¼ Pose ê²°ê³¼ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ê¸° ìœ„í•´
    violation_data = {'all_detections': {}, 'ready': False}
    pose_data = {'person_boxes': [], 'ready': False}
    fall_data = {'fall_detections': [], 'ready': False}
    face_data = {'yolo_face_results': None, 'ready': False}
    
    # ë™ê¸°í™”ë¥¼ ìœ„í•œ Lock ìƒì„±
    import threading
    data_lock = threading.Lock()
    
    # ê²°ê³¼ ì½œë°±: ì™„ë£Œë˜ë©´ ìºì‹œì— ì €ì¥
    def save_violation_result(future):
        try:
            violation_results = future.result()  # íƒ€ì„ì•„ì›ƒ ì—†ì´ ì™„ë£Œ ëŒ€ê¸°
            # ê²°ê³¼ íŒŒì‹± (PPE ëª¨ë¸ì˜ Person ë°•ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í†µí•©)
            all_detections = {}
            ppe_person_boxes = []  # PPE ëª¨ë¸ì—ì„œ ê°ì§€ëœ Person ë°•ìŠ¤ ëª©ë¡
            
            if violation_results and len(violation_results) > 0:
                for det in violation_results[0].boxes:
                    class_id = int(det.cls[0])
                    class_name = safety_system.violation_model.names[class_id]
                    conf = float(det.conf[0])
                    
                    if class_name in config.Thresholds.IGNORED_CLASSES:
                        continue
                    
                    # Person í´ë˜ìŠ¤ëŠ” ë³„ë„ë¡œ ì €ì¥ (ê¸°ì¤€ ë°•ìŠ¤ë¡œ ì‚¬ìš©)
                    if class_name == 'Person':
                        person_conf_threshold = config.Thresholds.PERSON_CONFIDENCE
                        if conf >= person_conf_threshold:
                            bbox_resized = det.xyxy[0].cpu().numpy()
                            bbox_original = bbox_resized * np.array([w_scale, h_scale, w_scale, h_scale])
                            bbox_clipped = utils.clip_bbox_xyxy(bbox_original, orig_w, orig_h)
                            if bbox_clipped is not None:
                                # ë°•ìŠ¤ í¬ê¸° ê²€ì¦ (ì†/ì‘ì€ ë¬¼ì²´ ì œì™¸)
                                box_w = bbox_clipped[2] - bbox_clipped[0]
                                box_h = bbox_clipped[3] - bbox_clipped[1]
                                box_area = box_w * box_h
                                box_ratio = box_h / box_w if box_w > 0 else 0  # ì„¸ë¡œ/ê°€ë¡œ ë¹„ìœ¨
                                
                                # â­ ì‚¬ëŒ ì¡°ê±´ ê°•í™”: ìµœì†Œ í¬ê¸° + ì„¸ë¡œë¡œ ê¸´ ë°•ìŠ¤ (ì†/ë¶€ë¶„ ì œì™¸)
                                # - ìµœì†Œ ë„ˆë¹„ 80, ë†’ì´ 120, ë©´ì  15000
                                # - ì„¸ë¡œ/ê°€ë¡œ ë¹„ìœ¨ >= 1.0 (ì‚¬ëŒì€ ë³´í†µ ì„¸ë¡œê°€ ë” ê¹€)
                                is_valid_person = (
                                    box_w >= 80 and 
                                    box_h >= 120 and 
                                    box_area >= 15000 and
                                    box_ratio >= 1.0  # ì„¸ë¡œê°€ ê°€ë¡œë³´ë‹¤ ê¸´ ë°•ìŠ¤ë§Œ
                                )
                                
                                if is_valid_person:
                                    ppe_person_boxes.append({
                                        'bbox': list(bbox_clipped),
                                        'conf': conf,
                                        'class': 'Person'
                                    })
                                else:
                                    logging.debug(f"[CAM-{cam_id}] PPE Person í•„í„°ë§: w={box_w:.0f}, h={box_h:.0f}, area={box_area:.0f}, ratio={box_ratio:.2f}")
                        continue
                    
                    class_threshold = config.Thresholds.CLASS_CONFIDENCE_THRESHOLDS.get(
                        class_name, config.Thresholds.YOLO_CONFIDENCE
                    )
                    
                    if conf >= class_threshold:
                        bbox_resized = det.xyxy[0].cpu().numpy()
                        bbox_original = bbox_resized * np.array([w_scale, h_scale, w_scale, h_scale])
                        bbox_clipped = utils.clip_bbox_xyxy(bbox_original, orig_w, orig_h)
                        if bbox_clipped is not None:
                            if class_name not in all_detections:
                                all_detections[class_name] = []
                            all_detections[class_name].append({'bbox': list(bbox_clipped), 'conf': conf})
            
            # PPE Person ë°•ìŠ¤ ë¡œê¹…
            if ppe_person_boxes:
                logging.debug(f"[CAM-{cam_id}] ğŸ” PPE Person ê°ì§€: {len(ppe_person_boxes)}ëª…")
            
            with data_lock:
                violation_data['all_detections'] = all_detections
                violation_data['ppe_person_boxes'] = ppe_person_boxes  # PPE Person ë°•ìŠ¤ ì €ì¥
                violation_data['ready'] = True
                
                # Pose ê²°ê³¼ë„ ì¤€ë¹„ë˜ì—ˆìœ¼ë©´ violations ìƒì„±
                if pose_data['ready']:
                    _create_violations_from_results(cam_id, timestamp, violation_data, pose_data)
        except Exception as e:
            logging.debug(f"Violation ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def save_pose_result(future):
        try:
            pose_results = future.result()
            # Pose ê²°ê³¼ì—ì„œ ì‚¬ëŒ ë°•ìŠ¤ ì¶”ì¶œ (confidence ë° í¬ê¸° ê²€ì¦)
            person_boxes = []
            if pose_results and len(pose_results) > 0 and pose_results[0].boxes is not None:
                boxes = pose_results[0].boxes.xyxy.cpu().numpy()
                confidences = pose_results[0].boxes.conf.cpu().numpy() if pose_results[0].boxes.conf is not None else None
                keypoints = pose_results[0].keypoints if hasattr(pose_results[0], 'keypoints') and pose_results[0].keypoints is not None else None
                
                # ë””ë²„ê¹…: ì „ì²´ ê°ì§€ëœ ì‚¬ëŒ ìˆ˜ ë¡œê¹…
                logging.debug(f"[CAM-{cam_id}] ğŸ” Pose ëª¨ë¸ ê°ì§€: {len(boxes)}ëª…, conf ë²”ìœ„: {confidences.min():.2f}~{confidences.max():.2f}" if confidences is not None and len(confidences) > 0 else f"[CAM-{cam_id}] ğŸ” Pose ëª¨ë¸ ê°ì§€: {len(boxes)}ëª…")
                
                for idx, box in enumerate(boxes):
                    # Confidence í•„í„°ë§ (ë” ì—„ê²©í•˜ê²Œ)
                    if confidences is not None and len(confidences) > idx:
                        conf = float(confidences[idx])
                        # Pose confidenceë¥¼ ë‚®ì¶°ì„œ ëˆ„ìš´ ì‚¬ëŒë„ ê°ì§€ (0.15 -> 0.10)
                        min_pose_conf = 0.10
                        if conf < min_pose_conf:
                            logging.debug(f"[CAM-{cam_id}] Pose confidence ë‚®ìŒ: {conf:.3f} < {min_pose_conf:.3f}, ì œì™¸")
                            continue
                        
                        # ë””ë²„ê¹…: ê° ë°•ìŠ¤ì˜ ë¹„ìœ¨ ë¡œê¹…
                        box_w_debug = (box[2] - box[0]) * w_scale
                        box_h_debug = (box[3] - box[1]) * h_scale
                        ratio_debug = box_w_debug / box_h_debug if box_h_debug > 0 else 0
                        logging.debug(f"[CAM-{cam_id}] ì‚¬ëŒ {idx}: conf={conf:.2f}, ë°•ìŠ¤ë¹„ìœ¨={ratio_debug:.2f}, í¬ê¸°={box_w_debug:.0f}x{box_h_debug:.0f}")
                    
                    # ë°•ìŠ¤ í¬ê¸° ê²€ì¦
                    box_w = (box[2] - box[0]) * w_scale
                    box_h = (box[3] - box[1]) * h_scale
                    box_area = box_w * box_h
                    
                    # ìµœì†Œ í¬ê¸° ê²€ì¦
                    if box_w < config.Thresholds.MIN_PERSON_BOX_WIDTH or box_h < config.Thresholds.MIN_PERSON_BOX_HEIGHT:
                        continue
                    
                    # ìµœëŒ€ í¬ê¸° ê²€ì¦ (ë„ˆë¬´ í° ë°•ìŠ¤ëŠ” ì‚¬ëŒì´ ì•„ë‹ ê°€ëŠ¥ì„± ë†’ìŒ)
                    # í”„ë ˆì„ì˜ 30% ì´ìƒì„ ì°¨ì§€í•˜ë©´ ì œì™¸ (50% -> 30%ë¡œ ê°•í™”)
                    max_box_area = orig_w * orig_h * 0.3
                    if box_area > max_box_area:
                        logging.debug(f"[CAM-{cam_id}] Pose ë°•ìŠ¤ ë„ˆë¬´ í¼: {box_area:.0f} > {max_box_area:.0f}, ì œì™¸")
                        continue
                    
                    # ë°•ìŠ¤ ë¹„ìœ¨ ê²€ì¦ (ë„˜ì–´ì§ ê°ì§€ë¥¼ ìœ„í•´ ê°€ë¡œë¡œ ê¸´ ë°•ìŠ¤ë„ í—ˆìš©)
                    box_ratio = box_w / box_h if box_h > 0 else 0
                    # ì‚¬ëŒ ë°•ìŠ¤: ì„¸ë¡œë¡œ ê¸´ ê²½ìš° 0.25~1.0, ê°€ë¡œë¡œ ê¸´ ê²½ìš°(ë„˜ì–´ì§) 1.0~3.5
                    # ë„ˆë¬´ ê·¹ë‹¨ì ì¸ ë¹„ìœ¨ë§Œ ì œì™¸ (0.25 ë¯¸ë§Œ ë˜ëŠ” 3.5 ì´ˆê³¼)
                    is_horizontal_pose = box_ratio > 1.2  # ê°€ë¡œë¡œ ê¸´ ë°•ìŠ¤ (ë„˜ì–´ì§ ê°€ëŠ¥ì„±)
                    if box_ratio < 0.25 or box_ratio > 3.5:
                        logging.debug(f"[CAM-{cam_id}] Pose ë°•ìŠ¤ ë¹„ìœ¨ ì´ìƒ: {box_ratio:.2f}, ì œì™¸")
                        continue
                    
                    # â­ ë°œ/ì‹ ë°œ í•„í„°ë§: ê°€ë¡œë¡œ ê¸´ ë°•ìŠ¤(ë„˜ì–´ì§)ë„ ìµœì†Œ ë†’ì´ 130px ì´ìƒ
                    if is_horizontal_pose and box_h < 130:
                        logging.debug(f"[CAM-{cam_id}] Pose ë°•ìŠ¤ í•„í„°ë§ (ë°œ/ì‹ ë°œ ì˜ì‹¬): ê°€ë¡œë¡œ ê¸´ ë°•ìŠ¤ì¸ë° h={box_h:.0f} < 130")
                        continue
                    
                    # ê°€ë¡œë¡œ ê¸´ ë°•ìŠ¤(ë„˜ì–´ì§ í›„ë³´)ëŠ” ë³„ë„ í”Œë˜ê·¸ ì„¤ì •
                    if is_horizontal_pose:
                        logging.info(f"[CAM-{cam_id}] ğŸ”» ë„˜ì–´ì§ í›„ë³´ ê°ì§€: ë°•ìŠ¤ ë¹„ìœ¨={box_ratio:.2f} (ê°€ë¡œë¡œ ê¸´ ë°•ìŠ¤)")
                    
                    # ìµœì†Œ ë°•ìŠ¤ ë©´ì  ê²€ì¦ (ë„ˆë¬´ ì‘ì€ ë°•ìŠ¤ëŠ” ë…¸ì´ì¦ˆì¼ ê°€ëŠ¥ì„±)
                    min_box_area = config.Thresholds.MIN_PERSON_BOX_WIDTH * config.Thresholds.MIN_PERSON_BOX_HEIGHT * 1.5
                    if box_area < min_box_area:
                        logging.debug(f"[CAM-{cam_id}] Pose ë°•ìŠ¤ ë„ˆë¬´ ì‘ìŒ: {box_area:.0f} < {min_box_area:.0f}, ì œì™¸")
                        continue
                    
                    # í‚¤í¬ì¸íŠ¸ ê²€ì¦ (í‚¤í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ìµœì†Œ ê°œìˆ˜ í™•ì¸, ì—†ì–´ë„ confidenceê°€ ë†’ìœ¼ë©´ í—ˆìš©)
                    keypoint_valid = False
                    person_keypoints = None  # ì“°ëŸ¬ì§ ê°ì§€ìš© í‚¤í¬ì¸íŠ¸ ì €ì¥
                    
                    # ë””ë²„ê¹…: í‚¤í¬ì¸íŠ¸ ê°ì²´ ìƒíƒœ í™•ì¸
                    if keypoints is None:
                        logging.debug(f"[CAM-{cam_id}] ğŸ” Pose í‚¤í¬ì¸íŠ¸ ê°ì²´ ì—†ìŒ (keypoints is None)")
                    else:
                        logging.debug(f"[CAM-{cam_id}] ğŸ” Pose í‚¤í¬ì¸íŠ¸ ê°ì²´ ì¡´ì¬: type={type(keypoints)}, "
                                    f"has_xy={hasattr(keypoints, 'xy')}, has_conf={hasattr(keypoints, 'conf')}")
                    
                    if keypoints is not None:
                        try:
                            kpts = keypoints.xy[idx] if hasattr(keypoints, 'xy') and keypoints.xy is not None else None
                            kpts_conf = None
                            if hasattr(keypoints, 'conf') and keypoints.conf is not None:
                                if isinstance(keypoints.conf, (list, tuple)) and len(keypoints.conf) > idx:
                                    kpts_conf = keypoints.conf[idx]
                                elif hasattr(keypoints.conf, '__getitem__'):
                                    try:
                                        kpts_conf = keypoints.conf[idx]
                                    except (IndexError, TypeError):
                                        pass
                            
                            if kpts_conf is not None:
                                # numpy ë°°ì—´ë¡œ ë³€í™˜
                                if not isinstance(kpts_conf, np.ndarray):
                                    try:
                                        kpts_conf = kpts_conf.cpu().numpy() if hasattr(kpts_conf, 'cpu') else np.array(kpts_conf)
                                    except:
                                        kpts_conf = None
                                
                                if kpts_conf is not None and kpts_conf.size > 0:
                                    # ìµœì†Œ í‚¤í¬ì¸íŠ¸ ê°œìˆ˜ í™•ì¸ (confidence > 0.3ì¸ í‚¤í¬ì¸íŠ¸, 0.5 -> 0.3ìœ¼ë¡œ ì™„í™”)
                                    visible_kpts = int(np.sum(kpts_conf > 0.3))
                                    avg_conf = float(np.mean(kpts_conf))
                                    logging.debug(f"[CAM-{cam_id}] ğŸ” ì‚¬ëŒ {idx} í‚¤í¬ì¸íŠ¸: visible={visible_kpts}/17, avg_conf={avg_conf:.3f}, min_required={config.Thresholds.MIN_VISIBLE_KEYPOINTS}")
                                    
                                    if visible_kpts >= config.Thresholds.MIN_VISIBLE_KEYPOINTS:
                                        keypoint_valid = True
                                        # ì“°ëŸ¬ì§ ê°ì§€ìš© í‚¤í¬ì¸íŠ¸ ì €ì¥
                                        if kpts is not None:
                                            kpts_np = kpts.cpu().numpy() if hasattr(kpts, 'cpu') else np.array(kpts)
                                            person_keypoints = {
                                                'xy': kpts_np,
                                                'conf': kpts_conf,
                                                'visible_count': visible_kpts
                                            }
                                            logging.debug(f"[CAM-{cam_id}] âœ… ì‚¬ëŒ {idx} í‚¤í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {visible_kpts}ê°œ")
                        except Exception as e:
                            logging.debug(f"[CAM-{cam_id}] í‚¤í¬ì¸íŠ¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
                    
                    # â­ í‚¤í¬ì¸íŠ¸ê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ë” ì—„ê²©í•œ ê¸°ì¤€ ì ìš© (ì†/ë¶€ë¶„ ê°ì§€ ë°©ì§€)
                    if not keypoint_valid:
                        # í‚¤í¬ì¸íŠ¸ ì—†ìœ¼ë©´ ë¬´ì¡°ê±´ ì œì™¸! (ì†ì´ë‚˜ ë¶€ë¶„ë§Œ ì¡íˆëŠ” ê²ƒ ë°©ì§€)
                        logging.debug(f"[CAM-{cam_id}] Pose í‚¤í¬ì¸íŠ¸ ë¶€ì¡± ({config.Thresholds.MIN_VISIBLE_KEYPOINTS}ê°œ ë¯¸ë§Œ), ì œì™¸")
                        continue
                    
                    scaled_box_np = box * np.array([w_scale, h_scale, w_scale, h_scale])
                    clipped_box = utils.clip_bbox_xyxy(scaled_box_np, orig_w, orig_h)
                    if clipped_box is not None:
                        # í‚¤í¬ì¸íŠ¸ ì •ë³´ë„ í•¨ê»˜ ì €ì¥ (ì“°ëŸ¬ì§ ê°ì§€ìš©)
                        if person_keypoints is not None:
                            person_boxes.append({
                                'box': clipped_box,
                                'keypoints': person_keypoints,
                                'confidence': float(confidences[idx]) if confidences is not None and len(confidences) > idx else 0.0
                            })
                        else:
                            person_boxes.append({
                                'box': clipped_box,
                                'keypoints': None,
                                'confidence': float(confidences[idx]) if confidences is not None and len(confidences) > idx else 0.0
                            })
            
            with data_lock:
                pose_data['person_boxes'] = person_boxes
                pose_data['ready'] = True
                
                # Violation ê²°ê³¼ë„ ì¤€ë¹„ë˜ì—ˆìœ¼ë©´ violations ìƒì„±
                if violation_data['ready']:
                    _create_violations_from_results(cam_id, timestamp, violation_data, pose_data)
        except Exception as e:
            logging.debug(f"Pose ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def save_face_result(future):
        try:
            # â­ buffalo_lë¡œ Person í¬ë¡­ ë°©ì‹ ì–¼êµ´ ì¸ì‹
            # ì „ì²´ í”„ë ˆì„ ëŒ€ì‹  Person ë°•ìŠ¤ë¡œ í¬ë¡­í•´ì„œ ë” ì •í™•í•˜ê²Œ ì¸ì‹
            
            # ğŸ” ë””ë²„ê·¸: face_analyzerì™€ face_database ìƒíƒœ í™•ì¸
            logging.debug(f"[CAM-{cam_id}] ğŸ” save_face_result ì§„ì…: face_analyzer={face_analyzer is not None}, face_database={face_database is not None}")
            
            with data_lock:
                face_data['ready'] = True
            
            recognized_faces = []
            
            # ğŸ¦¬ buffalo_l Person í¬ë¡­ ë°©ì‹ (ë” ì •í™•í•œ ì¸ì‹!)
            if face_analyzer is None:
                logging.warning(f"[CAM-{cam_id}] âš ï¸ face_analyzerê°€ Noneì…ë‹ˆë‹¤!")
            if face_database is None:
                logging.warning(f"[CAM-{cam_id}] âš ï¸ face_databaseê°€ Noneì…ë‹ˆë‹¤!")
            
            if face_analyzer is not None and face_database is not None:
                try:
                    # Pose ë°ì´í„°ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸° (ìµœëŒ€ 100ms)
                    wait_start = time.time()
                    while not pose_data.get('ready', False) and (time.time() - wait_start) < 0.1:
                        time.sleep(0.01)
                    
                    # Person ë°•ìŠ¤ ê°€ì ¸ì˜¤ê¸°
                    person_boxes = pose_data.get('person_boxes', [])
                    
                    if not person_boxes:
                        # Personì´ ì—†ìœ¼ë©´ ì „ì²´ í”„ë ˆì„ìœ¼ë¡œ í´ë°±
                        logging.debug(f"[CAM-{cam_id}] ğŸ” frame ìƒíƒœ: shape={frame.shape if frame is not None else 'None'}, dtype={frame.dtype if frame is not None else 'None'}")
                        faces = face_analyzer.get(frame)
                        logging.debug(f"[CAM-{cam_id}] ğŸ¦¬ buffalo_l (ì „ì²´ í”„ë ˆì„): {len(faces)}ê°œ ì–¼êµ´")
                        
                        for face in faces:
                            try:
                                # â­ det_score ìµœì†Œ ì„ê³„ê°’ ì²´í¬ (ì˜¤íƒì§€ ë°©ì§€: ì†ë°”ë‹¥ ë“±)
                                # 0.5 â†’ 0.4ë¡œ ë‚®ì¶¤ (ëˆ„ìš´ ìƒíƒœ ì–¼êµ´ ì¸ì‹ë¥  í–¥ìƒ)
                                MIN_FACE_DET_SCORE = 0.4
                                if face.det_score < MIN_FACE_DET_SCORE:
                                    logging.debug(f"[CAM-{cam_id}] ì–¼êµ´ ì‹ ë¢°ë„ ë¶€ì¡±: {face.det_score:.3f} < {MIN_FACE_DET_SCORE} - ìŠ¤í‚µ")
                                    continue
                                
                                bbox = face.bbox.astype(int)
                                fx1, fy1, fx2, fy2 = bbox[0], bbox[1], bbox[2], bbox[3]
                                fx1 = max(0, min(fx1, orig_w))
                                fy1 = max(0, min(fy1, orig_h))
                                fx2 = max(0, min(fx2, orig_w))
                                fy2 = max(0, min(fy2, orig_h))
                                
                                # ìµœì†Œ í¬ê¸° ì²´í¬ (40x40 ì´ìƒ)
                                if (fx2 - fx1) < 40 or (fy2 - fy1) < 40:
                                    logging.debug(f"[CAM-{cam_id}] ì–¼êµ´ í¬ê¸° ë¶€ì¡±: {fx2-fx1}x{fy2-fy1} < 40x40 - ìŠ¤í‚µ")
                                    continue
                                
                                embedding = face.embedding
                                if embedding is None:
                                    continue
                                embedding = embedding / np.linalg.norm(embedding)
                                
                                person_name, similarity_score = utils.find_best_match_faiss(
                                    embedding, face_database, config.Thresholds.SIMILARITY
                                )
                                
                                if person_name != "Unknown":
                                    logging.info(f"[CAM-{cam_id}] âœ… ğŸ¦¬ buffalo_l ì¸ì‹: {person_name}, ìœ ì‚¬ë„={similarity_score:.3f}")
                                
                                recognized_faces.append({
                                    "box": [fx1, fy1, fx2, fy2],
                                    "bbox": [fx1, fy1, fx2, fy2],
                                    "name": person_name,
                                    "similarity": similarity_score,
                                    "isViolation": False,
                                    "ppe_violations": []
                                })
                            except Exception as e:
                                continue
                    else:
                        # â­ Person í¬ë¡­ ë°©ì‹ (ë” ì •í™•!)
                        logging.debug(f"[CAM-{cam_id}] ğŸ¦¬ buffalo_l Person í¬ë¡­: {len(person_boxes)}ëª… ì²˜ë¦¬")
                        
                        # ğŸ”’ ì´ë¯¸ ì‚¬ìš©ëœ ì–¼êµ´ ì¢Œí‘œ ì¶”ì  (ì¤‘ë³µ í• ë‹¹ ë°©ì§€)
                        used_face_centers = set()
                        
                        for person_info in person_boxes:
                            try:
                                # Person ë°•ìŠ¤ ì¶”ì¶œ
                                if isinstance(person_info, dict):
                                    person_box = person_info.get('box', person_info.get('bbox', []))
                                else:
                                    person_box = list(person_info)
                                
                                if len(person_box) < 4:
                                    continue
                                
                                px1, py1, px2, py2 = int(person_box[0]), int(person_box[1]), int(person_box[2]), int(person_box[3])
                                
                                # ê²½ê³„ ì²´í¬
                                px1 = max(0, min(px1, orig_w))
                                py1 = max(0, min(py1, orig_h))
                                px2 = max(0, min(px2, orig_w))
                                py2 = max(0, min(py2, orig_h))
                                
                                # Person í¬ê¸° ì²´í¬ (ìµœì†Œ 80x120)
                                if (px2 - px1) < 80 or (py2 - py1) < 120:
                                    continue
                                
                                # Person í¬ë¡­
                                person_crop = frame[py1:py2, px1:px2]
                                if person_crop.size == 0:
                                    continue
                                
                                # í¬ë¡­ëœ ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê°ì§€
                                faces = face_analyzer.get(person_crop)
                                
                                if len(faces) == 0:
                                    continue
                                
                                # â­ det_score ìµœì†Œ ì„ê³„ê°’ìœ¼ë¡œ í•„í„°ë§ (ì˜¤íƒì§€ ë°©ì§€)
                                # 0.5 â†’ 0.4ë¡œ ë‚®ì¶¤ (ëˆ„ìš´ ìƒíƒœ ì–¼êµ´ ì¸ì‹ë¥  í–¥ìƒ)
                                MIN_FACE_DET_SCORE = 0.4
                                valid_faces = [f for f in faces if f.det_score >= MIN_FACE_DET_SCORE]
                                
                                if len(valid_faces) == 0:
                                    logging.debug(f"[CAM-{cam_id}] Person í¬ë¡­ ë‚´ ìœ íš¨ ì–¼êµ´ ì—†ìŒ (det_score < {MIN_FACE_DET_SCORE})")
                                    continue
                                
                                # ê°€ì¥ í° ì–¼êµ´ ì„ íƒ (det_score ê¸°ì¤€)
                                best_face = max(valid_faces, key=lambda f: f.det_score)
                                
                                # ìµœì†Œ ì–¼êµ´ í¬ê¸° ì²´í¬ (í¬ë¡­ ë‚´ì—ì„œ 20x20 ì´ìƒ, ì™„í™”ë¨)
                                face_bbox = best_face.bbox.astype(int)
                                face_w = face_bbox[2] - face_bbox[0]
                                face_h = face_bbox[3] - face_bbox[1]
                                if face_w < 20 or face_h < 20:
                                    logging.debug(f"[CAM-{cam_id}] Person í¬ë¡­ ë‚´ ì–¼êµ´ í¬ê¸° ë¶€ì¡±: {face_w}x{face_h} < 20x20")
                                    continue
                                
                                # í¬ë¡­ ë‚´ ì¢Œí‘œë¥¼ ì›ë³¸ í”„ë ˆì„ ì¢Œí‘œë¡œ ë³€í™˜
                                bbox = best_face.bbox.astype(int)
                                fx1 = px1 + bbox[0]
                                fy1 = py1 + bbox[1]
                                fx2 = px1 + bbox[2]
                                fy2 = py1 + bbox[3]
                                
                                # ğŸ”’ ì–¼êµ´ ì¤‘ì‹¬ì´ Person ë°•ìŠ¤ ì¤‘ì•™ ì˜ì—­ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸ (ì´ë¦„ ëºì–´ê° ë°©ì§€)
                                face_center_x = (fx1 + fx2) // 2
                                face_center_y = (fy1 + fy2) // 2
                                
                                # Person ë°•ìŠ¤ì˜ ì¤‘ì•™ 80% ì˜ì—­ ê³„ì‚°
                                person_w = px2 - px1
                                person_h = py2 - py1
                                margin_x = int(person_w * 0.1)  # ì¢Œìš° 10% ë§ˆì§„
                                margin_y = int(person_h * 0.1)  # ìƒí•˜ 10% ë§ˆì§„
                                
                                inner_px1 = px1 + margin_x
                                inner_py1 = py1 + margin_y
                                inner_px2 = px2 - margin_x
                                inner_py2 = py2 - margin_y
                                
                                # ì–¼êµ´ ì¤‘ì‹¬ì´ ì¤‘ì•™ ì˜ì—­ ë°–ì´ë©´ ê±´ë„ˆë›°ê¸°
                                if not (inner_px1 <= face_center_x <= inner_px2 and inner_py1 <= face_center_y <= inner_py2):
                                    logging.debug(f"[CAM-{cam_id}] ì–¼êµ´ ì¤‘ì‹¬ì´ Person ë°•ìŠ¤ ê°€ì¥ìë¦¬ì— ìˆìŒ - ìŠ¤í‚µ (ì´ë¦„ ëºì–´ê° ë°©ì§€)")
                                    continue
                                
                                # ğŸ”’ ì´ë¯¸ ì‚¬ìš©ëœ ì–¼êµ´ì¸ì§€ ì²´í¬ (ì¤‘ë³µ í• ë‹¹ ë°©ì§€)
                                # ì–¼êµ´ ì¤‘ì‹¬ì„ 50í”½ì…€ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™” (ê°™ì€ ì–¼êµ´ íŒì •)
                                face_center_key = (face_center_x // 50, face_center_y // 50)
                                if face_center_key in used_face_centers:
                                    logging.debug(f"[CAM-{cam_id}] ì´ë¯¸ ì‚¬ìš©ëœ ì–¼êµ´ - ìŠ¤í‚µ (ì¤‘ë³µ í• ë‹¹ ë°©ì§€): center={face_center_key}")
                                    continue
                                used_face_centers.add(face_center_key)
                                
                                # ê²½ê³„ ì²´í¬
                                fx1 = max(0, min(fx1, orig_w))
                                fy1 = max(0, min(fy1, orig_h))
                                fx2 = max(0, min(fx2, orig_w))
                                fy2 = max(0, min(fy2, orig_h))
                                
                                # ì„ë² ë”©
                                embedding = best_face.embedding
                                if embedding is None:
                                    continue
                                embedding = embedding / np.linalg.norm(embedding)
                                
                                # FAISS ê²€ìƒ‰
                                person_name, similarity_score = utils.find_best_match_faiss(
                                    embedding, face_database, config.Thresholds.SIMILARITY
                                )
                                
                                if person_name != "Unknown":
                                    logging.info(f"[CAM-{cam_id}] âœ… ğŸ¦¬ buffalo_l í¬ë¡­ ì¸ì‹: {person_name}, ìœ ì‚¬ë„={similarity_score:.3f}")
                                
                                # Person ë°•ìŠ¤ë„ í•¨ê»˜ ì €ì¥ (ë§¤ì¹­ìš©)
                                recognized_faces.append({
                                    "box": [fx1, fy1, fx2, fy2],
                                    "bbox": [fx1, fy1, fx2, fy2],
                                    "person_box": [px1, py1, px2, py2],
                                    "name": person_name,
                                    "similarity": similarity_score,
                                    "isViolation": False,
                                    "ppe_violations": []
                                })
                            except Exception as e:
                                logging.debug(f"[CAM-{cam_id}] Person í¬ë¡­ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                                continue
                                
                except Exception as e:
                    logging.debug(f"[CAM-{cam_id}] buffalo_l ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            with results_cache_lock:
                merged = False
                for ts, rd in model_results_cache[cam_id]:
                    # íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­ ì •ë°€ë„: 0.1ì´ˆ (ì§€ì—° í—ˆìš© ë²”ìœ„ í™•ëŒ€)
                    if abs(ts - timestamp) < 0.1:
                        # ê¸°ì¡´ recognized_facesì™€ ë³‘í•©
                        if 'recognized_faces' not in rd:
                            rd['recognized_faces'] = []
                        existing_faces = rd.get('recognized_faces', [])
                        
                        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ IoU ê¸°ë°˜ìœ¼ë¡œ í™•ì¸ (ë°•ìŠ¤ ì¢Œí‘œ ì •í™• ì¼ì¹˜ê°€ ì•„ë‹Œ IoU ì‚¬ìš©)
                        new_faces = []
                        for new_face in recognized_faces:
                            new_box = new_face.get('box', [])
                            if len(new_box) != 4:
                                continue
                            new_box_tuple = tuple(map(int, new_box))
                            is_duplicate = False
                            for existing_face in existing_faces:
                                existing_box = existing_face.get('box', [])
                                if len(existing_box) != 4:
                                    continue
                                existing_box_tuple = tuple(map(int, existing_box))
                                # IoU ê¸°ë°˜ ì¤‘ë³µ í™•ì¸ (0.5 ì´ìƒì´ë©´ ê°™ì€ ì‚¬ëŒ)
                                iou = utils.calculate_iou(new_box_tuple, existing_box_tuple)
                                if iou > 0.5:
                                    is_duplicate = True
                                    break
                            if not is_duplicate:
                                new_faces.append(new_face)
                        
                        rd['recognized_faces'].extend(new_faces)
                        merged = True
                        break
                if not merged:
                    model_results_cache[cam_id].append((timestamp, {'recognized_faces': recognized_faces}))
            logging.debug(f"[CAM-{cam_id}] ë°±ê·¸ë¼ìš´ë“œ Face ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(recognized_faces)}ê°œ")
        except Exception as e:
            logging.debug(f"Face ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _create_violations_from_results(cam_id, timestamp, violation_data, pose_data):
        """Violationê³¼ Pose ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ violations ìƒì„± (Pose ë°•ìŠ¤ ê¸°ë³¸ ì‚¬ìš©)"""
        try:
            all_detections = violation_data['all_detections']
            
            # Pose + PPE Person ë°•ìŠ¤ í†µí•© (ë„˜ì–´ì§„ ì‚¬ëŒ ê°ì§€ ê°•í™”)
            pose_person_boxes = pose_data['person_boxes']
            ppe_person_boxes = violation_data.get('ppe_person_boxes', [])
            
            # 1. Pose ë°•ìŠ¤ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
            person_boxes = list(pose_person_boxes) if pose_person_boxes else []
            
            # 2. PPE Person ë°•ìŠ¤ ì¤‘ Poseì— ì—†ëŠ” ê²ƒ ì¶”ê°€ (ë„˜ì–´ì§„ ì‚¬ëŒ ê°ì§€)
            for ppe_person in ppe_person_boxes:
                ppe_bbox = ppe_person['bbox']
                is_new = True
                
                # Pose ë°•ìŠ¤ì™€ IoU ë¹„êµ
                for pose_box in pose_person_boxes:
                    if isinstance(pose_box, dict):
                        pose_bbox = pose_box.get('box', pose_box)
                    else:
                        pose_bbox = pose_box
                    
                    iou = utils.calculate_iou(tuple(ppe_bbox), tuple(pose_bbox))
                    if iou > 0.3:  # 30% ì´ìƒ ê²¹ì¹˜ë©´ ê°™ì€ ì‚¬ëŒ
                        is_new = False
                        break
                
                if is_new:
                    # ë„˜ì–´ì§„ ì‚¬ëŒ í›„ë³´ (Poseê°€ ëª» ì¡ì€ ì‚¬ëŒ)
                    box_w = ppe_bbox[2] - ppe_bbox[0]
                    box_h = ppe_bbox[3] - ppe_bbox[1]
                    box_area = box_w * box_h
                    box_ratio = box_w / box_h if box_h > 0 else 0
                    
                    # ===== ì†ë°”ë‹¥/ë¶€ë¶„ ê°ì§€ í•„í„°ë§ ê°•í™” =====
                    # 1. ìµœì†Œ ë©´ì  ì¡°ê±´: 20000 í”½ì…€ ì´ìƒ (15000 -> 20000, ë°œ/ì‹ ë°œ ì œì™¸ ê°•í™”)
                    if box_area < 20000:
                        logging.debug(f"[CAM-{cam_id}] PPE Person í•„í„°ë§ (ë©´ì  ë¶€ì¡±): area={box_area:.0f} < 20000")
                        continue
                    
                    # 2. ìµœì†Œ ì„¸ë¡œ í¬ê¸°: ì‚¬ëŒì€ ì„¸ë¡œê°€ ê¸¸ì–´ì•¼ í•¨ (ì†ë°”ë‹¥ ì œì™¸)
                    # ë„˜ì–´ì§„ ì‚¬ëŒ(ê°€ë¡œë¡œ ê¸´ ê²½ìš°)ì€ ì˜ˆì™¸ ì²˜ë¦¬
                    MIN_PPE_PERSON_HEIGHT = 120  # ìµœì†Œ ì„¸ë¡œ 120px
                    MIN_PPE_PERSON_WIDTH = 50    # ìµœì†Œ ê°€ë¡œ 50px
                    
                    if box_ratio < 1.2:  # ì„œìˆëŠ” ì‚¬ëŒ (ì„¸ë¡œê°€ ë” ê¸´ ê²½ìš°)
                        if box_h < MIN_PPE_PERSON_HEIGHT:
                            logging.debug(f"[CAM-{cam_id}] PPE Person í•„í„°ë§ (ì„¸ë¡œ ë¶€ì¡± - ì†ë°”ë‹¥ ì˜ì‹¬): h={box_h:.0f} < {MIN_PPE_PERSON_HEIGHT}")
                            continue
                        if box_w < MIN_PPE_PERSON_WIDTH:
                            logging.debug(f"[CAM-{cam_id}] PPE Person í•„í„°ë§ (ê°€ë¡œ ë¶€ì¡±): w={box_w:.0f} < {MIN_PPE_PERSON_WIDTH}")
                            continue
                    else:  # ë„˜ì–´ì§„ ì‚¬ëŒ (ê°€ë¡œê°€ ë” ê¸´ ê²½ìš°)
                        # ë„˜ì–´ì§ì€ ê°€ë¡œê°€ ê¸¸ì–´ì•¼ í•¨
                        if box_w < MIN_PPE_PERSON_HEIGHT:  # ê°€ë¡œê°€ ìµœì†Œ 120px
                            logging.debug(f"[CAM-{cam_id}] PPE Person í•„í„°ë§ (ë„˜ì–´ì§ í›„ë³´ ê°€ë¡œ ë¶€ì¡±): w={box_w:.0f} < {MIN_PPE_PERSON_HEIGHT}")
                            continue
                        # â­ ë°œ/ì‹ ë°œ í•„í„°ë§: ë„˜ì–´ì§„ ì‚¬ëŒë„ ìµœì†Œ ë†’ì´ 130px ì´ìƒ (ë°œ/ì‹ ë°œ ì œì™¸ ê°•í™”)
                        MIN_FALL_HEIGHT = 130
                        if box_h < MIN_FALL_HEIGHT:
                            logging.debug(f"[CAM-{cam_id}] PPE Person í•„í„°ë§ (ë„˜ì–´ì§ ë†’ì´ ë¶€ì¡± - ë°œ/ì‹ ë°œ ì˜ì‹¬): h={box_h:.0f} < {MIN_FALL_HEIGHT}")
                            continue
                    
                    # 3. ë¹„ìœ¨ ê²€ì¦: ë„ˆë¬´ ì •ì‚¬ê°í˜•ì— ê°€ê¹Œìš°ë©´ ì†ë°”ë‹¥ ì˜ì‹¬ (0.7~1.3 ë²”ìœ„)
                    if 0.7 <= box_ratio <= 1.3 and box_area < 30000:
                        logging.debug(f"[CAM-{cam_id}] PPE Person í•„í„°ë§ (ì •ì‚¬ê°í˜• - ì†ë°”ë‹¥ ì˜ì‹¬): ratio={box_ratio:.2f}, area={box_area:.0f}")
                        continue
                    
                    person_boxes.append({
                        'box': ppe_bbox, 
                        'source': 'ppe',
                        'is_fall_candidate': box_ratio >= 1.2  # ê°€ë¡œë¡œ ê¸´ ë°•ìŠ¤ëŠ” ë„˜ì–´ì§ í›„ë³´
                    })
                    
                    if box_ratio >= 1.2:
                        logging.warning(f"[CAM-{cam_id}] ğŸ”» ë„˜ì–´ì§ í›„ë³´ (PPE Person): ë¹„ìœ¨={box_ratio:.2f}, ë°•ìŠ¤={ppe_bbox}, ë©´ì ={box_area:.0f}")
                    else:
                        logging.debug(f"[CAM-{cam_id}] PPE Person ì¶”ê°€: ë¹„ìœ¨={box_ratio:.2f}, ë©´ì ={box_area:.0f}, w={box_w:.0f}, h={box_h:.0f}")
            
            logging.debug(f"[CAM-{cam_id}] í†µí•© Person ë°•ìŠ¤: Pose={len(pose_person_boxes)}ê°œ + PPE={len(ppe_person_boxes)}ê°œ â†’ ì´ {len(person_boxes)}ê°œ")
            
            violations = []
            # recognized_facesëŠ” ê¸°ì¡´ ê²°ê³¼ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ
            # ëŒ€ì‹  ìœ„ë°˜ ì •ë³´ë§Œ ì¶”ê°€í•  ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            new_recognized_faces = []  # ìœ„ë°˜ ì •ë³´ë§Œ ì¶”ê°€í•  ë¦¬ìŠ¤íŠ¸
            used_ppe_boxes = set()
            used_face_indices = set()  # â­ ì´ë¯¸ ë§¤ì¹­ëœ ì–¼êµ´ ì¸ë±ìŠ¤ ì¶”ì  (ì¤‘ë³µ ë§¤ì¹­ ë°©ì§€)
            
            # ê¸°ì¡´ ì–¼êµ´ ì¸ì‹ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ìµœê·¼ 0.5ì´ˆ ì´ë‚´ + ì˜ì—­ í¬í•¨ ê¸°ë°˜ ë§¤ì¹­)
            # â­ ì–¼êµ´ ë°•ìŠ¤ ì¢Œí‘œë„ í•¨ê»˜ ì €ì¥í•˜ì—¬ Person ë°•ìŠ¤ ë‚´ë¶€ í¬í•¨ ì—¬ë¶€ í™•ì¸
            existing_faces_list = []  # [(center_x, center_y, name, index, face_box), ...]
            current_time = time.time()
            face_index = 0  # ì–¼êµ´ ì¸ë±ìŠ¤
            with results_cache_lock:
                if cam_id in model_results_cache:
                    for ts, rd in reversed(model_results_cache[cam_id]):
                        if current_time - ts <= 1.5:  # 1.5ì´ˆ ì´ë‚´ ê²°ê³¼ ì‚¬ìš© (ì–¼êµ´ ì¸ì‹ íƒ€ì´ë° ê°œì„ : 0.5 -> 1.5ì´ˆ)
                            for face in rd.get('recognized_faces', []):
                                box = face.get('box', [])
                                name = face.get('name', 'Unknown')
                                if len(box) == 4 and name != 'Unknown':
                                    cx = (box[0] + box[2]) / 2
                                    cy = (box[1] + box[3]) / 2
                                    # â­ ì–¼êµ´ ë°•ìŠ¤ ì¢Œí‘œë„ í•¨ê»˜ ì €ì¥
                                    existing_faces_list.append((cx, cy, name, face_index, box))
                                    face_index += 1
                        else:
                            break
            
            if existing_faces_list:
                logging.debug(f"[CAM-{cam_id}] ê¸°ì¡´ ì–¼êµ´ ì¸ì‹ ê²°ê³¼: {len(existing_faces_list)}ëª…")
            
            # â­â­ í˜„ì¬ í”„ë ˆì„ì—ì„œ buffalo_lì´ ê°ì§€í•œ ì–¼êµ´ ë°•ìŠ¤ ëª©ë¡ (ë’·ëª¨ìŠµ ì´ë¦„ í• ë‹¹ ë°©ì§€)
            # save_face_resultì—ì„œ ì €ì¥í•œ ìµœì‹  ì–¼êµ´ ê²°ê³¼ë§Œ ì‚¬ìš©
            current_frame_faces = []  # [(face_box, name), ...]
            with results_cache_lock:
                if cam_id in model_results_cache and model_results_cache[cam_id]:
                    latest_ts, latest_rd = model_results_cache[cam_id][-1]
                    # 1.5ì´ˆ ì´ë‚´ì˜ ìµœì‹  ê²°ê³¼ ì‚¬ìš© (ì–¼êµ´ ì¸ì‹ íƒ€ì´ë° ê°œì„ : 0.3 -> 1.5ì´ˆ)
                    if current_time - latest_ts <= 1.5:
                        for face in latest_rd.get('recognized_faces', []):
                            box = face.get('box', [])
                            name = face.get('name', 'Unknown')
                            if len(box) == 4 and name != 'Unknown':
                                current_frame_faces.append((box, name))
            
            logging.debug(f"[CAM-{cam_id}] í˜„ì¬ í”„ë ˆì„ ì–¼êµ´: {len(current_frame_faces)}ëª…")
            
            # ê° ì‚¬ëŒ ë°•ìŠ¤ì— ëŒ€í•´ PPE ìœ„ë°˜ í™•ì¸
            for person_box in person_boxes:
                # person_boxê°€ dictì¸ ê²½ìš° (í‚¤í¬ì¸íŠ¸ ì •ë³´ í¬í•¨) ì²˜ë¦¬
                is_fall_candidate = False
                if isinstance(person_box, dict):
                    box = person_box.get('box', person_box)
                    x1, y1, x2, y2 = map(int, box)
                    is_fall_candidate = person_box.get('is_fall_candidate', False)
                else:
                    x1, y1, x2, y2 = map(int, person_box)
                
                # â­ ë°•ìŠ¤ ë¹„ìœ¨ë¡œ ë„˜ì–´ì§ ì§ì ‘ ê°ì§€ (ê°€ë¡œ > ì„¸ë¡œ = ì“°ëŸ¬ì§)
                box_width = x2 - x1
                box_height = y2 - y1
                box_area = box_width * box_height
                box_ratio = box_width / box_height if box_height > 0 else 0
                # ë„˜ì–´ì§ ì¡°ê±´: ë¹„ìœ¨ >= 1.8 AND ë©´ì  >= 15000 (ì†/ì‘ì€ ë¬¼ì²´ ì œì™¸)
                if box_ratio >= 1.8 and box_area >= 15000:
                    is_fall_candidate = True
                    logging.warning(f"[CAM-{cam_id}] ğŸ”» ë°•ìŠ¤ ë¹„ìœ¨ ê¸°ë°˜ ë„˜ì–´ì§ ê°ì§€ (ë³´ì¡°): ratio={box_ratio:.2f}, area={box_area}")
                
                ppe_violations, ppe_boxes = _process_ppe_detection(
                    (x1, y1, x2, y2), 
                    all_detections, 
                    used_ppe_boxes
                )
                
                # ë„˜ì–´ì§ í›„ë³´ë©´ ìœ„ë°˜ì— ì¶”ê°€
                if is_fall_candidate:
                    if 'ë„˜ì–´ì§' not in ppe_violations:
                        ppe_violations.append('ë„˜ì–´ì§')
                        logging.warning(f"[CAM-{cam_id}] âš ï¸ ë„˜ì–´ì§ ìœ„ë°˜ ì¶”ê°€ (ë°•ìŠ¤ ë¹„ìœ¨={box_ratio:.2f})")
                
                # ë§ˆìŠ¤í¬ ì œì™¸í•œ ìœ„ë°˜ë§Œ ì²´í¬
                filtered_violations = [v for v in ppe_violations if v != "ë§ˆìŠ¤í¬"]
                
                # cam_idë¥¼ areaë¡œ ë§¤í•‘
                area_map = {0: "A-1", 1: "A-2", 2: "B-1", 3: "B-2"}
                area = area_map.get(cam_id, f"A-{cam_id+1}")
                
                # ê¸°ì¡´ ì–¼êµ´ ì¸ì‹ ê²°ê³¼ì—ì„œ ì´ë¦„ ì°¾ê¸° (ì˜ì—­ í¬í•¨ + ê±°ë¦¬ ê¸°ë°˜ ë§¤ì¹­)
                person_box_tuple = tuple(map(int, [x1, y1, x2, y2]))
                recognized_name = "Unknown"
                worker = "ì•Œ ìˆ˜ ì—†ìŒ"
                
                # â­â­â­ 1ë‹¨ê³„: í˜„ì¬ í”„ë ˆì„ì—ì„œ ê°ì§€ëœ ì–¼êµ´ë§Œ ë§¤ì¹­ (ë’·ëª¨ìŠµ ì´ë¦„ í• ë‹¹ ì™„ì „ ë°©ì§€)
                # í˜„ì¬ í”„ë ˆì„ì—ì„œ buffalo_lì´ ê°ì§€í•˜ì§€ ëª»í•œ ì‚¬ëŒì—ê²ŒëŠ” ì´ë¦„ í• ë‹¹ ì•ˆ í•¨
                person_cx = (x1 + x2) / 2
                person_cy = (y1 + y2) / 2
                person_w = x2 - x1
                person_h = y2 - y1
                
                # í˜„ì¬ í”„ë ˆì„ ì–¼êµ´ì—ì„œ ë¨¼ì € ë§¤ì¹­ ì‹œë„
                for face_box_current, face_name_current in current_frame_faces:
                    fx1, fy1, fx2, fy2 = face_box_current
                    face_cx = (fx1 + fx2) / 2
                    face_cy = (fy1 + fy2) / 2
                    
                    # ì–¼êµ´ ì¤‘ì‹¬ì´ Person ë°•ìŠ¤ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
                    if x1 <= face_cx <= x2 and y1 <= face_cy <= y2:
                        # ì–¼êµ´ì´ Person ìƒë‹¨ 60%ì— ìˆëŠ”ì§€ í™•ì¸
                        person_top_60 = y1 + person_h * 0.6
                        if face_cy <= person_top_60:
                            recognized_name = face_name_current
                            worker = face_name_current
                            logging.debug(f"[CAM-{cam_id}] í˜„ì¬ í”„ë ˆì„ ì–¼êµ´ ë§¤ì¹­: {face_name_current}")
                            break
                
                # í˜„ì¬ í”„ë ˆì„ì—ì„œ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ, ê¸°ì¡´ ìºì‹œì—ì„œ ì‹œë„ (ë‹¨, ë” ì—„ê²©í•œ ì¡°ê±´)
                best_match_score = -1  # ë§¤ì¹­ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                best_face_index = None
                
                # í˜„ì¬ í”„ë ˆì„ ë§¤ì¹­ ì„±ê³µ ì‹œ ìºì‹œ ë§¤ì¹­ ìŠ¤í‚µ
                if recognized_name != "Unknown":
                    best_face_index = -1  # ìºì‹œ ë§¤ì¹­ ë¶ˆí•„ìš” í”Œë˜ê·¸
                
                for face_cx, face_cy, face_name, face_idx, face_box in existing_faces_list:
                    # â­ í˜„ì¬ í”„ë ˆì„ì—ì„œ ì´ë¯¸ ë§¤ì¹­ë¨ â†’ ìºì‹œ ë§¤ì¹­ ìŠ¤í‚µ
                    if best_face_index == -1:
                        break
                    
                    # â­ ì´ë¯¸ ë‹¤ë¥¸ Personì— ë§¤ì¹­ëœ ì–¼êµ´ì€ ê±´ë„ˆë›°ê¸°
                    if face_idx in used_face_indices:
                        continue
                    
                    fx1, fy1, fx2, fy2 = face_box
                    face_w = fx2 - fx1
                    face_h = fy2 - fy1
                    
                    # ===== 1ë‹¨ê³„: ì–¼êµ´ ë°•ìŠ¤ê°€ Person ë°•ìŠ¤ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸ =====
                    # ì–¼êµ´ ì¤‘ì‹¬ì´ Person ë°•ìŠ¤ ë‚´ë¶€ì— ìˆì–´ì•¼ í•¨
                    if not (x1 <= face_cx <= x2 and y1 <= face_cy <= y2):
                        logging.debug(f"[CAM-{cam_id}] ì–¼êµ´ ì¤‘ì‹¬ì´ Person ë°•ìŠ¤ ë°–: face=({face_cx:.0f},{face_cy:.0f}), person=({x1},{y1},{x2},{y2}) - ìŠ¤í‚µ")
                        continue
                    
                    # ===== 2ë‹¨ê³„: ì–¼êµ´ì´ Person ë°•ìŠ¤ ìƒë‹¨ 60%ì— ìˆì–´ì•¼ í•¨ (ë¨¸ë¦¬ ìœ„ì¹˜) =====
                    # ë’·ëª¨ìŠµ Personì˜ ê²½ìš° ì•ì‚¬ëŒì˜ ì–¼êµ´ì´ í•˜ë‹¨ì— ìœ„ì¹˜í•  ìˆ˜ ìˆìŒ
                    person_top_60_percent = y1 + person_h * 0.6
                    if face_cy > person_top_60_percent:
                        logging.debug(f"[CAM-{cam_id}] ì–¼êµ´ì´ Person ë°•ìŠ¤ í•˜ë‹¨ì— ìˆìŒ (ë’·ëª¨ìŠµ ì˜ì‹¬): face_cy={face_cy:.0f}, top60%={person_top_60_percent:.0f} - ìŠ¤í‚µ")
                        continue
                    
                    # ===== 3ë‹¨ê³„: ì–¼êµ´ ë°•ìŠ¤ì˜ ëŒ€ë¶€ë¶„ì´ Person ë°•ìŠ¤ ì•ˆì— í¬í•¨ =====
                    # ì–¼êµ´ ë°•ìŠ¤ì™€ Person ë°•ìŠ¤ì˜ êµì§‘í•© ê³„ì‚°
                    inter_x1 = max(x1, fx1)
                    inter_y1 = max(y1, fy1)
                    inter_x2 = min(x2, fx2)
                    inter_y2 = min(y2, fy2)
                    
                    if inter_x1 < inter_x2 and inter_y1 < inter_y2:
                        inter_area = (inter_x2 - inter_x1) * (inter_y2 - inter_y1)
                        face_area = max(face_w * face_h, 1)
                        containment_ratio = inter_area / face_area  # ì–¼êµ´ì´ Personì— í¬í•¨ëœ ë¹„ìœ¨
                        
                        # ì–¼êµ´ì˜ 70% ì´ìƒì´ Person ë°•ìŠ¤ ì•ˆì— ìˆì–´ì•¼ í•¨
                        if containment_ratio < 0.7:
                            logging.debug(f"[CAM-{cam_id}] ì–¼êµ´ í¬í•¨ ë¹„ìœ¨ ë¶€ì¡±: {containment_ratio:.2f} < 0.7 - ìŠ¤í‚µ")
                            continue
                    else:
                        # êµì§‘í•© ì—†ìŒ
                        continue
                    
                    # ===== 4ë‹¨ê³„: ê±°ë¦¬ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° =====
                    distance = ((person_cx - face_cx) ** 2 + (person_cy - face_cy) ** 2) ** 0.5
                    if distance > 200:  # 200í”½ì…€ ì´ˆê³¼ë©´ ì œì™¸
                        continue
                    
                    # ë§¤ì¹­ ì ìˆ˜: ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡, í¬í•¨ ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
                    match_score = containment_ratio * (1.0 - distance / 200.0)
                    
                    if match_score > best_match_score:
                        best_match_score = match_score
                        recognized_name = face_name
                        worker = face_name
                        best_face_index = face_idx
                        logging.debug(f"[CAM-{cam_id}] ì–¼êµ´ ë§¤ì¹­ í›„ë³´: {face_name}, ì ìˆ˜={match_score:.3f}, ê±°ë¦¬={distance:.0f}, í¬í•¨ë¹„ìœ¨={containment_ratio:.2f}")
                
                # â­ ë§¤ì¹­ëœ ì–¼êµ´ ì¸ë±ìŠ¤ ê¸°ë¡ (ì¤‘ë³µ ë§¤ì¹­ ë°©ì§€)
                if best_face_index is not None:
                    used_face_indices.add(best_face_index)
                    logging.debug(f"[CAM-{cam_id}] ìµœì¢… ì–¼êµ´ ë§¤ì¹­: {recognized_name}, ì ìˆ˜={best_match_score:.3f}")
                
                # â­ ëª¨ë“  ì‚¬ëŒ ì¶”ê°€ (ìœ„ë°˜ ì—¬ë¶€ ê´€ê³„ì—†ì´) - ì´ˆë¡ ë°•ìŠ¤ í‘œì‹œë¥¼ ìœ„í•´!
                if filtered_violations:
                    # ìœ„ë°˜ì´ ìˆëŠ” ê²½ìš°
                    unique_violations = list(set(filtered_violations))
                    ppe_violations_display = []
                    for v in unique_violations:
                        if v == "ì•ˆì „ëª¨":
                            ppe_violations_display.append("ì•ˆì „ëª¨")
                        elif v == "ì•ˆì „ì¡°ë¼":
                            ppe_violations_display.append("ì•ˆì „ì¡°ë¼")
                        elif v == "ë„˜ì–´ì§":
                            ppe_violations_display.append("ë„˜ì–´ì§")
                    
                    # ë„˜ì–´ì§ì´ ìˆìœ¼ë©´ hazardë¥¼ ë„˜ì–´ì§ìœ¼ë¡œ ì„¤ì •
                    if "ë„˜ì–´ì§" in ppe_violations_display:
                        hazard = "âš ï¸ ë„˜ì–´ì§ ê°ì§€"
                    else:
                        hazard = f"PPE ìœ„ë°˜ë‚´ì—­: {', '.join(ppe_violations_display)}" if ppe_violations_display else "ìœ„ë°˜ ê°ì§€"
                    
                    violations.append({
                        "person_box": [x1, y1, x2, y2],
                        "violations": unique_violations,
                        "recognized_name": recognized_name,
                        "worker": worker,
                        "area": area,
                        "level": "WARNING",
                        "hazard": hazard
                    })
                    
                    # new_recognized_facesì— ìœ„ë°˜ ì •ë³´ ì¶”ê°€
                    new_recognized_faces.append({
                        "box": [x1, y1, x2, y2],
                        "bbox": [x1, y1, x2, y2],
                        "name": recognized_name,
                        "type": "Violation",
                        "isViolation": True,
                        "ppe_violations": unique_violations,
                    })
                else:
                    # â­ ìœ„ë°˜ì´ ì—†ëŠ” ê²½ìš° - ì•ˆì „! (ì´ˆë¡ ë°•ìŠ¤)
                    violations.append({
                        "person_box": [x1, y1, x2, y2],
                        "violations": [],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ = ì•ˆì „
                        "recognized_name": recognized_name,
                        "worker": worker,
                        "area": area,
                        "level": "SAFE",
                        "hazard": "ì•ˆì „"
                    })
                    
                    new_recognized_faces.append({
                        "box": [x1, y1, x2, y2],
                        "bbox": [x1, y1, x2, y2],
                        "name": recognized_name,
                        "type": "Safe",
                        "isViolation": False,
                        "ppe_violations": [],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ = ì•ˆì „
                    })
            
            # ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ ê²°ê³¼ì— ë³‘í•©)
            result_dict = {
                'violations': violations, 
                'violation_count': len(violations), 
                'all_detections': all_detections,
                'recognized_faces': new_recognized_faces  # ìœ„ë°˜ ì •ë³´ë§Œ í¬í•¨
            }
            with results_cache_lock:
                # ê¸°ì¡´ ê²°ê³¼ ì°¾ê¸° ë° ë³‘í•©
                merged = False
                for idx, (ts, rd) in enumerate(model_results_cache[cam_id]):
                    # íƒ€ì„ìŠ¤íƒ¬í”„ ë§¤ì¹­ ì •ë°€ë„: 0.1ì´ˆ (ì§€ì—° í—ˆìš© ë²”ìœ„ í™•ëŒ€)
                    if abs(ts - timestamp) < 0.1:
                        # ê¸°ì¡´ ê²°ê³¼ì— ë³‘í•© (recognized_facesì™€ violationsëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°)
                        # í‚¤ê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
                        if 'recognized_faces' not in rd:
                            rd['recognized_faces'] = []
                        if 'violations' not in rd:
                            rd['violations'] = []
                        if 'all_detections' not in rd:
                            rd['all_detections'] = {}
                        
                        existing_faces = rd.get('recognized_faces', [])
                        existing_violations = rd.get('violations', [])
                        
                        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ IoU ê¸°ë°˜ìœ¼ë¡œ í™•ì¸ (ë°•ìŠ¤ ì¢Œí‘œ ì •í™• ì¼ì¹˜ê°€ ì•„ë‹Œ IoU ì‚¬ìš©)
                        new_faces = []
                        for new_face in new_recognized_faces:
                            new_box = new_face.get('box', [])
                            if len(new_box) != 4:
                                continue
                            new_box_tuple = tuple(map(int, new_box))
                            is_duplicate = False
                            for existing_face in existing_faces:
                                existing_box = existing_face.get('box', [])
                                if len(existing_box) != 4:
                                    continue
                                existing_box_tuple = tuple(map(int, existing_box))
                                # IoU ê¸°ë°˜ ì¤‘ë³µ í™•ì¸ (0.5 ì´ìƒì´ë©´ ê°™ì€ ì‚¬ëŒ)
                                iou = utils.calculate_iou(new_box_tuple, existing_box_tuple)
                                if iou > 0.5:
                                    # ê°™ì€ ì‚¬ëŒì´ë©´ ê¸°ì¡´ ì–¼êµ´ ì¸ì‹ ì •ë³´ëŠ” ìœ ì§€í•˜ê³ , ìœ„ë°˜ ì •ë³´ë§Œ ì—…ë°ì´íŠ¸
                                    if 'name' in existing_face and existing_face['name'] != 'Unknown':
                                        # ê¸°ì¡´ ì–¼êµ´ ì¸ì‹ ì •ë³´ê°€ ìˆìœ¼ë©´ ìœ„ë°˜ ì •ë³´ë§Œ ì—…ë°ì´íŠ¸
                                        existing_face['isViolation'] = True
                                        existing_face['ppe_violations'] = new_face.get('ppe_violations', [])
                                        existing_face['type'] = new_face.get('type', '')
                                    is_duplicate = True
                                    break
                            if not is_duplicate:
                                new_faces.append(new_face)
                        
                        new_violations = []
                        for new_viol in violations:
                            new_box = new_viol.get('person_box', [])
                            if len(new_box) != 4:
                                continue
                            new_box_tuple = tuple(map(int, new_box))
                            is_duplicate = False
                            for existing_viol in existing_violations:
                                existing_box = existing_viol.get('person_box', [])
                                if len(existing_box) != 4:
                                    continue
                                existing_box_tuple = tuple(map(int, existing_box))
                                # IoU ê¸°ë°˜ ì¤‘ë³µ í™•ì¸ (0.5 ì´ìƒì´ë©´ ê°™ì€ ì‚¬ëŒ)
                                iou = utils.calculate_iou(new_box_tuple, existing_box_tuple)
                                if iou > 0.5:
                                    is_duplicate = True
                                    break
                            if not is_duplicate:
                                new_violations.append(new_viol)
                        
                        # ë³‘í•©
                        rd['recognized_faces'].extend(new_faces)
                        rd['violations'].extend(new_violations)
                        rd['violation_count'] = len(rd['violations'])
                        rd['all_detections'].update(all_detections)
                        merged = True
                        logging.info(f"[CAM-{cam_id}] ë°±ê·¸ë¼ìš´ë“œ ê²°ê³¼ ë³‘í•©: ê¸°ì¡´ ìœ„ë°˜={len(existing_violations)}ê°œ, ìƒˆ ìœ„ë°˜={len(new_violations)}ê°œ, ìµœì¢…={len(rd['violations'])}ê°œ")
                        break
                
                if not merged:
                    # ì €ì¥í•  ë•Œ í˜„ì¬ ì‹œê°„ ì‚¬ìš© (ì •ë¦¬ ë¡œì§ì—ì„œ ì¦‰ì‹œ ì‚­ì œ ë°©ì§€)
                    save_time = time.time()
                    model_results_cache[cam_id].append((save_time, result_dict))
                    logging.info(f"[CAM-{cam_id}] ë°±ê·¸ë¼ìš´ë“œ ê²°ê³¼ ì‹ ê·œ ì €ì¥: {len(violations)}ê°œ ìœ„ë°˜, {len(new_recognized_faces)}ê°œ ì–¼êµ´")
                
                # ì˜¤ë˜ëœ ê²°ê³¼ ì œê±° (CACHE_TTL ì‚¬ìš© - 3.0ì´ˆ)
                current_time = time.time()
                model_results_cache[cam_id] = [
                    (ts, rd) for ts, rd in model_results_cache[cam_id]
                    if current_time - ts <= CACHE_TTL
                ]
            
            logging.info(f"[CAM-{cam_id}] ë°±ê·¸ë¼ìš´ë“œ Violation+Pose ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(violations)}ê°œ ìœ„ë°˜, {len(new_recognized_faces)}ê°œ ì–¼êµ´")
        except Exception as e:
            logging.error(f"Violations ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
    
    def save_fall_result(future):
        """Fall Detection ëª¨ë¸ ê²°ê³¼ ì²˜ë¦¬ (ëˆ„ìš´ ì‚¬ëŒ ì§ì ‘ ê°ì§€)"""
        try:
            fall_results = future.result()
            if fall_results is None:
                with data_lock:
                    fall_data['ready'] = True
                return
            
            fall_detections = []
            if fall_results and len(fall_results) > 0 and fall_results[0].boxes is not None:
                boxes = fall_results[0].boxes.xyxy.cpu().numpy()
                confidences = fall_results[0].boxes.conf.cpu().numpy() if fall_results[0].boxes.conf is not None else None
                classes = fall_results[0].boxes.cls.cpu().numpy() if fall_results[0].boxes.cls is not None else None
                
                for idx, box in enumerate(boxes):
                    conf = float(confidences[idx]) if confidences is not None and len(confidences) > idx else 0.5
                    cls = int(classes[idx]) if classes is not None and len(classes) > idx else 0
                    
                    # Fall í´ë˜ìŠ¤ë§Œ ê°ì§€ (í´ë˜ìŠ¤ 0=Person, 1=Fall)
                    # â­ í´ë˜ìŠ¤ 1("Fall")ì´ê³  confidenceê°€ 0.75 ì´ìƒì¸ ê²½ìš° ì²˜ë¦¬ (ì˜¤íƒì§€ ë°©ì§€)
                    if cls == 1 and conf >= 0.75:
                        # â­ resized_frame ì‚¬ìš©í•˜ë¯€ë¡œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ë™ì¼í•˜ê²Œ ìŠ¤ì¼€ì¼ë§
                        bbox_resized = box
                        bbox_original = bbox_resized * np.array([w_scale, h_scale, w_scale, h_scale])
                        bbox_clipped = utils.clip_bbox_xyxy(bbox_original, orig_w, orig_h)
                        
                        if bbox_clipped is not None:
                            # ë°•ìŠ¤ í¬ê¸° í•„í„°ë§ (ë„ˆë¬´ ì‘ì€ ë°•ìŠ¤ ì œì™¸ - ìµœì†Œ 60x60 í”½ì…€)
                            box_width = bbox_clipped[2] - bbox_clipped[0]
                            box_height = bbox_clipped[3] - bbox_clipped[1]
                            box_area = box_width * box_height
                            frame_area = orig_w * orig_h
                            
                            if box_width < 60 or box_height < 60:
                                logging.debug(f"[CAM-{cam_id}] Fall ë°•ìŠ¤ í•„í„°ë§: í¬ê¸° ë„ˆë¬´ ì‘ìŒ ({box_width:.0f}x{box_height:.0f})")
                                continue
                            
                            # â­ ë„ˆë¬´ í° ë°•ìŠ¤ ì œì™¸ (í™”ë©´ì˜ 40% ì´ìƒ ì°¨ì§€í•˜ë©´ ì˜¤íƒì§€)
                            if box_area > frame_area * 0.4:
                                logging.debug(f"[CAM-{cam_id}] Fall ë°•ìŠ¤ í•„í„°ë§: í¬ê¸° ë„ˆë¬´ í¼ (í™”ë©´ì˜ {box_area/frame_area*100:.1f}%)")
                                continue
                            
                            # â­ ë°•ìŠ¤ ë¹„ìœ¨ í™•ì¸ (ë„˜ì–´ì§„ ìì„¸ = ê°€ë¡œ > ì„¸ë¡œ)
                            box_ratio = box_width / box_height if box_height > 0 else 0
                            if box_ratio < 1.2:
                                logging.debug(f"[CAM-{cam_id}] Fall ë°•ìŠ¤ í•„í„°ë§: ë¹„ìœ¨ ë¶€ì í•© (ratio={box_ratio:.2f} < 1.2)")
                                continue
                            
                            # â­ ë°œ/ì‹ ë°œ í•„í„°ë§: ë„˜ì–´ì§„ ì‚¬ëŒë„ ìµœì†Œ ë†’ì´ 130px ì´ìƒ
                            if box_height < 130:
                                logging.debug(f"[CAM-{cam_id}] Fall ë°•ìŠ¤ í•„í„°ë§: ë†’ì´ ë¶€ì¡± (ë°œ/ì‹ ë°œ ì˜ì‹¬): h={box_height:.0f} < 130")
                                continue
                            
                            fall_detections.append({
                                'bbox': list(bbox_clipped),
                                'conf': conf,
                                'class': cls
                            })
                            logging.warning(f"[CAM-{cam_id}] ğŸ”» Fall ëª¨ë¸ ê°ì§€: í´ë˜ìŠ¤=Fall, conf={conf:.2f}, ë°•ìŠ¤={bbox_clipped}")
            
            with data_lock:
                fall_data['fall_detections'] = fall_detections
                fall_data['ready'] = True
                
                # Fall Detection ê²°ê³¼ë¥¼ ê¸°ì¡´ Pose ê²°ê³¼ì™€ ë³‘í•©
                if fall_detections:
                    current_time = time.time()
                    with results_cache_lock:
                        if cam_id not in model_results_cache:
                            model_results_cache[cam_id] = []
                        
                        for fall_det in fall_detections:
                            x1, y1, x2, y2 = [int(c) for c in fall_det['bbox']]
                            fall_center_x = (x1 + x2) / 2
                            fall_center_y = (y1 + y2) / 2
                            
                            # ì¤‘ë³µ ì²´í¬ (ê°™ì€ ìœ„ì¹˜ì—ì„œ 0.5ì´ˆ ë‚´ ì¤‘ë³µ ë°©ì§€ - ë¹ ë¥¸ ì—…ë°ì´íŠ¸ í—ˆìš©)
                            is_duplicate = False
                            for ts, rd in model_results_cache[cam_id]:
                                if current_time - ts < 0.5:
                                    for v in rd.get('violations', []):
                                        if v.get('violation_type') == 'ë„˜ì–´ì§':
                                            v_center_x = (v.get('x1', 0) + v.get('x2', 0)) / 2
                                            v_center_y = (v.get('y1', 0) + v.get('y2', 0)) / 2
                                            if abs(v_center_x - fall_center_x) < 100 and abs(v_center_y - fall_center_y) < 100:
                                                is_duplicate = True
                                                break
                            
                            if is_duplicate:
                                continue
                            
                            # ê°€ì¥ ê°€ê¹Œìš´ ê¸°ì¡´ ê²°ê³¼ ì°¾ê¸° (ì´ë¦„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°)
                            best_match_name = "Unknown"
                            best_match_distance = float('inf')
                            
                            for ts, rd in model_results_cache[cam_id]:
                                if current_time - ts < 2.0:  # 2ì´ˆ ì´ë‚´ ê²°ê³¼
                                    for face in rd.get('recognized_faces', []):
                                        face_box = face.get('box', [0, 0, 0, 0])
                                        if len(face_box) >= 4:
                                            face_center_x = (face_box[0] + face_box[2]) / 2
                                            face_center_y = (face_box[1] + face_box[3]) / 2
                                            distance = ((fall_center_x - face_center_x) ** 2 + (fall_center_y - face_center_y) ** 2) ** 0.5
                                            if distance < best_match_distance and distance < 200:  # 200í”½ì…€ ì´ë‚´
                                                best_match_distance = distance
                                                best_match_name = face.get('name', 'Unknown')
                            
                            # ë„˜ì–´ì§ ìœ„ë°˜ + ì–¼êµ´ ì •ë³´ í•¨ê»˜ ì €ì¥
                            new_violation = {
                                'violation_type': 'ë„˜ì–´ì§',
                                'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2,
                                'bbox': [x1, y1, x2, y2],
                                'person_box': [x1, y1, x2, y2],
                                'box': [x1, y1, x2, y2],
                                'confidence': fall_det['conf'],
                                'timestamp': timestamp,
                                'cam_id': cam_id
                            }
                            
                            # ì–¼êµ´ ì •ë³´ë„ í•¨ê»˜ ì €ì¥ (ì´ë¦„ ë¼ë²¨ë§ìš©)
                            new_face = {
                                'name': best_match_name,
                                'box': [x1, y1, x2, y2],
                                'bbox': [x1, y1, x2, y2],
                                'ppe_violations': ['ë„˜ì–´ì§'],
                                'isViolation': True
                            }
                            
                            # ì €ì¥í•  ë•Œ í˜„ì¬ ì‹œê°„ ì‚¬ìš© (ì •ë¦¬ ë¡œì§ì—ì„œ ì¦‰ì‹œ ì‚­ì œ ë°©ì§€)
                            save_time = time.time()
                            model_results_cache[cam_id].append((save_time, {
                                'violations': [new_violation],
                                'recognized_faces': [new_face],
                                'frame_timestamp': save_time
                            }))
                            
                            if best_match_name != "Unknown":
                                logging.warning(f"[CAM-{cam_id}] âš ï¸ ë„˜ì–´ì§ ê°ì§€! ì´ë¦„={best_match_name}, conf={fall_det['conf']:.2f}, ìœ„ì¹˜=({x1}, {y1})")
                            else:
                                logging.warning(f"[CAM-{cam_id}] âš ï¸ ë„˜ì–´ì§ ê°ì§€! conf={fall_det['conf']:.2f}, ìœ„ì¹˜=({x1}, {y1})")
        except Exception as e:
            logging.debug(f"Fall ëª¨ë¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            with data_lock:
                fall_data['ready'] = True
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê²°ê³¼ ì €ì¥
    yolo_executor.submit(save_violation_result, violation_future)
    yolo_executor.submit(save_pose_result, pose_future)
    if fall_future:
        yolo_executor.submit(save_fall_result, fall_future)
    
    # ğŸ¦¬ buffalo_l ì–¼êµ´ ì¸ì‹: face_analyzerê°€ ìˆìœ¼ë©´ í•­ìƒ ì‹¤í–‰ (face_detection_future ì—†ì–´ë„ ë¨)
    if face_analyzer is not None:
        face_recognition_executor.submit(save_face_result, face_detection_future)


def _generate_person_box_key(cam_id: int, matched_entry: Optional[Dict], x1: int, y1: int, x2: int, y2: int) -> str:
    """
    person_box_key ìƒì„± í—¬í¼ í•¨ìˆ˜
    
    Args:
        cam_id: ì¹´ë©”ë¼ ID
        matched_entry: ìºì‹œì—ì„œ ì°¾ì€ í•­ëª© (None ê°€ëŠ¥)
        x1, y1, x2, y2: ì‚¬ëŒ ë°•ìŠ¤ ì¢Œí‘œ
    
    Returns:
        person_box_key ë¬¸ìì—´
    """
    if matched_entry is not None:
        cached_name = matched_entry.get('name', 'Unknown')
        if cached_name != "Unknown":
            return f"{cam_id}_{cached_name}"
    return f"{int(x1)}_{int(y1)}_{int(x2)}_{int(y2)}"

def process_single_frame(
    frame_bytes: bytes,
    cam_id: int
) -> Tuple[bytes, Dict[str, Any]]:
    """
    ë‹¨ì¼ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        frame_bytes: í”„ë ˆì„ ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
        cam_id: ì¹´ë©”ë¼ ID
        
    Returns:
        Tuple[bytes, Dict[str, Any]]: ì²˜ë¦¬ëœ í”„ë ˆì„ ë°”ì´íŠ¸ì™€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # cam_id íƒ€ì… í†µì¼ (int)
    try:
        cam_id = int(cam_id)
    except (ValueError, TypeError):
        pass
    
    # ì„±ëŠ¥ ì¸¡ì •ìš© ë”•ì…”ë„ˆë¦¬
    perf_timings = {
        'total': 0.0,
        'decode': 0.0,
        'resize': 0.0,
        'yolo_violation': 0.0,
        'yolo_pose': 0.0,
        'parse_results': 0.0,
        'face_recognition': 0.0,
        'rendering': 0.0,
        'encoding': 0.0
    }
    
    total_start = time.time()
    
    # 1ë‹¨ê³„ ìµœì í™”: í”„ë ˆì„ ìŠ¤í‚µ 30% (30 FPS ëª©í‘œ)
    # 10í”„ë ˆì„ ì¤‘ 3ê°œ ìŠ¤í‚µ = 30% ìŠ¤í‚µ
    if not hasattr(process_single_frame, '_frame_counters'):
        process_single_frame._frame_counters = {}
    if cam_id not in process_single_frame._frame_counters:
        process_single_frame._frame_counters[cam_id] = 0
    process_single_frame._frame_counters[cam_id] += 1
    frame_counter = process_single_frame._frame_counters[cam_id]
    
    # í”„ë ˆì„ ìŠ¤í‚µ ë¹„í™œì„±í™” (ì‹¤ì‹œê°„ ì²˜ë¦¬ - ëª¨ë“  í”„ë ˆì„)
    # skip_pattern = [3, 6, 9]  # ë¹„í™œì„±í™”
    should_skip = False  # ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬
    
    if should_skip:
        # ìŠ¤í‚µëœ í”„ë ˆì„: í”„ë¡œë•ì…˜ ìµœì í™” - ìµœì†Œí•œì˜ ì²˜ë¦¬ë§Œ ìˆ˜í–‰ (PIL ì œê±°, OpenCV ì§ì ‘ ì‚¬ìš©)
        try:
            nparr = np.frombuffer(frame_bytes, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            if frame is None:
                # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ì´ì „ í”„ë ˆì„ ì¬ì‚¬ìš©
                if cam_id in _last_rendered_frames:
                    last_frame_bytes, last_result = _last_rendered_frames[cam_id]
                    logging.debug(f"[CAM-{cam_id}] í”„ë ˆì„ ìŠ¤í‚µ (30% ìµœì í™”): {frame_counter}ë²ˆì§¸ í”„ë ˆì„, ë””ì½”ë”© ì‹¤íŒ¨ë¡œ ì´ì „ í”„ë ˆì„ ì¬ì‚¬ìš©")
                    return last_frame_bytes, last_result
                # ì´ì „ í”„ë ˆì„ë„ ì—†ìœ¼ë©´ ë¹ˆ í”„ë ˆì„
                empty_frame = np.zeros((480, 640, 3), dtype=np.uint8)
                _, buffer = cv2.imencode('.jpg', empty_frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
                return buffer.tobytes(), {"timestamp": time.time(), "recognized_faces": [], "violations": [], "violation_count": 0, "performance": {"skipped": True}}
            
            orig_h, orig_w = frame.shape[:2]
            
            # ì´ì „ ê²°ê³¼ì—ì„œ ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            last_result = None
            if cam_id in _last_rendered_frames:
                _, last_result = _last_rendered_frames[cam_id]
            
            # ë Œë”ë§ í•„ìš” ì—¬ë¶€ í™•ì¸ (í”„ë ˆì„ ë³µì‚¬ ìµœì†Œí™”)
            needs_rendering = last_result and (len(last_result.get("recognized_faces", [])) > 0 or len(last_result.get("violations", [])) > 0)
            
            if needs_rendering:
                # ë Œë”ë§ì´ í•„ìš”í•  ë•Œë§Œ í”„ë ˆì„ ë³µì‚¬ (ë©”ëª¨ë¦¬ ìµœì í™”)
                processed_frame = frame.copy()
                recognized_faces = last_result.get("recognized_faces", [])
                violations = last_result.get("violations", [])
                
                # ê³µí†µ ë Œë”ë§ í•¨ìˆ˜ í˜¸ì¶œ
                processed_frame = render_frame_results(
                    processed_frame,
                    recognized_faces,
                    violations,
                    cam_id,
                    orig_w,
                    orig_h
                )
            else:
                # ë Œë”ë§ì´ í•„ìš” ì—†ìœ¼ë©´ í”„ë ˆì„ ë³µì‚¬ ì—†ì´ ì›ë³¸ ì‚¬ìš©
                processed_frame = frame

            
            # ë¦¬ì‚¬ì´ì¦ˆ ìµœì í™”: í•„ìš”í•  ë•Œë§Œ ë¦¬ì‚¬ì´ì¦ˆ
            stream_width = 1280
            if processed_frame.shape[1] > stream_width:
                aspect_ratio = processed_frame.shape[0] / processed_frame.shape[1]
                stream_height = int(stream_width * aspect_ratio)
                processed_frame = cv2.resize(processed_frame, (stream_width, stream_height), 
                                           interpolation=cv2.INTER_LINEAR)
            
            # ì¸ì½”ë”© í’ˆì§ˆ ì¡°ì • (í”„ë¡œë•ì…˜: 95 - ê³ í™”ì§ˆ)
            _, buffer = cv2.imencode('.jpg', processed_frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
            
            # ì´ì „ ê²°ê³¼ ì¬ì‚¬ìš© (ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ìœ ì§€)
            result = last_result.copy() if last_result else {
                "recognized_faces": [],
                "violations": [],
                "violation_count": 0,
                "frame_width": orig_w,
                "frame_height": orig_h,
                "cam_id": cam_id
            }
            # íƒ€ì„ìŠ¤íƒ¬í”„ ê°±ì‹  (ìƒˆ í”„ë ˆì„ì´ë¯€ë¡œ)
            result["timestamp"] = time.time()
            result["performance"] = {"skipped": True}
            
            logging.debug(f"[CAM-{cam_id}] í”„ë ˆì„ ìŠ¤í‚µ (30% ìµœì í™”): {frame_counter}ë²ˆì§¸ í”„ë ˆì„, AI ì²˜ë¦¬ ìŠ¤í‚µ (í”„ë¡œë•ì…˜ ìµœì í™”)")
            return buffer.tobytes(), result
            
        except Exception as e:
            logging.warning(f"[CAM-{cam_id}] ìŠ¤í‚µ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ì´ì „ í”„ë ˆì„ ì¬ì‚¬ìš©
            if cam_id in _last_rendered_frames:
                last_frame_bytes, last_result = _last_rendered_frames[cam_id]
                return last_frame_bytes, last_result
            # ì´ì „ í”„ë ˆì„ë„ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
            empty_frame = np.zeros((480, 640, 3), dtype=np.uint8)
            _, buffer = cv2.imencode('.jpg', empty_frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
            return buffer.tobytes(), {"recognized_faces": [], "violations": [], "violation_count": 0, "performance": {"skipped": True}}
    
    # SafetySystem ì´ˆê¸°í™” í™•ì¸ ë° ì—ëŸ¬ ì²˜ë¦¬ ê°œì„ 
    # ì „ì—­ ë³€ìˆ˜ ì•ˆì „í•˜ê²Œ ì½ê¸° (ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ ëŒ€ë¹„ - ë½ ì‚¬ìš©)
    # state ëª¨ë“ˆì„ ì§ì ‘ importí•˜ì—¬ ìµœì‹  ê°’ì„ ì½ë„ë¡ ìˆ˜ì •
    with safety_system_lock:
        safety_system = state.safety_system_instance
        is_none = safety_system is None
        logging.debug(f"[CAM-{cam_id}] SafetySystem í™•ì¸: ì¡´ì¬={not is_none}")
    
    if safety_system is None:
        logging.warning(f"[CAM-{cam_id}] SafetySystemì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ˆê¸°í™” ì™„ë£Œ ëŒ€ê¸° ì¤‘... (ì—ëŸ¬ í”„ë ˆì„ ë°˜í™˜)")
        # ì—ëŸ¬ í”„ë ˆì„ ìƒì„±
        error_frame = np.zeros((480, 640, 3), dtype=np.uint8)
        cv2.putText(error_frame, "System Initializing...", (50, 240), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
        ret, buffer = cv2.imencode('.jpg', error_frame)
        return buffer.tobytes(), {"error": "System not ready", "recognized_faces": [], "violations": []}
    
    # SafetySystemì´ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if safety_system.violation_model is None or safety_system.pose_model is None:
        logging.warning(f"[CAM-{cam_id}] í•„ìˆ˜ ëª¨ë¸ì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì—ëŸ¬ í”„ë ˆì„ ë°˜í™˜.")
        error_frame = np.zeros((480, 640, 3), dtype=np.uint8)
        cv2.putText(error_frame, "Models Loading...", (50, 240), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
        ret, buffer = cv2.imencode('.jpg', error_frame)
        return buffer.tobytes(), {"error": "Models not ready", "recognized_faces": [], "violations": []}
    
    # í•¨ìˆ˜ ì‹œì‘ (ë¡œê¹… ìµœì†Œí™”ë¡œ ì„±ëŠ¥ í–¥ìƒ)
    logging.debug(f"[CAM-{cam_id}] process_single_frame ì‹œì‘")

    # í•¨ìˆ˜ ë‚´ì—ì„œ orig_h, orig_w ê¸°ë³¸ê°’ ì„¤ì • (ì˜¤ë¥˜ ë°©ì§€)
    orig_h, orig_w = 480, 640
    frame = None # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì´ˆê¸°í™”

    # í”„ë ˆì„ ë³´ì¥ ë°©ì‹: íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë¡ (í”„ë ˆì„ ë²„í¼ ë° ëª¨ë¸ ê²°ê³¼ ìºì‹œì— ì‚¬ìš©)
    timestamp = time.time()
    
    try:
        # 1. ë°”ì´íŠ¸ë¥¼ ì´ë¯¸ì§€ë¡œ ë””ì½”ë”©
        decode_start = time.time()
        nparr = np.frombuffer(frame_bytes, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        perf_timings['decode'] = (time.time() - decode_start) * 1000  # ms
        if frame is None:
            logging.warning(f"í”„ë ˆì„ ë””ì½”ë”© ì‹¤íŒ¨ (CAM-{cam_id})")
            empty_frame = np.zeros((orig_h, orig_w, 3), dtype=np.uint8)
            _, buffer = cv2.imencode('.jpg', empty_frame)
            return buffer.tobytes(), {"timestamp": time.time(), "recognized_faces": [], "violations": [], "violation_count": 0}
        orig_h, orig_w = frame.shape[:2]
        
        # í”„ë ˆì„ ë³´ì¥ ë°©ì‹: í”„ë ˆì„ ë²„í¼ì— ì €ì¥ (ìµœê·¼ 1ì´ˆë§Œ ìœ ì§€)
        with frame_buffer_lock:
            frame_buffer[cam_id].append((timestamp, frame_bytes, frame.copy()))
            # ì˜¤ë˜ëœ í”„ë ˆì„ ì œê±° (1ì´ˆ ì´ìƒ ì§€ë‚œ í”„ë ˆì„)
            frame_buffer[cam_id] = [
                (ts, fb, f) for ts, fb, f in frame_buffer[cam_id]
                if timestamp - ts <= MAX_BUFFER_SECONDS
            ]
        
        # í”„ë ˆì„ ë³´ì¥ ë°©ì‹: ìºì‹œì—ì„œ ê°€ì¥ ìµœê·¼ ê²°ê³¼ ì°¾ê¸°
        best_result = None
        best_time_diff = float('inf')
        MAX_CACHE_AGE = 3.0  # ìºì‹œ ìœ íš¨ ì‹œê°„ (2.0 -> 3.0ì´ˆë¡œ í™•ëŒ€, ê¹œë¹¡ì„ ë°©ì§€)
        current_time = time.time()
        
        with results_cache_lock:
            if cam_id in model_results_cache and len(model_results_cache[cam_id]) > 0:
                # ê°€ì¥ ìµœê·¼ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì•„ë‹Œ í˜„ì¬ ì‹œê°„ ê¸°ì¤€)
                for result_ts, result_dict in reversed(model_results_cache[cam_id]):
                    time_diff = current_time - result_ts  # í˜„ì¬ ì‹œê°„ê³¼ì˜ ì°¨ì´ë¡œ ê³„ì‚°
                    if time_diff <= MAX_CACHE_AGE:
                        # ê°€ì¥ ìµœê·¼ ê²°ê³¼ë¥¼ ë¬´ì¡°ê±´ ì‚¬ìš© (faces/violations ìœ ë¬´ ê´€ê³„ì—†ì´)
                        best_result = result_dict
                        best_time_diff = time_diff
                        # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
                        violations_count = len(result_dict.get('violations', []))
                        faces_count = len(result_dict.get('recognized_faces', []))
                        recognized_names = [f.get('name', 'Unknown') for f in result_dict.get('recognized_faces', [])]
                        logging.debug(f"[CAM-{cam_id}] ìºì‹œì—ì„œ ê²°ê³¼ ì°¾ìŒ: age={time_diff:.3f}s, violations={violations_count}ê°œ, faces={faces_count}ê°œ, ì´ë¦„={recognized_names}")
                        break  # ê°€ì¥ ìµœê·¼ì˜ ìœ íš¨í•œ ê²°ê³¼ ì‚¬ìš©
                
                # ì˜¤ë˜ëœ ê²°ê³¼ ì œê±° (5ì´ˆ ì´ìƒ, ê¹œë¹¡ì„ ë°©ì§€)
                model_results_cache[cam_id] = [
                    (ts, rd) for ts, rd in model_results_cache[cam_id]
                    if current_time - ts <= 5.0
                ]
        
        # í”„ë ˆì„ ìœ íš¨ì„± ê²€ì‚¬: í¬ê¸°ê°€ ë„ˆë¬´ ì‘ê±°ë‚˜ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if orig_h < 100 or orig_w < 100:
            logging.warning(f"í”„ë ˆì„ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {orig_w}x{orig_h} (CAM-{cam_id})")
            empty_frame = np.zeros((orig_h, orig_w, 3), dtype=np.uint8)
            _, buffer = cv2.imencode('.jpg', empty_frame)
            return buffer.tobytes(), {"timestamp": time.time(), "recognized_faces": [], "violations": [], "violation_count": 0}
        
        # ê²€ì€ í”„ë ˆì„ ì²´í¬ (ìº ì´ êº¼ì§„ ìƒíƒœ): í‰ê·  ë°ê¸°ê°€ ë§¤ìš° ë‚®ìœ¼ë©´ AI ì²˜ë¦¬ë§Œ ê±´ë„ˆëœ€
        # ê¸°ì¤€: BGR í‰ê· ì´ 2.0 ë¯¸ë§Œì´ë©´ ê±°ì˜ ì™„ì „íˆ ê²€ì€ í”„ë ˆì„ìœ¼ë¡œ ê°„ì£¼
        frame_mean = np.mean(frame)
        if frame_mean < 2.0:  # í‰ê·  ë°ê¸°ê°€ 2 ë¯¸ë§Œì´ë©´ ê²€ì€ í”„ë ˆì„ìœ¼ë¡œ ê°„ì£¼
            logging.debug(f"[CAM-{cam_id}] ê²€ì€ í”„ë ˆì„ ê°ì§€ - AI ì²˜ë¦¬ ê±´ë„ˆëœ€")
            # ê²€ì€ í”„ë ˆì„ë„ ìŠ¤íŠ¸ë¦¼ì—ëŠ” í‘œì‹œí•˜ë˜, ìœ„ë°˜ ê°ì§€ëŠ” í•˜ì§€ ì•ŠìŒ (í’ˆì§ˆ ìµœì í™”: 100 â†’ 85)
            # ë³µì‚¬ ìµœì í™”: ì¸ì½”ë”©ë§Œ í•„ìš”í•˜ë¯€ë¡œ ì›ë³¸ í”„ë ˆì„ ì§ì ‘ ì‚¬ìš©
            _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
            return buffer.tobytes(), {"timestamp": time.time(), "recognized_faces": [], "violations": [], "violation_count": 0, "performance": {}}

        # 2. ëª¨ë¸ ì…ë ¥ í¬ê¸°ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ (ì›ë³¸ ë¹„ìœ¨ ìœ ì§€, ìµœì í™”)
        # ê° ì¹´ë©”ë¼ì˜ ì›ë³¸ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ìµœëŒ€ í¬ê¸°ë¥¼ ì œí•œ
        # ì˜ˆ: 1920x1080 (16:9) -> 1024x576 (16:9 ìœ ì§€, ìµœì )
        #     1280x720 (16:9) -> 1024x576 (16:9 ìœ ì§€, ìµœì )
        resize_start = time.time()
        max_input_size = max(config.SystemConfig.MODEL_INPUT_WIDTH, config.SystemConfig.MODEL_INPUT_HEIGHT)
        
        # ì›ë³¸ ë¹„ìœ¨ ê³„ì‚°
        orig_ratio = orig_w / orig_h
        
        # ìµœëŒ€ í¬ê¸°ë¥¼ ì œí•œí•˜ë©´ì„œ ë¹„ìœ¨ ìœ ì§€ (ë‹¤ìš´ìŠ¤ì¼€ì¼ë§ë§Œ ìˆ˜í–‰, ì—…ìŠ¤ì¼€ì¼ë§ì€ í•˜ì§€ ì•ŠìŒ)
        if orig_w > max_input_size or orig_h > max_input_size:
            # ë‹¤ìš´ìŠ¤ì¼€ì¼ë§: ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ìµœëŒ€ í¬ê¸° ì œí•œ
            if orig_w > orig_h:
                # ê°€ë¡œê°€ ë” ê¸´ ê²½ìš° (landscape)
                new_w = max_input_size
                new_h = int(max_input_size / orig_ratio)
            else:
                # ì„¸ë¡œê°€ ë” ê¸´ ê²½ìš° (portrait)
                new_h = max_input_size
                new_w = int(max_input_size * orig_ratio)
            # INTER_LINEARê°€ INTER_AREAë³´ë‹¤ ì•½ê°„ ë¹ ë¦„ (ì†ë„ ìµœì í™”)
            resized_frame = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
            logging.debug(f"[CAM-{cam_id}] í”„ë ˆì„ ë¦¬ì‚¬ì´ì¦ˆ (ë‹¤ìš´ìŠ¤ì¼€ì¼, ë¹„ìœ¨ ìœ ì§€): {orig_w}x{orig_h} -> {new_w}x{new_h} (ë¹„ìœ¨: {orig_ratio:.3f})")
        else:
            # ì›ë³¸ì´ ì‘ì€ ê²½ìš°: ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì—…ìŠ¤ì¼€ì¼ë§ í•˜ì§€ ì•ŠìŒ - í’ˆì§ˆ ì €í•˜ ë°©ì§€)
            # ë©”ëª¨ë¦¬ ìµœì í™”: ë³µì‚¬ ëŒ€ì‹  ë·° ì‚¬ìš© (ë¦¬ì‚¬ì´ì¦ˆê°€ ì—†ìœ¼ë¯€ë¡œ ì›ë³¸ í”„ë ˆì„ ì¬ì‚¬ìš©)
            resized_frame = frame  # copy() ì œê±°: ë¦¬ì‚¬ì´ì¦ˆê°€ ì—†ìœ¼ë¯€ë¡œ ì›ë³¸ í”„ë ˆì„ ì¬ì‚¬ìš©
            logging.debug(f"[CAM-{cam_id}] í”„ë ˆì„ ë¦¬ì‚¬ì´ì¦ˆ ê±´ë„ˆëœ€ (ì›ë³¸ í¬ê¸° ìœ ì§€): {orig_w}x{orig_h} (ë¹„ìœ¨: {orig_ratio:.3f})")
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ë³€í™˜ì„ ìœ„í•œ ìŠ¤ì¼€ì¼ ê³„ì‚° (ë¦¬ì‚¬ì´ì¦ˆëœ í¬ê¸° ê¸°ì¤€)
        # ì •í™•í•œ ìŠ¤ì¼€ì¼ ê³„ì‚°: ì›ë³¸ í¬ê¸° / ë¦¬ì‚¬ì´ì¦ˆëœ í¬ê¸°
        resized_w = resized_frame.shape[1]
        resized_h = resized_frame.shape[0]
        w_scale = orig_w / resized_w
        h_scale = orig_h / resized_h
        
        # ìŠ¤ì¼€ì¼ ê°’ ë¡œê¹… (ë””ë²„ê¹…ìš©)
        logging.debug(f"[CAM-{cam_id}] ìŠ¤ì¼€ì¼ ê³„ì‚°: ì›ë³¸={orig_w}x{orig_h}, ë¦¬ì‚¬ì´ì¦ˆ={resized_w}x{resized_h}, w_scale={w_scale:.4f}, h_scale={h_scale:.4f}")
        perf_timings['resize'] = (time.time() - resize_start) * 1000  # ms

        # 3. ì²˜ë¦¬ëœ í”„ë ˆì„ ìƒì„± (ë Œë”ë§ì´ í•„ìš”í•  ë•Œë§Œ ë³µì‚¬)
        # ë©”ëª¨ë¦¬ ìµœì í™”: ë Œë”ë§ì´ í•„ìš”í•  ë•Œë§Œ ë³µì‚¬ (ë‚˜ì¤‘ì— ë³µì‚¬)
        processed_frame = None  # ë‚˜ì¤‘ì— í•„ìš”í•  ë•Œë§Œ ë³µì‚¬
        renderer = utils.TextRenderer(frame.shape)

        # 4. ëª¨ë“  ëª¨ë¸ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (ê°œë³„ ì‹¤í–‰ í›„ ê²°ê³¼ë§Œ í•©ì¹˜ê¸°)
        model_start = time.time()
        
        logging.debug(f"[CAM-{cam_id}] í”„ë ˆì„ ì²˜ë¦¬ ì‹œì‘ (ìˆ˜ì‹  í¬ê¸°: {len(frame_bytes)} bytes)")

        # SafetySystemì€ ì´ë¯¸ ìœ„ì—ì„œ ë½ìœ¼ë¡œ ì½ì—ˆìœ¼ë¯€ë¡œ ì¬ì‚¬ìš© (1013ë²ˆ ì¤„ ì œê±°)
        # safety_system ë³€ìˆ˜ëŠ” ì´ë¯¸ 929ë²ˆ ì¤„ì—ì„œ ì„¤ì •ë¨
        
        # GPU ìµœì í™” ì„¤ì • (half precision, ë°°ì¹˜ ì²˜ë¦¬ ë“±)
        base_half_precision = config.SystemConfig.ENABLE_HALF_PRECISION and 'cuda' in str(safety_system.device)
        
        # YOLO ëª¨ë¸ ì…ë ¥ í¬ê¸° ì„¤ì • (ëª¨ë¸ë³„ë¡œ ë‹¤ë¦„)
        # TensorRT ì—”ì§„ì´ 640x640ìœ¼ë¡œ ë¹Œë“œë˜ì—ˆìœ¼ë¯€ë¡œ 640x640 ì‚¬ìš© (ê³ ì • í¬ê¸°)
        # ë™ì  ì…ë ¥ í¬ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì—”ì§„ ë¹Œë“œ í¬ê¸°ì™€ ì¼ì¹˜í•´ì•¼ í•¨
        violation_imgsz = 640  # Violation ëª¨ë¸: 640x640 (TensorRT ì—”ì§„ í¬ê¸°ì™€ ì¼ì¹˜)
        pose_imgsz = 640       # Pose ëª¨ë¸: 640x640 (TensorRT ì—”ì§„ í¬ê¸°ì™€ ì¼ì¹˜)
        logging.debug(f"TensorRT ì—”ì§„ ì‚¬ìš©: Violation={violation_imgsz}x{violation_imgsz}, Pose={pose_imgsz}x{pose_imgsz}")
        
        # FPS ê¸°ë°˜ max_det ë™ì  ì¡°ì • (ì¸ì›ì´ ë§ì„ ë•Œ FPS ì €í•˜ ë°©ì§€)
        # NMS ì²˜ë¦¬ ì‹œê°„ì´ max_detì— ë¹„ë¡€í•˜ì—¬ ì¦ê°€í•˜ë¯€ë¡œ, FPSê°€ ë‚®ì„ ë•Œ max_detë¥¼ ë‚®ì¶° ì²˜ë¦¬ ì†ë„ í–¥ìƒ
        violation_max_det = 50  # ê¸°ë³¸ê°’
        pose_max_det = 30  # ê¸°ë³¸ê°’
        try:
            with frame_stats_lock:
                cam_stats = frame_stats.get(cam_id, {})
                recent_frames = cam_stats.get('recent_frame_times', [])
                if len(recent_frames) >= 2:
                    time_span = recent_frames[-1] - recent_frames[0]
                    if time_span > 0:
                        current_fps = (len(recent_frames) - 1) / time_span
                        
                        # FPS ê¸°ë°˜ max_det ì¡°ì • (NMS ì²˜ë¦¬ ì‹œê°„ ìµœì í™”, ë” ê³µê²©ì ì¸ ê°ì†Œ)
                        if current_fps >= 30:
                            violation_max_det = 30  # ë†’ì€ FPS: 50 -> 30 (40% ê°ì†Œ)
                            pose_max_det = 20       # ë†’ì€ FPS: 30 -> 20 (33% ê°ì†Œ)
                        elif current_fps >= 25:
                            violation_max_det = 25  # ì¤‘ê°„ FPS: 35 -> 25 (29% ê°ì†Œ)
                            pose_max_det = 18       # ì¤‘ê°„ FPS: 25 -> 18 (28% ê°ì†Œ)
                        elif current_fps >= 20:
                            violation_max_det = 20  # ì¤‘ê°„ FPS: 35 -> 20 (43% ê°ì†Œ)
                            pose_max_det = 15       # ì¤‘ê°„ FPS: 25 -> 15 (40% ê°ì†Œ)
                        elif current_fps >= 15:
                            violation_max_det = 15  # ë‚®ì€ FPS: 25 -> 15 (40% ê°ì†Œ)
                            pose_max_det = 12       # ë‚®ì€ FPS: 18 -> 12 (33% ê°ì†Œ)
                        else:
                            violation_max_det = 12  # ë§¤ìš° ë‚®ì€ FPS: 20 -> 12 (40% ê°ì†Œ)
                            pose_max_det = 10       # ë§¤ìš° ë‚®ì€ FPS: 15 -> 10 (33% ê°ì†Œ)
                        
                        logging.debug(f"[CAM-{cam_id}] FPS ê¸°ë°˜ max_det ì¡°ì •: FPS={current_fps:.1f}, Violation={violation_max_det}, Pose={pose_max_det}")
        except Exception:
            pass  # ì˜ˆì™¸ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
        
        # ì˜¤ì¸ì‹ ë°©ì§€ë¥¼ ìœ„í•´ NMS IoUì™€ max_detë„ ì¡°ì •
        # ì˜¤ì¸ì‹ ë°©ì§€ë¥¼ ìœ„í•´ NMS IoUì™€ max_det ì¡°ì •
        # ONNX ëª¨ë¸ì€ ONNX Runtimeì„ ì‚¬ìš©í•˜ë¯€ë¡œ PyTorch device íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬í•˜ì§€ ì•ŠìŒ
        # ONNX Runtimeì€ ëª¨ë¸ ë¡œë“œ ì‹œ ì´ë¯¸ GPU/CPUë¥¼ ì„¤ì •í–ˆìœ¼ë¯€ë¡œ device íŒŒë¼ë¯¸í„° ë¶ˆí•„ìš”
        # PyTorch CUDAê°€ ê°ì§€ë˜ì§€ ì•Šì•„ë„ ONNX Runtime CUDA ProviderëŠ” ì‚¬ìš© ê°€ëŠ¥í•  ìˆ˜ ìˆìŒ
        # YOLOê°€ device íŒŒë¼ë¯¸í„°ë¥¼ ìš”êµ¬í•˜ë¯€ë¡œ, ONNX ëª¨ë¸ì¼ ë•ŒëŠ” None ë˜ëŠ” ì „ë‹¬í•˜ì§€ ì•ŠìŒ
        violation_kwargs = {
            'conf': config.Thresholds.YOLO_CONFIDENCE,
            'verbose': False,
            'iou': 0.55,  # NMS IoU (0.5 -> 0.55, ì¤‘ë³µ ë°•ìŠ¤ ì œê±° ê°•í™”, ì˜¤ì¸ì‹ ë°©ì§€)
            'max_det': violation_max_det,  # FPS ê¸°ë°˜ ë™ì  ì¡°ì • (ê¸°ë³¸ê°’: 50)
            # device íŒŒë¼ë¯¸í„° ì œê±°: ONNX ëª¨ë¸ì€ ONNX Runtimeì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬
            # YOLOê°€ ë‚´ë¶€ì ìœ¼ë¡œ deviceë¥¼ ìš”êµ¬í•˜ë©´ Noneìœ¼ë¡œ ì„¤ì •í•˜ê±°ë‚˜ ì „ë‹¬í•˜ì§€ ì•ŠìŒ
        }
        pose_kwargs = {
            'conf': config.Thresholds.POSE_CONFIDENCE,
            'verbose': False,
            'iou': 0.55,  # NMS IoU (0.5 -> 0.55, ì¤‘ë³µ ë°•ìŠ¤ ì œê±° ê°•í™”, ì˜¤ì¸ì‹ ë°©ì§€)
            'max_det': pose_max_det,  # FPS ê¸°ë°˜ ë™ì  ì¡°ì • (ê¸°ë³¸ê°’: 30)
            # device íŒŒë¼ë¯¸í„° ì œê±°: ONNX ëª¨ë¸ì€ ONNX Runtimeì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬
            # YOLOê°€ ë‚´ë¶€ì ìœ¼ë¡œ deviceë¥¼ ìš”êµ¬í•˜ë©´ Noneìœ¼ë¡œ ì„¤ì •í•˜ê±°ë‚˜ ì „ë‹¬í•˜ì§€ ì•ŠìŒ
        }
        
        # ONNX ëª¨ë¸ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ í•­ìƒ imgsz ì„¤ì •
        violation_kwargs.update({
            'half': base_half_precision,
            'imgsz': violation_imgsz,  # Violation ëª¨ë¸: 832x832
        })
        pose_kwargs.update({
            'half': base_half_precision,
            'imgsz': pose_imgsz,  # Pose ëª¨ë¸: 832x832
        })
        
        # ì–¼êµ´ ì¸ì‹ ëª¨ë¸ ë° DB ê°€ì ¸ì˜¤ê¸° (ë³‘ë ¬ ì‹¤í–‰ ì¤€ë¹„)
        face_model = safety_system.face_model
        face_analyzer = safety_system.face_analyzer  # buffalo_l (ì‹¤ì œ ì‚¬ìš©!)
        fast_recognizer = safety_system.fast_recognizer  # í´ë°±ìš©
        face_database = safety_system.face_database
        
        # ğŸ” ë””ë²„ê·¸: face_analyzer ìƒíƒœ í™•ì¸ (ì²˜ìŒ 1ë²ˆë§Œ)
        if not hasattr(process_single_frame, '_face_analyzer_logged'):
            logging.warning(f"ğŸ” [ì´ˆê¸°í™”] face_analyzer={face_analyzer is not None}, face_database={face_database is not None}")
            process_single_frame._face_analyzer_logged = True
        
        # ì–¼êµ´ íƒì§€ ê°„ê²© ì²´í¬ (ë³‘ë ¬ ì‹¤í–‰ ì „ì— í™•ì¸)
        should_detect_faces_global = True
        with face_detection_lock:
            current_frame = frame_stats.get(cam_id, {}).get('frame_count', 0)
            last_frame = last_face_detection_frame.get(cam_id, -config.Thresholds.FACE_DETECTION_INTERVAL)
            if current_frame - last_frame < config.Thresholds.FACE_DETECTION_INTERVAL:
                should_detect_faces_global = False
        
        # ëª¨ë“  ëª¨ë¸ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (ê°œë³„ ì‹¤í–‰ í›„ ê²°ê³¼ë§Œ í•©ì¹˜ê¸°)
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ëŠ” í•„ìš”ì‹œì—ë§Œ (ë§¤ í”„ë ˆì„ì€ ì˜¤ë²„í—¤ë“œ)
        # 100 í”„ë ˆì„ë§ˆë‹¤ í•œ ë²ˆì”©ë§Œ ì •ë¦¬ (ë©€í‹° GPU ì§€ì›)
        # ONNX Runtimeì€ ìì²´ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ í•˜ë¯€ë¡œ PyTorch ë©”ëª¨ë¦¬ ì •ë¦¬ëŠ” ì„ íƒì 
        if 'cuda' in str(safety_system.device) and frame_stats.get(cam_id, {}).get('frame_count', 0) % 100 == 0:
            # PyTorch CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ (í˜¸í™˜ì„±ì„ ìœ„í•´, ONNX Runtimeì€ ìì²´ ê´€ë¦¬)
            try:
                if torch.cuda.is_available():
                    for gpu_id in range(torch.cuda.device_count()):
                        torch.cuda.empty_cache()
            except:
                pass
        
        # GPU ìµœê³  ì„±ëŠ¥ ì„¤ì • (ë©€í‹° GPU ì§€ì›)
        # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” ë°°ì¹˜ ì²˜ë¦¬ê°€ ì˜¤íˆë ¤ ì§€ì—°ì„ ìœ ë°œí•˜ë¯€ë¡œ ë°°ì¹˜ íŒŒë¼ë¯¸í„° ì œê±°
        if 'cuda' in str(safety_system.device):
            if not safety_system.violation_uses_trt:
                violation_kwargs.update({
                    'half': True,  # Half precision í™œì„±í™” (GPU ì„±ëŠ¥ í–¥ìƒ)
                    'agnostic_nms': False
                    # ë°°ì¹˜ íŒŒë¼ë¯¸í„° ì œê±°: ì‹¤ì‹œê°„ ì²˜ë¦¬ì—ì„œëŠ” ì¦‰ì‹œ ì²˜ë¦¬ (ë°°ì¹˜=1)ê°€ ê°€ì¥ ë¹ ë¦„
                })
            if not safety_system.pose_uses_trt:
                pose_kwargs.update({
                    'half': True,  # Half precision í™œì„±í™” (GPU ì„±ëŠ¥ í–¥ìƒ)
                    'agnostic_nms': False
                    # ë°°ì¹˜ íŒŒë¼ë¯¸í„° ì œê±°: ì‹¤ì‹œê°„ ì²˜ë¦¬ì—ì„œëŠ” ì¦‰ì‹œ ì²˜ë¦¬ (ë°°ì¹˜=1)ê°€ ê°€ì¥ ë¹ ë¦„
                })
        else:
            if not safety_system.violation_uses_trt:
                violation_kwargs['half'] = False
            if not safety_system.pose_uses_trt:
                pose_kwargs['half'] = False
        
        # ëŒë‹¤ í•¨ìˆ˜ ì˜¤ë²„í—¤ë“œ ì œê±°: ì§ì ‘ í•¨ìˆ˜ í˜¸ì¶œë¡œ ìµœì í™”
        # ì„±ëŠ¥ ìµœì í™”: resized_frame ì‚¬ìš© (ì´ë¯¸ ìµœì  í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆë¨)
        # YOLOê°€ imgsz íŒŒë¼ë¯¸í„°ë¡œ ì¶”ê°€ ë¦¬ì‚¬ì´ì¦ˆë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ resized_frame ì‚¬ìš©ì´ ë” ë¹ ë¦„
        def run_violation_model():
            # ONNX ëª¨ë¸ì€ ONNX Runtimeì„ ì‚¬ìš©í•˜ë¯€ë¡œ device íŒŒë¼ë¯¸í„° ì—†ì´ ì‹¤í–‰
            # ONNX Runtimeì€ ëª¨ë¸ ë¡œë“œ ì‹œ ì´ë¯¸ GPU/CPU Providerë¥¼ ì„¤ì •í–ˆìœ¼ë¯€ë¡œ ìë™ ì²˜ë¦¬
            return safety_system.violation_model(resized_frame, **violation_kwargs)
        
        def run_pose_model():
            # ONNX ëª¨ë¸ì€ ONNX Runtimeì„ ì‚¬ìš©í•˜ë¯€ë¡œ device íŒŒë¼ë¯¸í„° ì—†ì´ ì‹¤í–‰
            # ONNX Runtimeì€ ëª¨ë¸ ë¡œë“œ ì‹œ ì´ë¯¸ GPU/CPU Providerë¥¼ ì„¤ì •í–ˆìœ¼ë¯€ë¡œ ìë™ ì²˜ë¦¬
            return safety_system.pose_model(resized_frame, **pose_kwargs)
        
        def run_fall_model():
            # Fall Detection ëª¨ë¸ í™œì„±í™” - ì™„ì „íˆ ì“°ëŸ¬ì§„ ì‚¬ëŒì€ Poseê°€ ëª»ì¡ìœ¼ë¯€ë¡œ í•„ìš”
            if safety_system.fall_model is None:
                return None
            fall_kwargs = {
                'conf': 0.45,  # Fall ê°ì§€ ì„ê³„ê°’
                'iou': 0.5,
                'verbose': False,
                'classes': [1],  # Fall í´ë˜ìŠ¤ë§Œ
            }
            # â­ ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ë™ì¼í•˜ê²Œ resized_frame ì‚¬ìš© (ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§ í†µì¼)
            return safety_system.fall_model(resized_frame, **fall_kwargs)
        
        # â­ PPE(Violation)ì™€ ìœ„í—˜ ê°ì§€(Pose), Fall Detection ëª¨ë¸ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        import sys
        logging.debug(f"[CAM-{cam_id}] ğŸ”„ YOLO ëª¨ë¸ ì‹¤í–‰ ì¤€ë¹„: Violation={safety_system.violation_model is not None}, Pose={safety_system.pose_model is not None}, ì…ë ¥ í¬ê¸°={resized_frame.shape}")
        violation_future = yolo_executor.submit(run_violation_model)  # PPE ìœ„ë°˜ ê°ì§€ ëª¨ë¸ (ë³‘ë ¬)
        pose_future = yolo_executor.submit(run_pose_model)  # ìœ„í—˜ í–‰ë™ ê°ì§€ ëª¨ë¸ (ë³‘ë ¬)
        fall_future = yolo_executor.submit(run_fall_model)  # ë„˜ì–´ì§ ê°ì§€ ëª¨ë¸ (ë³‘ë ¬, ì „ì²´ í”„ë ˆì„)
        # print ì œê±° - ë¡œê·¸ íŒŒì¼ì—ë§Œ ê¸°ë¡ (ì½˜ì†” ë…¸ì´ì¦ˆ ê°ì†Œ)
        logging.debug(f"[CAM-{cam_id}] YOLO ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘")
        
        # ========================================
        # 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸: ì–¼êµ´ ê°ì§€ëŠ” ì¡°ê±´ë¶€ ì‹¤í–‰
        # ========================================
        # Step 1: YOLO Pose + PPEëŠ” í•­ìƒ ì‹¤í–‰ (ìœ„ì—ì„œ ì´ë¯¸ ì‹¤í–‰ë¨)
        # Step 2: ì–¼êµ´ ê°ì§€ëŠ” ì•„ë˜ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•  ë•Œë§Œ ì‹¤í–‰
        #   - ì¡°ê±´ A: ìœ„ë°˜ ì‚¬í•­ ë°œìƒ ì‹œ (ì“°ëŸ¬ì§ OR ì•ˆì „ì¥ë¹„ ë¯¸ì°©ìš©)
        #   - ì¡°ê±´ B: ìƒˆë¡œìš´ ì‚¬ëŒ ë“±ì¥ ì‹œ (Track IDê°€ ì²˜ìŒ ìƒì„±ë˜ì—ˆì„ ë•Œ)
        #   - ì¡°ê±´ C: ì£¼ê¸°ì  í™•ì¸ (í•´ë‹¹ IDì— ëŒ€í•´ 1ì´ˆì— 1ë²ˆë§Œ)
        
        face_detection_future = None
        should_run_face_detection = False
        face_detection_reason = "none"
        
        # â­ buffalo_lë¡œ ì–¼êµ´ ê°ì§€ (face_analyzer ì‚¬ìš©)
        # face_modelì€ None (YOLO Face ëŒ€ì‹  buffalo_l ì‚¬ìš©)
        if face_analyzer is None:
            logging.debug(f"ğŸ” [CAM-{cam_id}] ì–¼êµ´ ê°ì§€ ìŠ¤í‚µ: face_analyzer=None")
        elif not should_detect_faces_global:
            with face_detection_lock:
                current_frame = frame_stats.get(cam_id, {}).get('frame_count', 0)
                last_frame = last_face_detection_frame.get(cam_id, -config.Thresholds.FACE_DETECTION_INTERVAL)
                frame_interval = current_frame - last_frame
            logging.debug(f"ğŸ” [CAM-{cam_id}] ì–¼êµ´ ê°ì§€ ìŠ¤í‚µ: ê°„ê²© ë¶€ì¡± (í˜„ì¬={current_frame}, ë§ˆì§€ë§‰={last_frame}, ê°„ê²©={frame_interval}, ìµœì†Œ={config.Thresholds.FACE_DETECTION_INTERVAL})")
        
        # â­ buffalo_l ì‚¬ìš© ì‹œ YOLO Face ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
        # face_analyzerê°€ ìˆìœ¼ë©´ save_face_resultì—ì„œ buffalo_lë¡œ í†µí•© ì²˜ë¦¬
        if face_analyzer is not None and should_detect_faces_global:
            logging.debug(f"ğŸ¦¬ [CAM-{cam_id}] buffalo_l ì–¼êµ´ ê°ì§€ ì˜ˆì • (save_face_resultì—ì„œ ì²˜ë¦¬)")
            # YOLO Face ëŒ€ì‹  buffalo_l ì‚¬ìš© (save_face_resultì—ì„œ face_analyzer.get() í˜¸ì¶œ)
            # face_detection_futureëŠ” Noneìœ¼ë¡œ ìœ ì§€ (buffalo_lì€ ë³„ë„ ì²˜ë¦¬)
        
        # ëª¨ë“  ëª¨ë¸ ê²°ê³¼ ëŒ€ê¸° (ë³‘ë ¬ ì‹¤í–‰, íƒ€ì„ì•„ì›ƒ ìµœì í™”: ì‹¤ì‹œê°„ ì²˜ë¦¬ ì†ë„ í–¥ìƒ)
        # GPU í™˜ê²½ì—ì„œëŠ” ì²« ì‹¤í–‰ ì‹œ warmupì´ í•„ìš”í•˜ë¯€ë¡œ íƒ€ì„ì•„ì›ƒ ì¦ê°€
        # CPU ëª¨ë“œì—ì„œëŠ” ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ì–´ì„œ íƒ€ì„ì•„ì›ƒì„ ë” ê¸¸ê²Œ ì„¤ì •
        if not hasattr(process_single_frame, '_model_warmed_up'):
            # ì²« ì‹¤í–‰: warmupì„ ìœ„í•´ íƒ€ì„ì•„ì›ƒ ì¦ê°€
            model_timeout = 5.0 if 'cuda' in str(safety_system.device) else 10.0  # CPU: 6.0 -> 10.0 (ë³‘ëª© í•´ê²°)
            process_single_frame._model_warmed_up = True
        else:
            # ì´í›„ ì‹¤í–‰: ì •ìƒ ì²˜ë¦¬ ì†ë„ (CPU ëª¨ë“œì—ì„œëŠ” ë” ê¸´ íƒ€ì„ì•„ì›ƒ í•„ìš”)
            model_timeout = 3.0 if 'cuda' in str(safety_system.device) else 10.0  # CPU: 4.0 -> 10.0 (ë³‘ëª© í•´ê²°)
        
        # ========================================
        # ë™ê¸°ì‹ ì²˜ë¦¬: ëª¨ë¸ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë ¸ë‹¤ê°€ ë°”ë¡œ ë Œë”ë§ (ì‹¤ì‹œê°„, ì§€ì—° ì—†ìŒ)
        # ========================================
        
        # 1. Violation + Pose ëª¨ë¸ ê²°ê³¼ ë™ì‹œ ê¸°ë‹¤ë¦¬ê¸° (íƒ€ì„ì•„ì›ƒ ì—†ìŒ)
        violation_start = time.time()
        try:
            violation_results = violation_future.result() or []
        except Exception:
            violation_results = []
        perf_timings['yolo_violation'] = (time.time() - violation_start) * 1000
        
        pose_start = time.time()
        try:
            pose_results = pose_future.result() or []
        except Exception:
            pose_results = []
        perf_timings['yolo_pose'] = (time.time() - pose_start) * 1000
        
        # 3. ê²°ê³¼ ì¦‰ì‹œ íŒŒì‹± (ìºì‹œ ì‚¬ìš© ì•ˆ í•¨)
        best_result = None
        recognized_faces = []
        violations_found = []
        all_detections = {}
        
        # Violation ê²°ê³¼ íŒŒì‹± (Person í¬í•¨)
        if violation_results and len(violation_results) > 0:
            for det in violation_results[0].boxes:
                class_id = int(det.cls[0])
                class_name = safety_system.violation_model.names[class_id]
                conf = float(det.conf[0])
                
                if class_name in config.Thresholds.IGNORED_CLASSES:
                    continue
                
                # Person í´ë˜ìŠ¤ëŠ” ë³„ë„ ì„ê³„ê°’ ì‚¬ìš©
                if class_name == 'Person':
                    class_threshold = config.Thresholds.PERSON_CONFIDENCE
                else:
                    class_threshold = config.Thresholds.CLASS_CONFIDENCE_THRESHOLDS.get(
                        class_name, config.Thresholds.YOLO_CONFIDENCE
                    )
                
                if conf >= class_threshold:
                    bbox_resized = det.xyxy[0].cpu().numpy()
                    bbox_original = bbox_resized * np.array([w_scale, h_scale, w_scale, h_scale])
                    bbox_clipped = utils.clip_bbox_xyxy(bbox_original, orig_w, orig_h)
                    if bbox_clipped is not None:
                        if class_name not in all_detections:
                            all_detections[class_name] = []
                        all_detections[class_name].append({'bbox': list(bbox_clipped), 'conf': conf})
        
        # Pose ê²°ê³¼ íŒŒì‹± (person boxes)
        person_boxes = []
        frame_area = orig_w * orig_h
        if pose_results and len(pose_results) > 0 and pose_results[0].boxes is not None:
            for idx, box in enumerate(pose_results[0].boxes.xyxy.cpu().numpy()):
                conf = float(pose_results[0].boxes.conf.cpu().numpy()[idx]) if pose_results[0].boxes.conf is not None else 0.5
                if conf >= 0.25:
                    scaled_box = box * np.array([w_scale, h_scale, w_scale, h_scale])
                    clipped = utils.clip_bbox_xyxy(scaled_box, orig_w, orig_h)
                    if clipped is not None:
                        # â­ Pose ë°•ìŠ¤ë„ ë¹„ìœ¨ í™•ì¸í•´ì„œ ë„˜ì–´ì§ ê°ì§€
                        box_w = clipped[2] - clipped[0]
                        box_h = clipped[3] - clipped[1]
                        box_area = box_w * box_h
                        box_ratio = box_w / box_h if box_h > 0 else 0
                        
                        # ë„˜ì–´ì§: ê°€ë¡œê°€ ì„¸ë¡œë³´ë‹¤ 1.6ë°° ì´ìƒ (Fall ëª¨ë¸ ë³´ì¡°, 1.3 â†’ 1.6)
                        is_fall = (box_ratio >= 1.6 and box_area >= 10000 and box_area <= frame_area * 0.4)
                        
                        if is_fall:
                            logging.warning(f"[CAM-{cam_id}] ğŸ”» Pose ë„˜ì–´ì§ ê°ì§€: ë¹„ìœ¨={box_ratio:.2f}, ë©´ì ={box_area:.0f}")
                        
                        person_boxes.append({'box': list(clipped), 'source': 'pose', 'is_fall': is_fall})
        
        # PPE Person ë°•ìŠ¤ ì¶”ê°€ (Poseê°€ ëª» ì¡ì€ ë„˜ì–´ì§„ ì‚¬ëŒ ê°ì§€)
        if 'Person' in all_detections:
            for ppe_person in all_detections['Person']:
                ppe_bbox = ppe_person['bbox']
                is_new = True
                
                # Pose ë°•ìŠ¤ì™€ ë¹„êµ (ì¤‘ë³µ ì œê±°)
                for pose_box in person_boxes:
                    if utils.calculate_iou(ppe_bbox, pose_box['box']) > 0.3:
                        is_new = False
                        break
                
                if is_new:
                    # ë„˜ì–´ì§„ ì‚¬ëŒ í›„ë³´ íŒë‹¨ (ì—„ê²©í•œ ê¸°ì¤€)
                    box_w = ppe_bbox[2] - ppe_bbox[0]
                    box_h = ppe_bbox[3] - ppe_bbox[1]
                    box_area = box_w * box_h
                    box_ratio = box_w / box_h if box_h > 0 else 0
                    
                    # ë„˜ì–´ì§ ì¡°ê±´ (Fall ëª¨ë¸ ë³´ì¡°):
                    # 1. ê°€ë¡œê°€ ì„¸ë¡œë³´ë‹¤ 1.8ë°° ì´ìƒ (Fall ëª¨ë¸ì´ ì£¼ë ¥ì´ë¯€ë¡œ ë³´ì¡°ëŠ” ì—„ê²©í•˜ê²Œ)
                    # 2. ë°•ìŠ¤ ë©´ì ì´ ìµœì†Œ 12000 í”½ì…€ ì´ìƒ (ë…¸ì´ì¦ˆ ì œì™¸)
                    # 3. ë°•ìŠ¤ ë©´ì ì´ í”„ë ˆì„ì˜ 30% ì´í•˜
                    frame_area = orig_w * orig_h
                    is_fall = (box_ratio >= 1.8 and 
                              box_area >= 12000 and 
                              box_area <= frame_area * 0.30)
                    
                    person_boxes.append({
                        'box': ppe_bbox,
                        'source': 'ppe',
                        'is_fall': is_fall
                    })
                    
                    if is_fall:
                        logging.warning(f"[CAM-{cam_id}] ğŸ”» ë„˜ì–´ì§ ê°ì§€: ë¹„ìœ¨={box_ratio:.2f}, ë©´ì ={box_area:.0f}")
        
        # PPE ìœ„ë°˜ í™•ì¸ (person_boxì™€ PPE ë§¤ì¹­)
        for person_data in person_boxes:
            box = person_data['box']
            x1, y1, x2, y2 = map(int, box)
            is_fall = person_data.get('is_fall', False)
            ppe_violations = []
            
            # ë„˜ì–´ì§ì´ë©´ ë°”ë¡œ ìœ„ë°˜ ì¶”ê°€
            if is_fall:
                ppe_violations.append('ë„˜ì–´ì§')
            
            # ì•ˆì „ëª¨ ì²´í¬ (ê±°ë¦¬ ê¸°ë°˜ ë§¤ì¹­ - ë” ì •í™•)
            if 'NO-Hardhat' in all_detections:
                person_cx, person_cy = (x1 + x2) / 2, (y1 + y2) / 2
                person_h = y2 - y1
                for ppe in all_detections['NO-Hardhat']:
                    ppe_bbox = ppe['bbox']
                    ppe_cx = (ppe_bbox[0] + ppe_bbox[2]) / 2
                    ppe_cy = (ppe_bbox[1] + ppe_bbox[3]) / 2
                    # ì•ˆì „ëª¨ëŠ” ë¨¸ë¦¬ ìœ„ì¹˜ (ìƒë‹¨ 30%)ì— ìˆì–´ì•¼ í•¨
                    if abs(ppe_cx - person_cx) < (x2 - x1) * 0.5 and ppe_cy < y1 + person_h * 0.4:
                        ppe_violations.append('ì•ˆì „ëª¨')
                        break
            
            # ë§ˆìŠ¤í¬ ì²´í¬ ë¹„í™œì„±í™” (ì‚¬ìš©ì ìš”ì²­)
            # if 'NO-Mask' in all_detections:
            #     pass
            
            # ì•ˆì „ì¡°ë¼ ì²´í¬ (ê±°ë¦¬ ê¸°ë°˜)
            if 'NO-Safety Vest' in all_detections:
                person_cx = (x1 + x2) / 2
                person_h = y2 - y1
                for ppe in all_detections['NO-Safety Vest']:
                    ppe_bbox = ppe['bbox']
                    ppe_cx = (ppe_bbox[0] + ppe_bbox[2]) / 2
                    ppe_cy = (ppe_bbox[1] + ppe_bbox[3]) / 2
                    # ì•ˆì „ì¡°ë¼ëŠ” ëª¸í†µ ìœ„ì¹˜ (30%~80%)ì— ìˆì–´ì•¼ í•¨
                    if abs(ppe_cx - person_cx) < (x2 - x1) * 0.6 and y1 + person_h * 0.2 < ppe_cy < y1 + person_h * 0.8:
                        ppe_violations.append('ì•ˆì „ì¡°ë¼')
                        break
            
            # ëª¨ë“  ì‚¬ëŒ ë°•ìŠ¤ í‘œì‹œ (ìœ„ë°˜ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´)
            has_violation = bool(ppe_violations) or is_fall
            recognized_faces.append({
                'box': [x1, y1, x2, y2],
                'bbox': [x1, y1, x2, y2],
                'name': 'Unknown',
                'ppe_violations': ppe_violations,
                'isViolation': has_violation
            })
            if has_violation:
                violations_found.append({
                    'person_box': [x1, y1, x2, y2],
                    'violations': ppe_violations
                })
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì–¼êµ´ ì¸ì‹ ê³„ì† ì‹¤í–‰ (ê²°ê³¼ëŠ” ìºì‹œì— ì €ì¥)
        _submit_models_background_simple(
            frame, resized_frame, cam_id, timestamp, safety_system,
            violation_future, pose_future, fall_future, face_detection_future,
            violation_kwargs, pose_kwargs, face_model, face_analyzer, fast_recognizer,
            face_database, orig_w, orig_h, w_scale, h_scale
        )
        
        perf_timings['face_recognition'] = 0.0  # ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬
        
        # ìºì‹œì—ì„œ ì–¼êµ´ ì¸ì‹ ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (ìˆìœ¼ë©´)
        from state import get_latest_cache
        cached_result = get_latest_cache(cam_id, max_age=CACHE_TTL)
            
        # ìºì‹œì—ì„œ ì–¼êµ´ ì´ë¦„ ê°€ì ¸ì™€ì„œ ë™ê¸°ì‹ ê²°ê³¼ì™€ ë³‘í•©
        if cached_result:
            cached_faces = cached_result.get('recognized_faces', [])
            for rf in recognized_faces:
                rf_cx = (rf['box'][0] + rf['box'][2]) / 2
                rf_cy = (rf['box'][1] + rf['box'][3]) / 2
                
                for cf in cached_faces:
                    cf_box = cf.get('box', [])
                    cf_name = cf.get('name', 'Unknown')
                    if len(cf_box) == 4 and cf_name != 'Unknown':
                        cf_cx = (cf_box[0] + cf_box[2]) / 2
                        cf_cy = (cf_box[1] + cf_box[3]) / 2
                        distance = ((rf_cx - cf_cx)**2 + (rf_cy - cf_cy)**2)**0.5
                        if distance < 150:  # 150í”½ì…€ ì´ë‚´ë©´ ê°™ì€ ì‚¬ëŒ
                            rf['name'] = cf_name
                            break
        
        # ë™ê¸°ì‹ ê²°ê³¼ë¡œ ë Œë”ë§ (ìºì‹œ ëŒ€ê¸° ì—†ìŒ)
        if recognized_faces or violations_found:
            if processed_frame is None:
                processed_frame = frame.copy()
            
            # â­â­ ì¤‘ë³µ ë°•ìŠ¤ ì œê±° (ê°™ì€ ìœ„ì¹˜ì— ì—¬ëŸ¬ ë°•ìŠ¤ ë°©ì§€)
            # violations_found ì¤‘ë³µ ì œê±°
            deduplicated_violations = []
            used_violation_boxes = set()
            for vf in violations_found:
                box = vf.get('person_box', [])
                if len(box) != 4:
                    continue
                box_key = (int(box[0]//20), int(box[1]//20), int(box[2]//20), int(box[3]//20))
                if box_key not in used_violation_boxes:
                    used_violation_boxes.add(box_key)
                    deduplicated_violations.append(vf)
            violations_found = deduplicated_violations
            
            # recognized_faces ì¤‘ë³µ ì œê±°
            # ë°•ìŠ¤ ì¢Œí‘œê°€ ìœ ì‚¬í•˜ë©´ (20í”½ì…€ ë‹¨ìœ„ ê·¸ë£¹í™”) í•˜ë‚˜ë§Œ ìœ ì§€
            deduplicated_faces = []
            used_boxes = set()
            for rf in recognized_faces:
                box = rf.get('box', [])
                if len(box) != 4:
                    continue
                box_key = (int(box[0]//20), int(box[1]//20), int(box[2]//20), int(box[3]//20))  # 20í”½ì…€ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
                if box_key not in used_boxes:
                    used_boxes.add(box_key)
                    deduplicated_faces.append(rf)
                else:
                    # ì´ë¯¸ ìˆëŠ” ë°•ìŠ¤ë©´ ì´ë¦„ì´ ìˆëŠ” ìª½ ìš°ì„ 
                    for i, existing_rf in enumerate(deduplicated_faces):
                        existing_box = existing_rf.get('box', [])
                        if len(existing_box) == 4:
                            existing_key = (int(existing_box[0]//20), int(existing_box[1]//20), int(existing_box[2]//20), int(existing_box[3]//20))
                            if existing_key == box_key:
                                # ì´ë¦„ì´ ìˆëŠ” ìª½ ìš°ì„ 
                                if rf.get('name', 'Unknown') != 'Unknown' and existing_rf.get('name', 'Unknown') == 'Unknown':
                                    deduplicated_faces[i] = rf
                                break
            
            recognized_faces = deduplicated_faces
            
            # ê³µí†µ ë Œë”ë§ í•¨ìˆ˜ í˜¸ì¶œ
            processed_frame = render_frame_results(
                processed_frame,
                recognized_faces,
                violations_found,
                cam_id,
                orig_w,
                orig_h
            )
        
            # ë Œë”ë§ ì™„ë£Œ í›„ ë°”ë¡œ ë¦¬í„´ (í”„ë ˆì„ ë³´ì¥ ë°©ì‹)
            perf_timings['total'] = (time.time() - total_start) * 1000
            logging.debug(f"[CAM-{cam_id}] ìºì‹œ ê²°ê³¼ ë Œë”ë§ ì™„ë£Œ: ì–¼êµ´={len(recognized_faces)}ê°œ, ìœ„ë°˜={len(violations_found)}ê°œ, ì²˜ë¦¬ì‹œê°„={perf_timings['total']:.1f}ms")
            
            _, buffer = cv2.imencode('.jpg', processed_frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
            return buffer.tobytes(), {
                "timestamp": time.time(),  # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                "recognized_faces": recognized_faces,
                "violations": violations_found,
                "violation_count": len(violations_found),
                "performance": perf_timings,
                "frame_width": orig_w,
                "frame_height": orig_h,
                "cam_id": cam_id
            }
        
        # í”„ë ˆì„ ë³´ì¥ ë°©ì‹: ìºì‹œì— ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í”„ë ˆì„ ë°˜í™˜
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ì²˜ë¦¬ ì¤‘ì´ë¯€ë¡œ, ë‹¤ìŒ í”„ë ˆì„ì—ì„œ ê²°ê³¼ê°€ í‘œì‹œë¨
        if not best_result:
            logging.debug(f"[CAM-{cam_id}] ìºì‹œì— ê²°ê³¼ ì—†ìŒ, ì›ë³¸ í”„ë ˆì„ ë°˜í™˜")
            perf_timings['total'] = (time.time() - total_start) * 1000
            _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
            return buffer.tobytes(), {
                "timestamp": time.time(),  # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                "recognized_faces": [],
                "violations": [],
                "violation_count": 0,
                "performance": perf_timings,
                "frame_width": orig_w,
                "frame_height": orig_h,
                "cam_id": cam_id
            }
        
        # ì•„ë˜ ì½”ë“œëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ (ìœ„ì—ì„œ ëª¨ë‘ ë¦¬í„´)
        # ê¸°ì¡´ íŒŒì‹± ë¡œì§ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ë¨
        parse_start = time.time()
        perf_timings['parse_results'] = 0.0
        
        if False:  # ê¸°ì¡´ íŒŒì‹± ë¡œì§ ë¹„í™œì„±í™” (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬)
            # 5-1. YOLO violation ê²°ê³¼ íŒŒì‹±
            all_detections = {}
            filtered_count = 0  # if ë¸”ë¡ ë°–ì—ì„œ ì´ˆê¸°í™”
            low_conf_count = 0  # if ë¸”ë¡ ë°–ì—ì„œ ì´ˆê¸°í™”
            if violation_results and len(violation_results) > 0:
                violation_box_count = len(violation_results[0].boxes) if violation_results[0].boxes is not None else 0
                logging.debug(f"[CAM-{cam_id}] ğŸ“¦ YOLO Violation ê²°ê³¼ íŒŒì‹± ì‹œì‘: {violation_box_count}ê°œ ë°•ìŠ¤, confidence ì„ê³„ê°’={config.Thresholds.YOLO_CONFIDENCE}")
                for det in violation_results[0].boxes:
                    class_id = int(det.cls[0])
                    class_name = safety_system.violation_model.names[class_id]
                    conf = float(det.conf[0])
                    
                    # Safety Con ë“± ì˜¤íƒì§€ í´ë˜ìŠ¤ í•„í„°ë§
                    if class_name in config.Thresholds.IGNORED_CLASSES:
                        filtered_count += 1
                        logging.debug(f"[CAM-{cam_id}] í´ë˜ìŠ¤ í•„í„°ë§: {class_name} (IGNORED_CLASSES)")
                        continue
                    
                    # í´ë˜ìŠ¤ë³„ confidence ì„ê³„ê°’ ì ìš© (ì•ˆì „ì¡°ë¼ ì¸ì‹ ê°œì„ )
                    class_threshold = config.Thresholds.CLASS_CONFIDENCE_THRESHOLDS.get(
                        class_name, 
                        config.Thresholds.YOLO_CONFIDENCE
                    )
                    
                    if conf >= class_threshold:
                        # ë¦¬ì‚¬ì´ì¦ˆëœ í”„ë ˆì„ ê¸°ì¤€ ì¢Œí‘œë¥¼ ì›ë³¸ í”„ë ˆì„ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
                        bbox_resized = det.xyxy[0].cpu().numpy()
                        bbox_original = bbox_resized * np.array([w_scale, h_scale, w_scale, h_scale])
                        bbox_clipped = utils.clip_bbox_xyxy(bbox_original, orig_w, orig_h)
                        if bbox_clipped is not None:
                            if class_name not in all_detections:
                                all_detections[class_name] = []
                            # clip_bbox_xyxyëŠ” tupleì„ ë°˜í™˜í•˜ë¯€ë¡œ list()ë¡œ ë³€í™˜
                            all_detections[class_name].append({'bbox': list(bbox_clipped), 'conf': conf})
                            # PPE í´ë˜ìŠ¤ì¸ ê²½ìš°ì—ë§Œ ìƒì„¸ ë¡œê¹… (ë””ë²„ê¹…)
                            is_ppe_class = class_name in ['Hardhat', 'NO-Hardhat', 'Mask', 'NO-Mask', 'Safety Vest', 'NO-Safety Vest']
                            if is_ppe_class:
                                logging.info(f"[CAM-{cam_id}] âœ… PPE ê°ì§€: {class_name} (conf={conf:.3f}, threshold={class_threshold:.3f}, bbox={bbox_clipped})")
                            else:
                                logging.debug(f"[CAM-{cam_id}] âœ… Violation ê°ì§€: {class_name} (conf={conf:.3f}, threshold={class_threshold:.3f}, bbox={bbox_clipped})")
                    else:
                        low_conf_count += 1
                        logging.debug(f"[CAM-{cam_id}] ë‚®ì€ confidence: {class_name} (conf={conf:.3f} < {class_threshold:.3f})")
            
            if filtered_count > 0 or low_conf_count > 0:
                logging.info(f"[CAM-{cam_id}] í•„í„°ë§ í†µê³„: í•„í„°ë§ë¨={filtered_count}ê°œ, ë‚®ì€ confidence={low_conf_count}ê°œ, ìµœì¢… ê°ì§€={sum(len(v) for v in all_detections.values())}ê°œ")
            else:
                logging.warning(f"[CAM-{cam_id}] âš ï¸ YOLO Violation ëª¨ë¸ ê²°ê³¼ ì—†ìŒ ë˜ëŠ” ë¹ˆ ê²°ê³¼ (violation_results={violation_results})")
            
            if all_detections:
                logging.debug(f"[CAM-{cam_id}] ì´ {sum(len(v) for v in all_detections.values())}ê°œ violation ê°ì§€: {list(all_detections.keys())}")
            
            # 5-2. ì–¼êµ´ ê°ì§€ ê²°ê³¼ ì²˜ë¦¬ (YOLO ê²°ê³¼ë¥¼ InsightFace í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
            recognized_faces = []
            violations_found = []
            face_detected_boxes = []  # ì–¼êµ´ ê¸°ë°˜ ë°•ìŠ¤ (ë’¤ì— ìˆëŠ” ì‚¬ëŒìš©)
            
            # YOLO ì–¼êµ´ ê°ì§€ ê²°ê³¼ë¥¼ InsightFace í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            faces_in_frame = []
            result = None  # ì´ˆê¸°í™” ì¶”ê°€ (NameError ë°©ì§€)
            if yolo_face_results and len(yolo_face_results) > 0:
                result = yolo_face_results[0]
            
            # ë””ë²„ê¹…: YOLO Face ì „ì²´ ê°ì§€ ê²°ê³¼ ë¡œê¹…
            total_boxes = 0
            has_keypoints = False
            if result is not None:
                total_boxes = len(result.boxes) if result.boxes is not None else 0
                has_keypoints = result.keypoints is not None
                logging.debug(f"ğŸ” [CAM-{cam_id}] YOLO Face ì „ì²´ ê°ì§€: ë°•ìŠ¤={total_boxes}ê°œ, í‚¤í¬ì¸íŠ¸={has_keypoints}")
            else:
                logging.debug(f"ğŸ” [CAM-{cam_id}] YOLO Face ê²°ê³¼ ì—†ìŒ (yolo_face_results=None ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸)")
            
            # ì–¼êµ´ ê°ì§€ ìˆ˜ ê¸°ë¡ (ë™ì  confidence ì¡°ì •ìš©) - ê²°ê³¼ê°€ ì—†ì–´ë„ 0 ê¸°ë¡
            if not hasattr(process_single_frame, '_face_detection_history'):
                process_single_frame._face_detection_history = {}
            if cam_id not in process_single_frame._face_detection_history:
                process_single_frame._face_detection_history[cam_id] = []
            process_single_frame._face_detection_history[cam_id].append(total_boxes)
            # ìµœê·¼ 30í”„ë ˆì„ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ìµœì í™”)
            if len(process_single_frame._face_detection_history[cam_id]) > 30:
                process_single_frame._face_detection_history[cam_id] = process_single_frame._face_detection_history[cam_id][-30:]
            
            if result is not None and result.boxes is not None and len(result.boxes) > 0:
                # Keypoints ì „ì²´ ì¶”ì¶œ (ìˆìœ¼ë©´)
                all_keypoints = None
                if hasattr(result, 'keypoints') and result.keypoints is not None:
                    try:
                        all_keypoints = result.keypoints.xy.cpu().numpy()
                    except Exception as e:
                        logging.debug(f"Keypoints ì „ì²´ ë³€í™˜ ì‹¤íŒ¨: {e}")

                for i, box in enumerate(result.boxes):
                    conf = float(box.conf[0])
                    if conf >= config.Thresholds.FACE_DETECTION_CONFIDENCE:
                        bbox = box.xyxy[0].cpu().numpy()
                        fx1, fy1, fx2, fy2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])
                        
                        # ìŠ¤ì¼€ì¼ë§ ì ìš© (ë¦¬ì‚¬ì´ì¦ˆëœ í”„ë ˆì„ ê¸°ì¤€ì´ë¯€ë¡œ ì›ë³¸ í¬ê¸°ë¡œ ë³€í™˜)
                        fx1 = int(fx1 * w_scale)
                        fy1 = int(fy1 * h_scale)
                        fx2 = int(fx2 * w_scale)
                        fy2 = int(fy2 * h_scale)
                        
                        # í”„ë ˆì„ ê²½ê³„ ë‚´ë¡œ í´ë¦¬í•‘
                        fx1 = max(0, min(fx1, orig_w))
                        fy1 = max(0, min(fy1, orig_h))
                        fx2 = max(0, min(fx2, orig_w))
                        fy2 = max(0, min(fy2, orig_h))
                        
                        # YOLO keypoints ì¶”ì¶œ (ì¸¡ë©´ ì–¼êµ´ ì§€ì›: ì¼ë¶€ í‚¤í¬ì¸íŠ¸ë§Œ ìˆì–´ë„ ì²˜ë¦¬)
                        kps = None
                        if all_keypoints is not None and len(all_keypoints) > i:
                            try:
                                kps = all_keypoints[i].copy() # (5, 2) ë˜ëŠ” ì¼ë¶€ë§Œ ìˆì„ ìˆ˜ ìˆìŒ
                                # ìŠ¤ì¼€ì¼ë§ ì ìš©
                                kps[:, 0] *= w_scale
                                kps[:, 1] *= h_scale
                                
                                # ì¸¡ë©´ ì–¼êµ´ ì§€ì›: í‚¤í¬ì¸íŠ¸ê°€ 2ê°œ ì´ìƒì´ë©´ ì‚¬ìš© (5ê°œ ë¯¸ë§Œë„ í—ˆìš©)
                                if len(kps) < 2:
                                    logging.debug(f"í‚¤í¬ì¸íŠ¸ê°€ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤: {len(kps)}ê°œ (ì–¼êµ´ ë°•ìŠ¤ë§Œ ì‚¬ìš©)")
                                    kps = None  # í‚¤í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ Noneìœ¼ë¡œ ì„¤ì • (ì–¼êµ´ ë°•ìŠ¤ ê¸°ë°˜ ì •ë ¬ ì‚¬ìš©)
                            except Exception as e:
                                logging.debug(f"ê°œë³„ Keypoints ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                                kps = None
                        
                        # í‚¤í¬ì¸íŠ¸ê°€ ì—†ì–´ë„ ì–¼êµ´ ë°•ìŠ¤ë§Œìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥ (ì¸¡ë©´ ì–¼êµ´ ì§€ì›)
                        # kpsê°€ Noneì´ì–´ë„ ì–¼êµ´ ì¸ì‹ ì‹œë„ (ì–¼êµ´ ë°•ìŠ¤ ê¸°ë°˜ ì •ë ¬ ì‚¬ìš©)

                        # ê°„ë‹¨í•œ ì–¼êµ´ ê°ì²´ ìƒì„± (bbox, det_score, kps ì†ì„±)
                        class SimpleFace:
                            def __init__(self, bbox, det_score, kps=None):
                                self.bbox = bbox
                                self.det_score = det_score
                                self.kps = kps
                        
                        face_obj = SimpleFace([fx1, fy1, fx2, fy2], conf, kps)
                        faces_in_frame.append(face_obj)
                        
                        # ì–¼êµ´ ë°•ìŠ¤ë¥¼ ì‚¬ëŒ ë°•ìŠ¤ë¡œ í™•ì¥ (ì–¼êµ´ í¬ê¸°ì˜ 3-4ë°°)
                        face_w = fx2 - fx1
                        face_h = fy2 - fy1
                        
                        # ìµœì†Œ ì–¼êµ´ í¬ê¸° í•„í„°ë§ ì™„í™” (ë” ì‘ì€ ì–¼êµ´ë„ ê°ì§€)
                        min_face_size = config.Thresholds.MIN_FACE_SIZE  # 16í”½ì…€ ì´ìƒ
                        if face_w < min_face_size or face_h < min_face_size:
                            # ë„ˆë¬´ ì‘ì€ ì–¼êµ´ì€ ê±´ë„ˆë›°ì§€ë§Œ, ë¡œê¹…ì€ í•˜ì§€ ì•ŠìŒ (ë…¸ì´ì¦ˆ ë°©ì§€)
                            continue
                        
                        # ì–¼êµ´ ì¤‘ì‹¬ì 
                        face_cx = (fx1 + fx2) / 2
                        face_cy = (fy1 + fy2) / 2
                        # ì–¼êµ´ í¬ê¸°ì˜ 3.5ë°°ë¡œ í™•ì¥ (ìƒì²´ í¬í•¨)
                        expanded_w = face_w * 3.5
                        expanded_h = face_h * 3.5
                        # í™•ì¥ëœ ë°•ìŠ¤ (ì–¼êµ´ì´ ìƒë‹¨ ì¤‘ì•™ì— ìœ„ì¹˜)
                        expanded_x1 = max(0, int(face_cx - expanded_w / 2))
                        expanded_y1 = max(0, int(face_cy - face_h * 0.3))  # ì–¼êµ´ì´ ìƒë‹¨ì—
                        expanded_x2 = min(orig_w, int(face_cx + expanded_w / 2))
                        expanded_y2 = min(orig_h, int(face_cy + expanded_h * 0.7))  # í•˜ì²´ í¬í•¨
                        
                        # ìœ íš¨í•œ ë°•ìŠ¤ì¸ì§€ í™•ì¸
                        if expanded_x2 > expanded_x1 and expanded_y2 > expanded_y1:
                            face_detected_boxes.append({
                                'box': (expanded_x1, expanded_y1, expanded_x2, expanded_y2),
                                'face_bbox': (fx1, fy1, fx2, fy2),
                                'face': face_obj,
                                'confidence': conf
                            })
                            logging.debug(f"ì–¼êµ´ ê¸°ë°˜ ë°•ìŠ¤ ê°ì§€: ({expanded_x1}, {expanded_y1}, {expanded_x2}, {expanded_y2}), ì–¼êµ´ í¬ê¸°: {face_w:.1f}x{face_h:.1f}")
            
            perf_timings['parse_results'] = (time.time() - parse_start) * 1000  # ms

            # 7. ì‚¬ëŒ ê°ì§€ ë° ìƒíƒœ í™•ì¸
            if pose_results and pose_results[0].boxes is not None and len(pose_results[0].boxes) > 0:
                boxes = pose_results[0].boxes.xyxy.cpu().numpy()
                logging.info(f"[CAM-{cam_id}] YOLO Pose ëª¨ë¸ ê²°ê³¼: {len(boxes)}ëª… ê°ì§€")

            # ì¤‘ë³µ ì‚¬ëŒ ë°•ìŠ¤ ì œê±° (NMS ìœ ì‚¬) - ê²¹ì¹¨ì´ í° ë°•ìŠ¤ëŠ” í° ë°•ìŠ¤ í•˜ë‚˜ë§Œ ìœ ì§€
            # ìµœì í™”: ê±°ë¦¬ ê¸°ë°˜ í•„í„°ë§ ë¨¼ì € ìˆ˜í–‰í•˜ì—¬ ë¶ˆí•„ìš”í•œ IoU ê³„ì‚° ë°©ì§€
            try:
                if boxes is not None and len(boxes) > 1:
                    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
                    order = np.argsort(-areas)  # í° ë°•ìŠ¤ ìš°ì„ 
                    keep_indices = []
                    suppressed = np.zeros(len(boxes), dtype=bool)
                    
                    for idx in order:
                        if suppressed[idx]:
                            continue
                        keep_indices.append(idx)
                        x1, y1, x2, y2 = boxes[idx]
                        box_center_x = (x1 + x2) / 2
                        box_center_y = (y1 + y2) / 2
                        box_diagonal = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5
                        max_distance = box_diagonal * 1.2  # ë°•ìŠ¤ ëŒ€ê°ì„ ì˜ 1.2ë°° ì´ë‚´ë§Œ ê³ ë ¤
                        
                        for j in order:
                            if j == idx or suppressed[j]:
                                continue
                            
                            # ê±°ë¦¬ ê¸°ë°˜ í•„í„°ë§ ë¨¼ì € ìˆ˜í–‰ (IoU ê³„ì‚°ë³´ë‹¤ ë¹ ë¦„)
                            jx1, jy1, jx2, jy2 = boxes[j]
                            j_center_x = (jx1 + jx2) / 2
                            j_center_y = (jy1 + jy2) / 2
                            center_distance = ((box_center_x - j_center_x) ** 2 + (box_center_y - j_center_y) ** 2) ** 0.5
                            
                            # ê±°ë¦¬ê°€ ë„ˆë¬´ ë©€ë©´ IoU ê³„ì‚° ìƒëµ (ì„±ëŠ¥ í–¥ìƒ)
                            if center_distance > max_distance:
                                continue
                            
                            # IoU ê³„ì‚° (ê±°ë¦¬ í•„í„°ë§ í†µê³¼í•œ ê²½ìš°ë§Œ)
                            iou = utils.calculate_iou((x1, y1, x2, y2), tuple(boxes[j]))
                            # configì—ì„œ IoU ì„ê³„ê°’ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ 0.5ë¡œ ë” ì ê·¹ì ì¸ ì¤‘ë³µ ì œê±°)
                            iou_threshold = config.Thresholds.IOU_PERSON_DEDUP
                            if iou > iou_threshold:  # ë†’ì€ ê²¹ì¹¨ì€ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
                                suppressed[j] = True
                    
                    boxes = boxes[keep_indices]
                    if pose_results[0].keypoints is not None:
                        keypoints_list = [pose_results[0].keypoints[i] for i in keep_indices]
            except Exception:
                pass
            
            # pose_resultsê°€ ë¹„ì–´ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
            if pose_results and len(pose_results) > 0:
                keypoints_list = pose_results[0].keypoints if pose_results[0].keypoints else None
                confidences = pose_results[0].boxes.conf.cpu().numpy() if pose_results[0].boxes.conf is not None else None
                tracker_ids = pose_results[0].boxes.id.cpu().numpy() if pose_results[0].boxes.id is not None else None
            else:
                keypoints_list = None
                confidences = None
                tracker_ids = None
                boxes = np.array([])  # ë¹ˆ ë°•ìŠ¤ ë°°ì—´

            # í•„í„°ë§ëœ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
            valid_indices = []
            
            for i, box in enumerate(boxes):
                scaled_box_np = box * np.array([w_scale, h_scale, w_scale, h_scale])
                clipped_box = utils.clip_bbox_xyxy(scaled_box_np, orig_w, orig_h)
                if clipped_box is None:
                    continue

                # í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ í•„í„°ë§
                if keypoints_list is not None and len(keypoints_list) > i:
                    kpts = keypoints_list[i]
                    if kpts is not None and kpts.conf is not None:
                        visible_kpts_count = torch.sum(kpts.conf > config.Thresholds.POSE_CONFIDENCE).item()
                        if visible_kpts_count < config.Thresholds.MIN_VISIBLE_KEYPOINTS:
                            continue
                else:
                    continue
                
                # ëª¨ë“  í•„í„°ë§ì„ í†µê³¼í•œ ê²½ìš°, ì¸ë±ìŠ¤ë¥¼ ì €ì¥
                valid_indices.append(i)
            
            logging.info(f"[CAM-{cam_id}] í•„í„°ë§ í›„ ìœ íš¨í•œ ì‚¬ëŒ ìˆ˜: {len(valid_indices)}")

            # ì‚¬ëŒ ë°•ìŠ¤ ì¢Œí‘œë¥¼ ì›ë³¸ í”„ë ˆì„ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§ ë° í•„í„°ë§
            scaled_person_boxes = []
            valid_person_indices = []  # ìœ íš¨í•œ ì‚¬ëŒ ë°•ìŠ¤ ì¸ë±ìŠ¤
            filtered_boxes = []
            filtered_keypoints = []
            filtered_confidences = []
            filtered_tracker_ids = []

            for i, box in enumerate(boxes):
                scaled_box_np = box * np.array([w_scale, h_scale, w_scale, h_scale])
                clipped_box = utils.clip_bbox_xyxy(scaled_box_np, orig_w, orig_h)
                if clipped_box is None:
                    continue
                x1, y1, x2, y2 = map(int, clipped_box) # ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜

                original_box = clipped_box  # ì›ë³¸ ë°•ìŠ¤ ì €ì¥
                box_w = x2 - x1
                box_h = y2 - y1
                box_area = box_w * box_h
                aspect_ratio = box_w / box_h if box_h > 0 else 0

                # 1. í‚¤í¬ì¸íŠ¸ í™•ì¸ ë° ë°•ìŠ¤ ì¡°ì • (í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ë°•ìŠ¤ë¥¼ ë” ì •í™•í•˜ê²Œ ì¡°ì •)
                # ë©€ë¦¬ ìˆëŠ” ì‚¬ëŒë„ ê°ì§€í•˜ê¸° ìœ„í•´ ì™„í™” ì¡°ê±´ ì™„í™”
                num_valid_kpts = 0
                has_head_or_shoulders = False
                refined_box = None
                if keypoints_list is not None and i < len(keypoints_list):
                    keypoints = keypoints_list[i]
                    if keypoints is not None and keypoints.conf is not None:
                        conf_arr = keypoints.conf[0].cpu().numpy()
                        valid_kpts_mask = conf_arr > config.Thresholds.POSE_CONFIDENCE
                        num_valid_kpts = int(np.sum(valid_kpts_mask))
                        # nose(0), left_shoulder(5), right_shoulder(6)
                        idxs = [0, 5, 6]
                        for idx in idxs:
                            if idx < len(valid_kpts_mask) and valid_kpts_mask[idx]:
                                has_head_or_shoulders = True
                                break
                        
                        # í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ë°•ìŠ¤ ì¡°ì • (ì—¬ëŸ¬ ì‚¬ëŒ ë¶„ë¦¬ ê°œì„ )
                        if num_valid_kpts >= 4:  # ì¶©ë¶„í•œ í‚¤í¬ì¸íŠ¸ê°€ ìˆì„ ë•Œë§Œ ì¡°ì •
                            refined_box = utils.refine_box_from_keypoints(
                                keypoints, original_box, orig_w, orig_h, padding_ratio=0.15
                            )
                            if refined_box is not None:
                                # ì¡°ì •ëœ ë°•ìŠ¤ ì‚¬ìš©
                                x1, y1, x2, y2 = refined_box
                                box_w = x2 - x1
                                box_h = y2 - y1
                                box_area = box_w * box_h
                                aspect_ratio = box_w / box_h if box_h > 0 else 0
                                logging.debug(f"í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë°•ìŠ¤ ì¡°ì •: {original_box} -> {refined_box}")

                # ì˜¤íƒì§€ ë°©ì§€: í‚¤í¬ì¸íŠ¸ ê²€ì¦ ê°•í™”
                # ì™„í™” ì¡°ê±´ì„ ë” ì—„ê²©í•˜ê²Œ ì ìš©í•˜ì—¬ ì˜ì/ì±…ìƒ ë“± ì˜¤íƒì§€ ë°©ì§€
                # ìµœì†Œ 6ê°œ í‚¤í¬ì¸íŠ¸ì™€ ë¨¸ë¦¬/ì–´ê¹¨ê°€ ìˆì–´ì•¼ ì™„í™” ì¡°ê±´ ì ìš©
                use_relaxed = (num_valid_kpts >= 6 and has_head_or_shoulders) and (box_area < 5000)
                min_w = config.Thresholds.RELAXED_MIN_PERSON_BOX_WIDTH if use_relaxed else config.Thresholds.MIN_PERSON_BOX_WIDTH
                min_h = config.Thresholds.RELAXED_MIN_PERSON_BOX_HEIGHT if use_relaxed else config.Thresholds.MIN_PERSON_BOX_HEIGHT
                min_area = config.Thresholds.RELAXED_MIN_PERSON_BOX_AREA if use_relaxed else config.Thresholds.MIN_PERSON_BOX_AREA
                max_ar = config.Thresholds.RELAXED_MAX_PERSON_ASPECT_RATIO if use_relaxed else config.Thresholds.MAX_PERSON_ASPECT_RATIO
                min_ar = config.Thresholds.RELAXED_MIN_PERSON_ASPECT_RATIO if use_relaxed else config.Thresholds.MIN_PERSON_ASPECT_RATIO

                # 2. ìµœì†Œ í¬ê¸° í•„í„°ë§ (ë„ˆë¬´ ì‘ì€ ë°•ìŠ¤ëŠ” ì œì™¸)
                if box_w < min_w or box_h < min_h or box_area < min_area:
                    logging.debug(f"ì‚¬ëŒ ë°•ìŠ¤ í•„í„°ë§ (í¬ê¸° ì‘ìŒ): {box_w}x{box_h}, ë©´ì ={box_area} (relaxed={use_relaxed})")
                    continue

                # 3. ì¢…íš¡ë¹„ í•„í„°ë§ (ì†ì²˜ëŸ¼ ì„¸ë¡œë¡œ ê¸´ ê²ƒ ë˜ëŠ” ë„ˆë¬´ ê°€ë¡œë¡œ ê¸´ ê²ƒ ì œì™¸)
                if aspect_ratio > max_ar or aspect_ratio < min_ar:
                    logging.debug(f"ì‚¬ëŒ ë°•ìŠ¤ í•„í„°ë§ (ì¢…íš¡ë¹„ ì´ìƒ): {aspect_ratio:.2f} (relaxed={use_relaxed})")
                    continue

                # 4. í‚¤í¬ì¸íŠ¸ ê²€ì¦ (ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´ í•„í„°ë§ ê°•í™”)
                # ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´(ì˜ì, ì•ˆì „ëª¨ ë“±)ë¥¼ ì œì™¸í•˜ê¸° ìœ„í•´ ë¨¸ë¦¬/ì–´ê¹¨ê°€ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•¨
                if not has_head_or_shoulders:
                    # ë¨¸ë¦¬ë‚˜ ì–´ê¹¨ê°€ ì—†ìœ¼ë©´ ìµœì†Œ 6ê°œ í‚¤í¬ì¸íŠ¸ í•„ìš” (ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´ í•„í„°ë§)
                    min_kpts_required = 6
                    logging.debug(f"ì‚¬ëŒ ë°•ìŠ¤ í•„í„°ë§ (ë¨¸ë¦¬/ì–´ê¹¨ ì—†ìŒ): {num_valid_kpts} < {min_kpts_required} (ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´ ì˜ì‹¬)")
                    continue
                else:
                    # ë¨¸ë¦¬/ì–´ê¹¨ê°€ ìˆìœ¼ë©´ ì™„í™”ëœ ì¡°ê±´ ì‚¬ìš©
                    min_kpts_required = 4 if use_relaxed else config.Thresholds.MIN_VISIBLE_KEYPOINTS
                
                if num_valid_kpts < min_kpts_required:
                    logging.debug(f"ì‚¬ëŒ ë°•ìŠ¤ í•„í„°ë§ (í‚¤í¬ì¸íŠ¸ ë¶€ì¡±): {num_valid_kpts} < {min_kpts_required} (has_head={has_head_or_shoulders}, relaxed={use_relaxed})")
                    continue
                
                # 5. ì¶”ê°€ ê²€ì¦: í‚¤í¬ì¸íŠ¸ ë¶„í¬ í™•ì¸ (ì˜¤íƒì§€ ë°©ì§€)
                # ìƒì²´ í‚¤í¬ì¸íŠ¸(ë¨¸ë¦¬, ì–´ê¹¨, íŒ”ê¿ˆì¹˜)ê°€ ì¼ì • ë¹„ìœ¨ ì´ìƒ ìˆì–´ì•¼ í•¨
                upper_body_ratio = 0.0
                keypoint_spread_ok = True
                if keypoints_list is not None and i < len(keypoints_list):
                    keypoints = keypoints_list[i]
                    if keypoints is not None and keypoints.conf is not None:
                        conf_arr = keypoints.conf[0].cpu().numpy()
                        points = keypoints.xy[0].cpu().numpy()
                        
                        # ìƒì²´ í‚¤í¬ì¸íŠ¸ ì¸ë±ìŠ¤: nose(0), eyes(1,2), ears(3,4), shoulders(5,6), elbows(7,8)
                        upper_body_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8]
                        upper_body_valid = sum(1 for idx in upper_body_indices if idx < len(conf_arr) and conf_arr[idx] > config.Thresholds.POSE_CONFIDENCE)
                        upper_body_ratio = upper_body_valid / len(upper_body_indices) if len(upper_body_indices) > 0 else 0
                        
                        # ìƒì²´ í‚¤í¬ì¸íŠ¸ê°€ 30% ë¯¸ë§Œì´ë©´ ì˜¤íƒì§€ ê°€ëŠ¥ì„± ë†’ìŒ (25% -> 30% ê°•í™”, ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´ í•„í„°ë§)
                        # ì•ˆì „ëª¨, ì˜ì ë“±ì€ ìƒì²´ í‚¤í¬ì¸íŠ¸ê°€ ê±°ì˜ ì—†ìœ¼ë¯€ë¡œ ì´ë¥¼ ê°•í™”
                        if upper_body_ratio < 0.30:
                            logging.debug(f"ì‚¬ëŒ ë°•ìŠ¤ í•„í„°ë§ (ìƒì²´ í‚¤í¬ì¸íŠ¸ ë¶€ì¡±): {upper_body_valid}/{len(upper_body_indices)} ({upper_body_ratio:.2%})")
                            continue
                        
                        # í‚¤í¬ì¸íŠ¸ ë¶„ì‚° í™•ì¸: í•œ ì ì— ëª°ë ¤ìˆìœ¼ë©´ ì˜¤íƒì§€ (ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´ í•„í„°ë§ ê°•í™”)
                        valid_points = points[conf_arr > config.Thresholds.POSE_CONFIDENCE]
                        if len(valid_points) >= 3:
                            kpt_x_std = np.std(valid_points[:, 0])
                            kpt_y_std = np.std(valid_points[:, 1])
                            # ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ (í‘œì¤€í¸ì°¨ < 12px) í•œ ì ì— ëª°ë ¤ìˆìŒ (8px -> 12px ê°•í™”, ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´ í•„í„°ë§)
                            # ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´(ì•ˆì „ëª¨, ì˜ì ë“±)ëŠ” í‚¤í¬ì¸íŠ¸ê°€ í•œ ê³³ì— ì§‘ì¤‘ë˜ì–´ ìˆìŒ
                            min_spread = 12.0
                            if box_area > 10000:  # í° ë°•ìŠ¤ëŠ” ë” í° ë¶„ì‚° ìš”êµ¬
                                min_spread = 20.0
                            elif box_area > 50000:  # ë§¤ìš° í° ë°•ìŠ¤ëŠ” ë” ì—„ê²©
                                min_spread = 30.0
                            
                            if kpt_x_std < min_spread or kpt_y_std < min_spread:
                                logging.debug(f"ì‚¬ëŒ ë°•ìŠ¤ í•„í„°ë§ (í‚¤í¬ì¸íŠ¸ ë¶„ì‚° ë¶€ì¡±, ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´ ì˜ì‹¬): std_x={kpt_x_std:.1f}, std_y={kpt_y_std:.1f}, box_area={box_area}")
                                keypoint_spread_ok = False
                                continue
                            
                            # ì¶”ê°€ ê²€ì¦: í‚¤í¬ì¸íŠ¸ê°€ ë°•ìŠ¤ ì „ì²´ì— ë¶„ì‚°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                            # ë°•ìŠ¤ í¬ê¸° ëŒ€ë¹„ í‚¤í¬ì¸íŠ¸ ë¶„ì‚° ë¹„ìœ¨ í™•ì¸
                            box_width = x2 - x1
                            box_height = y2 - y1
                            if box_width > 0 and box_height > 0:
                                # í‚¤í¬ì¸íŠ¸ ë¶„ì‚°ì´ ë°•ìŠ¤ í¬ê¸°ì˜ ì¼ì • ë¹„ìœ¨ ì´ìƒì´ì–´ì•¼ í•¨
                                spread_ratio_x = kpt_x_std / box_width
                                spread_ratio_y = kpt_y_std / box_height
                                # ë°•ìŠ¤ í¬ê¸° ëŒ€ë¹„ ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ (10% ë¯¸ë§Œ) ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´ ì˜ì‹¬
                                if spread_ratio_x < 0.10 and spread_ratio_y < 0.10:
                                    logging.debug(f"ì‚¬ëŒ ë°•ìŠ¤ í•„í„°ë§ (ë°•ìŠ¤ ëŒ€ë¹„ í‚¤í¬ì¸íŠ¸ ë¶„ì‚° ë¶€ì¡±): spread_x={spread_ratio_x:.2%}, spread_y={spread_ratio_y:.2%}")
                                    continue

                # 4. violation_modelì—ì„œ íƒì§€ëœ ì‘ì€ ê°ì²´ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
                should_filter = False
                for class_name, detections in all_detections.items():
                    # 'person' í´ë˜ìŠ¤ëŠ” ì œì™¸ (pose_modelê³¼ ì¤‘ë³µ)
                    if class_name.lower() == 'person':
                        continue
                    # ì•ˆì „ ì¥ë¹„ëŠ” ì œì™¸
                    is_safety_gear = any(class_name in item.values() for item in config.Constants.SAFETY_RULES_MAP.values())
                    if is_safety_gear:
                        continue

                    # ì‘ì€ ê°ì²´(machinery, hand ë“±)ì™€ ê²¹ì¹˜ë©´ í•„í„°ë§
                    for det in detections:
                        if det and 'bbox' in det and det['bbox'] and len(det['bbox']) == 4:
                            dx1, dy1, dx2, dy2 = det['bbox']
                            det_area = (dx2 - dx1) * (dy2 - dy1)

                            # ì‘ì€ ê°ì²´ê°€ ì‚¬ëŒ ë°•ìŠ¤ ë‚´ë¶€ë‚˜ ê°€ê¹Œì´ ìˆìœ¼ë©´ í•„í„°ë§
                            det_center_x = (dx1 + dx2) / 2
                            det_center_y = (dy1 + dy2) / 2

                            if (x1 <= det_center_x <= x2 and y1 <= det_center_y <= y2) or \
                               (dx1 < x2 and dx2 > x1 and dy1 < y2 and dy2 > y1):
                                iou = utils.calculate_iou((x1, y1, x2, y2), (dx1, dy1, dx2, dy2))
                                # ì‘ì€ ê°ì²´ê°€ ì‚¬ëŒ ë°•ìŠ¤ ë©´ì ì˜ 20% ì´ìƒ ì°¨ì§€í•˜ê³  IOUê°€ 0.15 ì´ìƒì´ë©´ ì œì™¸ (ë” ì—„ê²©í•˜ê²Œ)
                                # ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´(ì˜ì, ì•ˆì „ëª¨ ë“±)ê°€ ì‚¬ëŒ ë°•ìŠ¤ì™€ ê²¹ì¹˜ë©´ ì œì™¸
                                if det_area > box_area * 0.2 and iou > 0.15:
                                    logging.debug(f"ì‚¬ëŒ ë°•ìŠ¤ í•„í„°ë§ (ì‘ì€ ê°ì²´ì™€ ê²¹ì¹¨, ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´ ì˜ì‹¬): {class_name}, IOU={iou:.2f}, det_area={det_area}, box_area={box_area}")
                                    should_filter = True
                                    break
                                # IOUê°€ ë§¤ìš° ë†’ìœ¼ë©´(0.4 ì´ìƒ) ì‚¬ëŒ ë°•ìŠ¤ ë‚´ë¶€ì— ê°ì²´ê°€ ìˆëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ì œì™¸
                                if iou > 0.4:
                                    logging.debug(f"ì‚¬ëŒ ë°•ìŠ¤ í•„í„°ë§ (ë†’ì€ IOUë¡œ ì¸í•œ ê²¹ì¹¨, ì‚¬ëŒì´ ì•„ë‹Œ ê°ì²´ ì˜ì‹¬): {class_name}, IOU={iou:.2f}")
                                    should_filter = True
                                    break

                    if should_filter:
                        break

                if should_filter:
                    continue

                # í™”ë©´ ê°€ì¥ìë¦¬ì— ë¶™ì€ ì¢ì€ ë°•ìŠ¤ í•„í„°ë§ (ì˜¤íƒì§€ ë°©ì§€ - ì™¼ìª½ êµ¬ì„ ì˜¤íƒì§€ í•´ê²°ìš©)
                # ì˜ˆ: x1ì´ 0ì— ê°€ê¹ê³  ë„ˆë¹„ê°€ 50px ë¯¸ë§Œì¸ ê²½ìš°
                if (x1 < 10 or x2 > orig_w - 10) and box_w < 60:
                     logging.debug(f"ê°€ì¥ìë¦¬ ì¢ì€ ë°•ìŠ¤ ì œê±°ë¨ (ì˜¤íƒì§€ ì˜ì‹¬): {box_w}x{box_h} at x={x1}")
                     continue

                # ì¤‘ë³µ ë°•ìŠ¤ ë°©ì§€ (scaled_person_boxesì— ì¶”ê°€í•˜ê¸° ì „ í™•ì¸)
                # YOLO Poseê°€ ë™ì¼í•œ ì‚¬ëŒì— ëŒ€í•´ ì¤‘ë³µ ë°•ìŠ¤ë¥¼ ë‚´ë±‰ëŠ” ê²½ìš° ë°©ì§€
                is_box_duplicate = False
                for existing_box in scaled_person_boxes:
                    # ê¸°ì¡´ ë°•ìŠ¤ì™€ í˜„ì¬ ë°•ìŠ¤ì˜ IoU ê³„ì‚°
                    # existing_boxëŠ” float arrayì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ intë¡œ ë³€í™˜
                    ex_x1, ex_y1, ex_x2, ex_y2 = map(int, existing_box)
                    iou = utils.calculate_iou((x1, y1, x2, y2), (ex_x1, ex_y1, ex_x2, ex_y2))
                    
                    # 70% ì´ìƒ ê²¹ì¹˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼í•˜ê³  ê±´ë„ˆëœ€
                    if iou > 0.7: 
                        is_box_duplicate = True
                        logging.debug(f"ì¤‘ë³µ ë°•ìŠ¤ í•„í„°ë§ë¨: IoU={iou:.2f}, Box={x1},{y1},{x2},{y2}")
                        break
                
                if is_box_duplicate:
                    continue

                # ëª¨ë“  í•„í„°ë§ì„ í†µê³¼í•œ ìœ íš¨í•œ ì‚¬ëŒ ë°•ìŠ¤
                scaled_person_boxes.append(scaled_box_np)
                valid_person_indices.append(i)
                filtered_boxes.append(box)
                if keypoints_list is not None and i < len(keypoints_list):
                    filtered_keypoints.append(keypoints_list[i])
                if confidences is not None:
                    filtered_confidences.append(confidences[i])
                if tracker_ids is not None:
                    filtered_tracker_ids.append(tracker_ids[i])

            # í•„í„°ë§ëœ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
            boxes = np.array(filtered_boxes) if filtered_boxes else np.array([])
            keypoints_list = filtered_keypoints if filtered_keypoints else None
            if confidences is not None:
                confidences = np.array(filtered_confidences) if filtered_confidences else np.array([])
            if tracker_ids is not None:
                tracker_ids = np.array(filtered_tracker_ids) if filtered_tracker_ids else np.array([])
            
            # ì–¼êµ´ ê¸°ë°˜ ë°•ìŠ¤ë¥¼ ê¸°ì¡´ pose ë°•ìŠ¤ì™€ ë³‘í•© (ë’¤ì— ìˆëŠ” ì‚¬ëŒ ì¶”ê°€)
            # ìµœì í™”: ê±°ë¦¬ ê¸°ë°˜ í•„í„°ë§ ë¨¼ì € ìˆ˜í–‰í•˜ì—¬ ë¶ˆí•„ìš”í•œ IoU ê³„ì‚° ë°©ì§€
            if face_detected_boxes:
                for face_box_data in face_detected_boxes:
                    fx1, fy1, fx2, fy2 = face_box_data['box']
                    face_center_x = (fx1 + fx2) / 2
                    face_center_y = (fy1 + fy2) / 2
                    face_diagonal = ((fx2 - fx1) ** 2 + (fy2 - fy1) ** 2) ** 0.5
                    max_distance = face_diagonal * 1.5  # ë°•ìŠ¤ ëŒ€ê°ì„ ì˜ 1.5ë°° ì´ë‚´ë§Œ ê³ ë ¤
                    
                    # ê¸°ì¡´ pose ë°•ìŠ¤ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸ (ê±°ë¦¬ ê¸°ë°˜ í•„í„°ë§ ë¨¼ì €)
                    is_duplicate = False
                    for existing_box in boxes:
                        ex_x1, ex_y1, ex_x2, ex_y2 = existing_box * np.array([w_scale, h_scale, w_scale, h_scale])
                        ex_clipped = utils.clip_bbox_xyxy((ex_x1, ex_y1, ex_x2, ex_y2), orig_w, orig_h)
                        if ex_clipped:
                            ex_x1, ex_y1, ex_x2, ex_y2 = ex_clipped
                            
                            # ê±°ë¦¬ ê¸°ë°˜ í•„í„°ë§ ë¨¼ì € ìˆ˜í–‰ (IoU ê³„ì‚°ë³´ë‹¤ ë¹ ë¦„)
                            ex_center_x = (ex_x1 + ex_x2) / 2
                            ex_center_y = (ex_y1 + ex_y2) / 2
                            center_distance = ((face_center_x - ex_center_x) ** 2 + (face_center_y - ex_center_y) ** 2) ** 0.5
                            
                            # ê±°ë¦¬ê°€ ë„ˆë¬´ ë©€ë©´ IoU ê³„ì‚° ìƒëµ (ì„±ëŠ¥ í–¥ìƒ)
                            if center_distance > max_distance:
                                continue
                            
                            # IoU ê³„ì‚° (ê±°ë¦¬ í•„í„°ë§ í†µê³¼í•œ ê²½ìš°ë§Œ)
                            iou = utils.calculate_iou((fx1, fy1, fx2, fy2), (ex_x1, ex_y1, ex_x2, ex_y2))
                            if iou > 0.3:  # ê¸°ì¡´ ë°•ìŠ¤ì™€ ê²¹ì¹˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
                                is_duplicate = True
                                break
                    
                    # ì¤‘ë³µì´ ì•„ë‹ˆë©´ ì–¼êµ´ ê¸°ë°˜ ë°•ìŠ¤ë¥¼ ì¶”ê°€
                    if not is_duplicate:
                        # ë¦¬ì‚¬ì´ì¦ˆëœ í”„ë ˆì„ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
                        scaled_face_box = np.array([fx1 / w_scale, fy1 / h_scale, fx2 / w_scale, fy2 / h_scale])
                        boxes = np.vstack([boxes, scaled_face_box.reshape(1, -1)]) if len(boxes) > 0 else scaled_face_box.reshape(1, -1)
                        # í‚¤í¬ì¸íŠ¸ëŠ” None (ì–¼êµ´ ê¸°ë°˜ ë°•ìŠ¤ëŠ” í‚¤í¬ì¸íŠ¸ ì—†ìŒ)
                        if keypoints_list is None:
                            keypoints_list = []
                        keypoints_list.append(None)
                        logging.debug(f"ì–¼êµ´ ê¸°ë°˜ ë°•ìŠ¤ ì¶”ê°€: ({fx1}, {fy1}, {fx2}, {fy2})")

            num_people = len(boxes)
            if num_people == 0:
                logging.debug("í•„í„°ë§ í›„ ìœ íš¨í•œ ì‚¬ëŒ íƒì§€ ì—†ìŒ")
            else:
                logging.debug(f"í•„í„°ë§ í›„ ìœ íš¨í•œ ì‚¬ëŒ ìˆ˜: {num_people}")

            # ì–¼êµ´ ì¸ì‹ ì‹œê°„ ì¸¡ì • ì‹œì‘
            face_recognition_start = time.time()

            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—… ëª©ë¡ ì¤€ë¹„
            face_recognition_tasks = []
            futures_with_index = []  # (person_data_list_index, future)
            person_data_list = []  # ìˆœì„œëŒ€ë¡œ ê²°ê³¼ë¥¼ ë§ì¶”ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸
            
            # PPE ë°•ìŠ¤ ì¤‘ë³µ ë§¤ì¹­ ë°©ì§€ë¥¼ ìœ„í•œ ì¶”ì  ì„¸íŠ¸ ì´ˆê¸°í™” (ë§¤ í”„ë ˆì„ë§ˆë‹¤ ì´ˆê¸°í™”)
            frame_state = state.get_frame_processing_state(cam_id)
            if 'used_ppe_boxes' not in frame_state:
                frame_state['used_ppe_boxes'] = set()
            frame_state['used_ppe_boxes'].clear()  # ë§¤ í”„ë ˆì„ë§ˆë‹¤ ì´ˆê¸°í™”

            # ì–¼êµ´ ì¸ì‹ ìš°ì„ ìˆœìœ„ ê³„ì‚°ì„ ìœ„í•œ ì„ì‹œ ë¦¬ìŠ¤íŠ¸
            face_recognition_candidates = []  # (priority_score, person_index, box, ...)

            faces_scheduled = 0
            # ìˆ˜ì •: valid_indices ëŒ€ì‹  scaled_person_boxesë¥¼ ìˆœíšŒí•˜ë„ë¡ ë³€ê²½ (ì¢Œí‘œ ì˜¤ë¥˜ ìˆ˜ì •)
            # scaled_person_boxesì—ëŠ” Pose ê¸°ë°˜ ë°•ìŠ¤ì™€ Face ê¸°ë°˜ ë°•ìŠ¤ê°€ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŒ
            num_pose_boxes = len(filtered_boxes) # Pose ê¸°ë°˜ ë°•ìŠ¤ ê°œìˆ˜ (Face ê¸°ë°˜ì€ ê·¸ ë’¤ì— ì¶”ê°€ë¨)
            
            for i, scaled_box in enumerate(scaled_person_boxes):
                # ë³€ìˆ˜ ì´ˆê¸°í™”
                # Pose ê¸°ë°˜ì¸ì§€ Face ê¸°ë°˜ì¸ì§€ êµ¬ë¶„
                if i < num_pose_boxes:
                    # Pose ê¸°ë°˜ ë°•ìŠ¤
                    original_idx = valid_person_indices[i] # ì›ë³¸ ì¸ë±ìŠ¤
                    person_id_text = f"P{original_idx}"
                    # tracker_idsëŠ” í•„í„°ë§ëœ Pose ë°•ìŠ¤ë“¤ì— ëŒ€í•œ ID ë¦¬ìŠ¤íŠ¸ì„ (928ë¼ì¸ì—ì„œ ì—…ë°ì´íŠ¸ë¨)
                    tracker_id = int(tracker_ids[i]) if tracker_ids is not None and len(tracker_ids) > i else None
                else:
                    # Face ê¸°ë°˜ ë°•ìŠ¤
                    person_id_text = f"F{i}"
                    tracker_id = None
                
                # scaled_boxëŠ” ì´ë¯¸ ì›ë³¸ í”„ë ˆì„ í¬ê¸°ì— ë§ì¶°ì ¸ ìˆìŒ (float -> int ë³€í™˜)
                x1, y1, x2, y2 = map(int, scaled_box)
                
                person_area = max(1, (x2 - x1) * (y2 - y1))
                person_height = max(1, y2 - y1)
                height_ratio = person_height / max(1, orig_h)
                matched_face = None
                face_quality_ok = False
                has_violation_or_danger = False
                immediate_recognition = False
                opportunistic_recognition = False
                cache_skip_recognition = False # ë³€ìˆ˜ ì´ˆê¸°í™” ì¶”ê°€

                # ì‚¬ëŒ ë°•ìŠ¤ ì˜ì—­ ì¶”ì¶œ (ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•´ ë³µì‚¬)
                person_img_for_detection = frame[y1:y2, x1:x2].copy()
                if person_img_for_detection.size == 0:
                    continue

                person_keypoints = keypoints_list[i] if keypoints_list and len(keypoints_list) > i else None

                person_data_list.append({
                    'index': i,
                    'person_id': person_id_text,
                    'box': (x1, y1, x2, y2),
                    'img': person_img_for_detection,
                    'keypoints': person_keypoints,
                    'tracker_id': tracker_id
                })
                
                # PPE ë° ìœ„í—˜ í–‰ë™ ë¶„ì„
                ppe_violations, ppe_boxes = _process_ppe_detection((x1, y1, x2, y2), all_detections, frame_state['used_ppe_boxes'])
                person_data_list[-1]['ppe_violations'] = ppe_violations
                person_data_list[-1]['ppe_boxes'] = ppe_boxes
                
                # ìœ„í—˜ í–‰ë™(ë„˜ì–´ì§) ê°ì§€ - í‚¤í¬ì¸íŠ¸ê°€ ì—†ì–´ë„ ë°•ìŠ¤ ë¹„ìœ¨ë¡œ ê°ì§€ ì‹œë„
                is_dangerous_detected, violation_type = False, ""
                try:
                    person_box_key = _generate_person_box_key(cam_id, None, x1, y1, x2, y2)
                    # FallSafe ëª¨ë¸ìš© person_crop (ì´ë¯¸ ìƒì„±ëœ person_img_for_detection ì‚¬ìš©)
                    person_crop = person_img_for_detection if person_img_for_detection.size > 0 else None
                    # FallSafe ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
                    fall_model = getattr(safety_system, 'fall_model', None)
                    
                    # í‚¤í¬ì¸íŠ¸ê°€ ì—†ì–´ë„ ë°•ìŠ¤ ë¹„ìœ¨ì´ ë†’ìœ¼ë©´ (ê°€ë¡œë¡œ ê¸´ ë°•ìŠ¤) ë„˜ì–´ì§ ê°ì§€ ì‹œë„
                    box_ratio = (x2 - x1) / (y2 - y1) if (y2 - y1) > 0 else 0
                    should_check_fall = person_keypoints is not None or box_ratio >= 1.5
                    
                    if should_check_fall:
                        is_dangerous_detected, violation_type = _process_dangerous_behavior(
                            person_keypoints, (x1, y1, x2, y2), cam_id, person_box_key,
                            person_crop=person_crop,
                            fall_model=fall_model
                        )
                        if is_dangerous_detected:
                            logging.warning(f"âš ï¸ [CAM-{cam_id}] ë„˜ì–´ì§ ê°ì§€ë¨: box_ratio={box_ratio:.2f}, "
                                          f"í‚¤í¬ì¸íŠ¸={'ìˆìŒ' if person_keypoints else 'ì—†ìŒ'}")
                    
                    person_data_list[-1]['is_dangerous'] = is_dangerous_detected
                    person_data_list[-1]['violation_type'] = violation_type
                except Exception as e:
                    logging.debug(f"ìœ„í—˜ í–‰ë™ ê°ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    person_data_list[-1]['is_dangerous'] = False
                    person_data_list[-1]['violation_type'] = ""
                
                has_violation_or_danger = len(ppe_violations) > 0 or is_dangerous_detected
                
                # ì–¼êµ´ í’ˆì§ˆ ë° ì¸ì‹ ì¡°ê±´ ê³„ì‚° (ì™„í™”: ì¼ë¶€ë§Œ ë³´ì—¬ë„ ì¸ì‹ ì‹œë„)
                if person_keypoints is not None:
                    try:
                        kpts_conf = person_keypoints.conf[0].cpu().numpy() if person_keypoints.conf is not None else None
                        if kpts_conf is not None:
                            nose_visible = kpts_conf[0] > config.Thresholds.POSE_CONFIDENCE
                            left_eye_visible = kpts_conf[1] > config.Thresholds.POSE_CONFIDENCE
                            right_eye_visible = kpts_conf[2] > config.Thresholds.POSE_CONFIDENCE
                            # ì™„í™”: ì¼ë¶€ë§Œ ë³´ì—¬ë„ ì–¼êµ´ í’ˆì§ˆ OKë¡œ ê°„ì£¼ (ì¸¡ë©´ ì–¼êµ´ ì§€ì›)
                            face_quality_ok = nose_visible or left_eye_visible or right_eye_visible
                    except Exception:
                        pass
                
                # ê¸°íšŒí˜• ì¸ì‹ ì¡°ê±´ ì™„í™” (ë” ë§ì€ ì‚¬ëŒ ì¸ì‹ ì‹œë„)
                opportunistic_recognition = not has_violation_or_danger and (
                    face_quality_ok or 
                    height_ratio >= 0.06 or  # 0.10 -> 0.06 (ë” ë©€ë¦¬ì„œë„ ì¸ì‹)
                    person_area >= 800  # ì‘ì€ ì‚¬ëŒë„ ì¸ì‹ ì‹œë„
                )
                # ìœ„ë°˜ì´ ìˆìœ¼ë©´ í•­ìƒ ì¦‰ì‹œ ì¸ì‹ ì‹œë„ (ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨í•´ë„ ìœ„ë°˜ì€ ê°ì§€ë˜ì–´ì•¼ í•¨)
                immediate_recognition = has_violation_or_danger
                
                # --- ì–¼êµ´ ì¸ì‹ ì‹¤í–‰ ì—¬ë¶€ ê²°ì • ---
                priority_score = 0.0  # ê¸°ë³¸ê°’ ì´ˆê¸°í™”
                allow_face_job = False

                # 1. ê¸°ë³¸ ì¡°ê±´ í™•ì¸ (ìœ„ë°˜ì´ ìˆìœ¼ë©´ ì¡°ê±´ ì™„í™”, ì—†ìœ¼ë©´ ë” ì—„ê²©í•˜ê²Œ)
                # ìœ„ë°˜ì´ ìˆìœ¼ë©´ ì–¼êµ´ ì¸ì‹ ì¡°ê±´ì„ ì™„í™”í•˜ì—¬ ìµœëŒ€í•œ ì¸ì‹ ì‹œë„
                # ìœ„ë°˜ì´ ì—†ìœ¼ë©´ ë” ì—„ê²©í•œ ì¡°ê±´ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ì¸ì‹ ì‹œë„ ê°ì†Œ
                if has_violation_or_danger:
                    # ìœ„ë°˜ì´ ìˆìœ¼ë©´ ì¡°ê±´ ì™„í™” (ë” ì‘ì€ ì–¼êµ´ë„ ì¸ì‹ ì‹œë„)
                    min_area_for_recognition = max(400, config.Thresholds.MIN_FACE_RECOGNITION_AREA // 2)  # ìµœì†Œ 400, ë˜ëŠ” ê¸°ì¡´ê°’ì˜ ì ˆë°˜
                    min_height_ratio_for_recognition = max(0.04, float(config.Thresholds.MIN_PERSON_HEIGHT_RATIO_FOR_FACE) * 0.75)  # ìµœì†Œ 0.04, ë˜ëŠ” ê¸°ì¡´ê°’ì˜ 75%
                else:
                    # ìœ„ë°˜ì´ ì—†ìœ¼ë©´ ë” ì—„ê²©í•œ ì¡°ê±´ (ë¶ˆí•„ìš”í•œ ì¸ì‹ ì‹œë„ ê°ì†Œ)
                    min_area_for_recognition = 2500  # 1200 -> 2500 (ì•½ 2ë°° ì¦ê°€)
                    min_height_ratio_for_recognition = 0.15  # 0.06 -> 0.15 (ì•½ 2.5ë°° ì¦ê°€)
                
                base_conditions_met = (
                    face_analyzer is not None and  # buffalo_l ì‚¬ìš©
                    face_database is not None and
                    person_area >= min_area_for_recognition and
                    height_ratio >= min_height_ratio_for_recognition
                )
                
                # 2. ìºì‹œ í™•ì¸ (tracker_idê°€ ìˆì„ ë•Œë§Œ)
                # ìœ„ë°˜ì´ ìˆìœ¼ë©´ ìºì‹œ ìŠ¤í‚µí•˜ì§€ ì•ŠìŒ (ì¬ì¸ì‹ ì‹œë„)
                # ì¸ì‹ë¥  í–¥ìƒ: ìºì‹œ ì‚¬ìš© ì‹œì—ë„ ìœ ì‚¬ë„ ì ìˆ˜ ê²€ì¦
                if tracker_id is not None and recent_identity_cache is not None:
                    cache = recent_identity_cache[cam_id]
                    if tracker_id in cache:
                        cached_entry = cache[tracker_id]
                        cached_name = cached_entry.get('name', 'Unknown')
                        cached_score = cached_entry.get('score', 0.0)
                        cache_age = time.time() - cached_entry.get('ts', 0)
                        
                        # ìºì‹œ ì‚¬ìš© ì¡°ê±´: ì´ë¦„ì´ ìˆê³ , ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ ì´ìƒì´ë©°, ì¿¨ë‹¤ìš´ ì‹œê°„ ë‚´
                        if cached_name != "Unknown" and \
                           cached_score >= config.Thresholds.SIMILARITY and \
                           cache_age < config.Thresholds.FACE_RECOGNITION_COOLDOWN_SECONDS:
                            # ìœ„ë°˜ì´ ì—†ì„ ë•Œë§Œ ìºì‹œ ìŠ¤í‚µ (ìœ„ë°˜ì´ ìˆìœ¼ë©´ ì¬ì¸ì‹ ì‹œë„)
                            if not has_violation_or_danger:
                                cache_skip_recognition = True
                                person_data_list[-1]['name'] = cached_name
                                person_data_list[-1]['similarity'] = cached_score
                                logging.debug(f"âœ… ìºì‹œ ì‚¬ìš©: tracker_id={tracker_id}, name={cached_name}, score={cached_score:.3f}")
                            else:
                                logging.debug(f"ğŸ”„ ìœ„ë°˜ ê°ì§€ë¡œ ìºì‹œ ìŠ¤í‚µ: ì¬ì¸ì‹ ì‹œë„ (tracker_id={tracker_id})")

                # 3. ìµœì¢… ì‹¤í–‰ ì—¬ë¶€ ê²°ì •
                # ìœ„ë°˜ì´ ìˆìœ¼ë©´ í•­ìƒ ì¸ì‹ ì‹œë„ (ì¡°ê±´ ì™„í™”ë¨)
                if base_conditions_met and not cache_skip_recognition and (immediate_recognition or opportunistic_recognition):
                    allow_face_job = True
                    # ì–¼êµ´ ì¸ì‹ ìš°ì„ ìˆœìœ„ ê³„ì‚°
                    face_size_score = 0.0
                    if matched_face and matched_face.bbox:
                        fx1, fy1, fx2, fy2 = matched_face.bbox
                        face_area = (fx2 - fx1) * (fy2 - fy1)
                        face_size_score = min(1.0, face_area / (orig_w * orig_h * 0.1))
                    
                    front_face_score = 0.0
                    if matched_face and hasattr(matched_face, 'kps') and matched_face.kps is not None:
                        front_face_score = 1.0
                    elif face_quality_ok:
                        front_face_score = 0.7
                    
                    urgency_score = 1.0 if immediate_recognition else 0.3
                    
                    priority_score = (face_size_score * 0.4 + front_face_score * 0.3 + urgency_score * 0.3)
                
                if allow_face_job:
                    face_recognition_candidates.append((
                        priority_score,
                        len(person_data_list) - 1,
                        person_img_for_detection,
                        person_id_text,
                        tracker_id,
                        matched_face,
                        immediate_recognition,
                        face_quality_ok
                    ))
                    faces_scheduled += 1
                else:
                    # ì–¼êµ´ ì¸ì‹ ìŠ¤í‚µ ì´ìœ  ë¡œê¹…
                    if has_violation_or_danger or face_quality_ok or height_ratio >= 0.06 or person_area >= 800:
                        skip_reasons = []
                        if tracker_id is None: skip_reasons.append("ì¶”ì  ID ì—†ìŒ")
                        if face_analyzer is None: skip_reasons.append("buffalo_l ëª¨ë¸ ì—†ìŒ")
                        if face_database is None: skip_reasons.append("FAISS DB ì—†ìŒ")
                        if not (immediate_recognition or opportunistic_recognition): skip_reasons.append("ì¸ì‹ ì¡°ê±´ ë¶ˆë§Œì¡±")
                        if not (person_area >= config.Thresholds.MIN_FACE_RECOGNITION_AREA): skip_reasons.append(f"ì˜ì—­ ë¶€ì¡± (area={person_area}, ìµœì†Œ={config.Thresholds.MIN_FACE_RECOGNITION_AREA})")
                        if not (height_ratio >= float(config.Thresholds.MIN_PERSON_HEIGHT_RATIO_FOR_FACE)): skip_reasons.append(f"ê±°ë¦¬ ì œí•œ (í‚¤ ë¹„ìœ¨ {height_ratio:.2f}, ìµœì†Œ={config.Thresholds.MIN_PERSON_HEIGHT_RATIO_FOR_FACE})")
                        if cache_skip_recognition: skip_reasons.append("ìºì‹œì—ì„œ ì´ë¦„ ë°œê²¬ (ì¬ì¸ì‹ ìŠ¤í‚µ)")
                        
                        logging.debug(f"âš ï¸ ì–¼êµ´ ì¸ì‹ ìŠ¤í‚µ: person_id={person_id_text}, tracker_id={tracker_id}, ì´ìœ ={', '.join(skip_reasons)}")
                
                # ì–¼êµ´ íƒì§€ í”„ë ˆì„ ì¹´ìš´í„° ì—…ë°ì´íŠ¸ (ì–¼êµ´ ì¸ì‹ ì‹¤í–‰ ì‹œ)
                with face_detection_lock:
                    current_frame = frame_stats.get(cam_id, {}).get('frame_count', 0)
                    last_face_detection_frame[cam_id] = current_frame

            # ì–¼êµ´ ì¸ì‹ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì‘ì—… ì œì¶œ (í”„ë ˆì„ ë“œë ë°©ì§€: ë™ì  ì œí•œ)
            if len(face_recognition_candidates) > 0:
                # ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ ìš°ì„ )
                face_recognition_candidates.sort(key=lambda x: x[0], reverse=True)
                
                # ì‚°ì—… í˜„ì¥ ëŒ€ì‘: ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
                # 1. ê¸°ë³¸ ì œí•œ: configì—ì„œ ì„¤ì •ëœ ìµœëŒ€ ì‘ì—… ìˆ˜ (ì‚°ì—… í˜„ì¥ ëŒ€ì‘: ê¸°ë³¸ê°’ ì¦ê°€)
                max_jobs_base = config.Thresholds.MAX_FACE_RECOGNITION_JOBS_PER_FRAME
                
                # 2. ì›Œì»¤ ìˆ˜ ê¸°ë°˜ ë™ì  ì¡°ì •: ì›Œì»¤ê°€ ë§ì•„ë„ ì‘ì—… ìˆ˜ ì œí•œ (ìš°ì„ ìˆœìœ„ ë†’ì€ ì‘ì—…ë§Œ)
                num_workers_available = face_recognition_executor._max_workers
                # ì›Œì»¤ ìˆ˜ì˜ 0.5ë°°ê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥ (ì‘ì—… ìˆ˜ ì œí•œ ê°•í™”)
                max_jobs_by_workers = max(max_jobs_base, int(num_workers_available * 0.5))
                
                # 3. FPS ê¸°ë°˜ ë™ì  ì¡°ì •: 30 FPS ëª©í‘œë¡œ ì‘ì—… ìˆ˜ ì œí•œ (í”„ë ˆì„ ë“œë ë°©ì§€)
                max_jobs_dynamic = max_jobs_by_workers
                try:
                    with frame_stats_lock:
                        cam_stats = frame_stats.get(cam_id, {})
                        recent_frames = cam_stats.get('recent_frame_times', [])
                        if len(recent_frames) >= 2:
                            time_span = recent_frames[-1] - recent_frames[0]
                            if time_span > 0:
                                current_fps = (len(recent_frames) - 1) / time_span
                                # 30 FPS ëª©í‘œ: FPSê°€ ë‚®ìœ¼ë©´ ì‘ì—… ìˆ˜ ê°ì†Œ
                                if current_fps < 20:
                                    max_jobs_dynamic = max(1, int(max_jobs_by_workers * 0.5))  # 50% ê°ì†Œ
                                elif current_fps < 25:
                                    max_jobs_dynamic = max(1, int(max_jobs_by_workers * 0.7))  # 30% ê°ì†Œ
                                elif current_fps >= 30:
                                    max_jobs_dynamic = max_jobs_by_workers  # ìµœëŒ€ ì‘ì—… ìˆ˜ ìœ ì§€
                            else:
                                current_fps = (len(recent_frames) - 1) / time_span
                                # FPSê°€ ë‚®ì„ ë•Œë§Œ ì œí•œ (ì‚°ì—… í˜„ì¥ ëŒ€ì‘: ë” ê´€ëŒ€í•œ ê¸°ì¤€)
                                if current_fps < 10:
                                    max_jobs_dynamic = max(3, max_jobs_by_workers // 2)  # ìµœì†Œ 3ê°œ
                                elif current_fps < 15:
                                    max_jobs_dynamic = max(5, int(max_jobs_by_workers * 0.7))
                                elif current_fps < 20:
                                    max_jobs_dynamic = max(7, int(max_jobs_by_workers * 0.85))
                                # FPS >= 20ì´ë©´ ì›Œì»¤ ê¸°ë°˜ ì œí•œ ì‚¬ìš© (ì œí•œ ì—†ìŒ)
                except Exception:
                    pass  # FPS ê³„ì‚° ì‹¤íŒ¨ ì‹œ ì›Œì»¤ ê¸°ë°˜ ì œí•œ ì‚¬ìš©
                
                # 4. í›„ë³´ ìˆ˜ê°€ ë§¤ìš° ë§ì„ ë•Œë§Œ ì œí•œ (ì‚°ì—… í˜„ì¥ ëŒ€ì‘: ë” ë§ì€ ì‚¬ëŒ ì²˜ë¦¬)
                if len(face_recognition_candidates) > max_jobs_dynamic * 3:
                    # í›„ë³´ê°€ ì›Œì»¤ ê¸°ë°˜ ì œí•œì˜ 3ë°° ì´ìƒì´ë©´ ì•½ê°„ ê°ì†Œ
                    max_jobs_dynamic = max(max_jobs_base, max_jobs_dynamic - 2)
                
                # 5. ìµœëŒ€ ì¸ì› ì œí•œ: FPS ê¸°ë°˜ ë™ì  ì¡°ì • (í”„ë ˆì„ ë“œë ë°©ì§€)
                # FPSê°€ ë†’ìœ¼ë©´ ë” ë§ì€ ì‚¬ëŒ ì²˜ë¦¬ ê°€ëŠ¥, ë‚®ìœ¼ë©´ ì œí•œ
                try:
                    with frame_stats_lock:
                        cam_stats = frame_stats.get(cam_id, {})
                        recent_frames = cam_stats.get('recent_frame_times', [])
                        if len(recent_frames) >= 2:
                            time_span = recent_frames[-1] - recent_frames[0]
                            if time_span > 0:
                                current_fps = (len(recent_frames) - 1) / time_span
                                # FPS ê¸°ë°˜ ìµœëŒ€ ì¸ì› ì œí•œ
                                if current_fps >= 30:
                                    MAX_PEOPLE_LIMIT = 8  # ë†’ì€ FPS: ìµœëŒ€ 8ëª…
                                elif current_fps >= 20:
                                    MAX_PEOPLE_LIMIT = 6  # ì¤‘ê°„ FPS: ìµœëŒ€ 6ëª…
                                elif current_fps >= 15:
                                    MAX_PEOPLE_LIMIT = 5  # ë‚®ì€ FPS: ìµœëŒ€ 5ëª…
                                else:
                                    MAX_PEOPLE_LIMIT = 3  # ë§¤ìš° ë‚®ì€ FPS: ìµœëŒ€ 3ëª…
                            else:
                                MAX_PEOPLE_LIMIT = 5  # ê¸°ë³¸ê°’
                        else:
                            MAX_PEOPLE_LIMIT = 5  # ê¸°ë³¸ê°’
                except Exception:
                    MAX_PEOPLE_LIMIT = 5  # ì˜ˆì™¸ ì‹œ ê¸°ë³¸ê°’
                
                max_jobs_dynamic = min(max_jobs_dynamic, MAX_PEOPLE_LIMIT)
                
                # ìµœì¢… ì œí•œ ì ìš©
                limited_candidates = face_recognition_candidates[:max_jobs_dynamic]
                
                if len(face_recognition_candidates) > max_jobs_dynamic:
                    skipped_count = len(face_recognition_candidates) - max_jobs_dynamic
                    logging.info(f"[CAM-{cam_id}] ì–¼êµ´ ì¸ì‹ ì‘ì—… ì œí•œ: {len(face_recognition_candidates)}ê°œ í›„ë³´ ì¤‘ {max_jobs_dynamic}ê°œë§Œ ì²˜ë¦¬ (ìµœëŒ€ 5ëª… ì œí•œ, í”„ë ˆì„ ë“œë ë°©ì§€), {skipped_count}ê°œ ìŠ¤í‚µ (ìš°ì„ ìˆœìœ„ ë‚®ì€ ì‘ì—…)")
                
                # FastIndustrialRecognizer ê°€ì ¸ì˜¤ê¸° (ëœë“œë§ˆí¬ ê¸°ë°˜ ê³ ì† ì²˜ë¦¬ìš©)
                fast_recognizer = getattr(safety_system, 'fast_recognizer', None)
                use_adaface = getattr(safety_system, 'use_adaface', False)
                adaface_model_path = getattr(safety_system, 'adaface_model_path', None)
                face_uses_trt = getattr(safety_system, 'face_uses_trt', False)
                
                logging.debug(f"ì–¼êµ´ ì¸ì‹ ìš°ì„ ìˆœìœ„ ì •ë ¬: {len(limited_candidates)}ê°œ ì‘ì—… ì œì¶œ, ìµœê³  ìš°ì„ ìˆœìœ„={limited_candidates[0][0]:.3f}")
                
                # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”: Fast Path ì–¼êµ´ë“¤ì„ ëª¨ì•„ì„œ ë°°ì¹˜ ì²˜ë¦¬
                fast_path_candidates = []  # (person_idx, tracker_id, matched_face, original_frame)
                fallback_candidates = []   # (priority_score, person_idx, person_img, person_id, tracker_id, matched_face, immediate, face_quality)
                
                for priority_score, person_idx, person_img, person_id, tracker_id, matched_face, immediate, face_quality in limited_candidates:
                    # Fast Path: ë¯¸ë¦¬ ê°ì§€ëœ ì–¼êµ´ì´ ìˆê³  í‚¤í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ë°°ì¹˜ ì²˜ë¦¬ ëŒ€ìƒ
                    has_fast_path = matched_face is not None and hasattr(matched_face, 'kps') and matched_face.kps is not None
                    if has_fast_path and fast_recognizer is not None:
                        fast_path_candidates.append((person_idx, tracker_id, matched_face, resized_frame))
                    else:
                        fallback_candidates.append((priority_score, person_idx, person_img, person_id, tracker_id, matched_face, immediate, face_quality))
                
                # Fast Path ë°°ì¹˜ ì²˜ë¦¬ (GPU í™œìš©ë¥  í–¥ìƒ)
                if len(fast_path_candidates) > 0 and fast_recognizer is not None:
                    try:
                        # ë°°ì¹˜ ì²˜ë¦¬: ì—¬ëŸ¬ ì–¼êµ´ì„ í•œ ë²ˆì— ì²˜ë¦¬
                        batch_frames = []
                        batch_kps = []
                        batch_indices = []  # (person_idx, tracker_id, matched_face)
                        
                        for person_idx, tracker_id, matched_face, original_frame in fast_path_candidates:
                            if matched_face.kps is not None:
                                batch_frames.append(original_frame)
                                batch_kps.append(matched_face.kps)
                                batch_indices.append((person_idx, tracker_id, matched_face))
                        
                        if len(batch_frames) > 0:
                            # ë°°ì¹˜ ì„ë² ë”© ì¶”ì¶œ (ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•ê³¼ ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©)
                            batch_results = fast_recognizer.get_embeddings_batch(
                                batch_frames, 
                                batch_kps,
                                use_enhanced_preprocessing=False,  # aivis-project1 ë°©ì‹: ê¸°ë³¸ ì „ì²˜ë¦¬ë§Œ ì‚¬ìš© (CLAHE)
                                use_tta=False  # ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì‹œì™€ ë™ì¼ (USE_TTA_FOR_DATABASE=False)
                            )
                            
                            # ê²°ê³¼ ì²˜ë¦¬
                            for (person_idx, tracker_id, matched_face), (embedding, aligned_face) in zip(batch_indices, batch_results):
                                if embedding is not None:
                                    face_bbox = tuple(map(int, matched_face.bbox)) if hasattr(matched_face, 'bbox') else None
                                    # ë°°ì¹˜ FAISS ê²€ìƒ‰ì„ ìœ„í•´ ì„ë² ë”© ì €ì¥
                                    embeddings_for_batch.append((person_idx, tracker_id, embedding, face_bbox))
                                    logging.debug(f"[CAM-{cam_id}] âœ… Fast Path ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: person_idx={person_idx}, tracker_id={tracker_id}")
                                else:
                                    # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ Fallbackìœ¼ë¡œ ì „í™˜
                                    fallback_candidates.append((0.5, person_idx, None, f"P{person_idx}", tracker_id, matched_face, True, True))
                    except Exception as e:
                        logging.warning(f"âš ï¸ Fast Path ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨, ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±: {e}")
                        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ëª¨ë“  Fast Path í›„ë³´ë¥¼ Fallbackìœ¼ë¡œ ì „í™˜
                        for person_idx, tracker_id, matched_face, _ in fast_path_candidates:
                            fallback_candidates.append((0.5, person_idx, None, f"P{person_idx}", tracker_id, matched_face, True, True))
                
                # Fallback: ê°œë³„ ì‘ì—… ì œì¶œ (YOLOë¡œ ë‹¤ì‹œ ê°ì§€í•˜ëŠ” ê²½ìš°)
                for priority_score, person_idx, person_img, person_id, tracker_id, matched_face, immediate, face_quality in fallback_candidates:
                    recognition_type = "ì¦‰ì‹œ" if immediate else "ê¸°íšŒí˜•"
                    logging.debug(f"[CAM-{cam_id}] ì–¼êµ´ ì¸ì‹ ì‘ì—… ì œì¶œ (ìš°ì„ ìˆœìœ„={priority_score:.3f}, {recognition_type}): person_idx={person_idx}, person_id={person_id}, ì–¼êµ´í’ˆì§ˆ={face_quality}, FastPath=False")
                    
                    future = face_recognition_executor.submit(
                        _process_face_recognition,
                        person_img.copy() if person_img is not None else None,  # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•´ ë³µì‚¬ í•„ìš”
                        person_id,
                        face_model,
                        face_database,
                        fast_recognizer,  # AdaFaceìš© (ì‹¤ì œ ì‚¬ìš©)
                        matched_face,  # ë¯¸ë¦¬ ê°ì§€ëœ ì–¼êµ´ (Fast Path)
                        resized_frame  # ì›ë³¸(ë¦¬ì‚¬ì´ì¦ˆëœ) í”„ë ˆì„
                    )
                    
                    face_recognition_tasks.append(future)
                    # tracker_idë¥¼ í•¨ê»˜ ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— ê²°ê³¼ë¥¼ ë§¤í•‘
                    futures_with_index.append((person_idx, tracker_id, future))
                
                # ì¿¨ë‹¤ìš´ íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ì‘ì—… ì™„ë£Œ í›„ ì—…ë°ì´íŠ¸ (ì•„ë˜ì—ì„œ ì²˜ë¦¬)

            # ë³‘ë ¬ë¡œ ì–¼êµ´ ì¸ì‹ ê²°ê³¼ ìˆ˜ì§‘ (ì¸ì›ìˆ˜ ì œí•œ í•´ì œ)
            # GPU ì‚¬ìš© ì‹œ ëª¨ë“  ì‚¬ëŒ ì²˜ë¦¬ ê°€ëŠ¥
            # ì œí•œ ì—†ì´ ëª¨ë“  ì–¼êµ´ ì¸ì‹ ì‘ì—… ì²˜ë¦¬
            
            face_recognition_results = {}
            # ë°°ì¹˜ FAISS ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ìˆ˜ì§‘
            embeddings_for_batch = []  # (person_idx, tracker_id, embedding, face_bbox)
            
            # ë¹„ë™ê¸° ì²˜ë¦¬: í”„ë ˆì„ ë“œë ë°©ì§€ë¥¼ ìœ„í•œ íƒ€ì„ì•„ì›ƒ ìµœì í™”
            # GPU ì‚¬ìš© ì‹œ ë” ê¸´ íƒ€ì„ì•„ì›ƒ í—ˆìš© (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë” ë§ì€ ì‘ì—… ì™„ë£Œ ê°€ëŠ¥)
            num_workers = face_recognition_executor._max_workers
            
            # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ (ì–¼êµ´ ì¸ì‹ ëª¨ë¸ì´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€)
            is_gpu_available = False
            try:
                if torch.cuda.is_available():
                    # ì–¼êµ´ ì¸ì‹ ëª¨ë¸ì´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸
                    if hasattr(safety_system, 'device_face') and safety_system.device_face.type == 'cuda':
                        is_gpu_available = True
                    elif hasattr(safety_system, 'fast_recognizer') and hasattr(safety_system.fast_recognizer, 'session'):
                        # ONNX Runtime ì„¸ì…˜ì˜ Provider í™•ì¸
                        session = safety_system.fast_recognizer.session
                        if session and 'CUDAExecutionProvider' in session.get_providers():
                            is_gpu_available = True
            except:
                pass
            
            # 30 FPS ëª©í‘œ: ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤í•˜ì—¬ íƒ€ì„ì•„ì›ƒ ì¦ê°€ (íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ ì‘ì—… ì‹¤íŒ¨ ë°©ì§€)
            # ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„: YOLO Face ê°ì§€ + ì„ë² ë”© ì¶”ì¶œ = ì•½ 50-100ms (ë¡œê·¸ í™•ì¸: 1046msê¹Œì§€ ì†Œìš”)
            # íƒ€ì„ì•„ì›ƒì„ 150-200msë¡œ ì¦ê°€í•˜ì—¬ ì‘ì—… ì™„ë£Œìœ¨ í–¥ìƒ
            if is_gpu_available:
                base_timeout = 0.15  # GPU ì‚¬ìš© ì‹œ ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ 150ms (50ms -> 150ms, ì‘ì—… ì™„ë£Œìœ¨ í–¥ìƒ)
                max_timeout = 0.20   # GPU ì‚¬ìš© ì‹œ ìµœëŒ€ íƒ€ì„ì•„ì›ƒ 200ms (80ms -> 200ms)
            else:
                base_timeout = 0.15  # CPU ì‚¬ìš© ì‹œ ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ 150ms (50ms -> 150ms, ì‘ì—… ì™„ë£Œìœ¨ í–¥ìƒ)
                max_timeout = 0.20   # CPU ì‚¬ìš© ì‹œ ìµœëŒ€ íƒ€ì„ì•„ì›ƒ 200ms (80ms -> 200ms)
            
            # ì‘ì—… ìˆ˜ ê¸°ë°˜ íƒ€ì„ì•„ì›ƒ ì¡°ì • (30 FPS ëª©í‘œ, ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤)
            if len(futures_with_index) > 0:
                if is_gpu_available:
                    # GPU ì‚¬ìš© ì‹œ: ì‘ì—… ìˆ˜ì— ë”°ë¼ íƒ€ì„ì•„ì›ƒ ì¡°ì •
                    if len(futures_with_index) <= 2:
                        timeout_seconds = 0.15  # 150ms (50ms -> 150ms, ì‘ì—… ì™„ë£Œìœ¨ í–¥ìƒ)
                    elif len(futures_with_index) <= 4:
                        timeout_seconds = 0.18  # 180ms (50ms -> 180ms, ì‘ì—… ì™„ë£Œìœ¨ í–¥ìƒ)
                    else:
                        timeout_seconds = 0.20  # 200ms (50ms -> 200ms, ì‘ì—… ì™„ë£Œìœ¨ í–¥ìƒ)
                else:
                    # CPU ì‚¬ìš© ì‹œ: ì‘ì—… ìˆ˜ì— ë”°ë¼ íƒ€ì„ì•„ì›ƒ ì¡°ì •
                    if len(futures_with_index) <= 2:
                        timeout_seconds = 0.15  # 150ms (50ms -> 150ms, ì‘ì—… ì™„ë£Œìœ¨ í–¥ìƒ)
                    elif len(futures_with_index) <= 4:
                        timeout_seconds = 0.18  # 180ms (50ms -> 180ms, ì‘ì—… ì™„ë£Œìœ¨ í–¥ìƒ)
                    else:
                        timeout_seconds = 0.20  # 200ms (50ms -> 200ms, ì‘ì—… ì™„ë£Œìœ¨ í–¥ìƒ)
            else:
                timeout_seconds = base_timeout
            
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ íƒ€ì„ì•„ì›ƒ ì¡°ì • (ë†’ì€ ìš°ì„ ìˆœìœ„ ì‘ì—…ì´ ë§ìœ¼ë©´ ì•½ê°„ ì¦ê°€)
            if len(face_recognition_candidates) > 0:
                # ì œì¶œëœ ì‘ì—…ë§Œ ê³ ë ¤ (limited_candidatesëŠ” ìŠ¤ì½”í”„ ë°–ì´ë¯€ë¡œ futures_with_index ê¸°ì¤€)
                submitted_candidates = [c for c in face_recognition_candidates if any(c[1] == idx for idx, _, _ in futures_with_index)]
                if len(submitted_candidates) > 0:
                    avg_priority = sum(c[0] for c in submitted_candidates) / len(submitted_candidates)
                    # í‰ê·  ìš°ì„ ìˆœìœ„ê°€ ë§¤ìš° ë†’ìœ¼ë©´(>0.8) íƒ€ì„ì•„ì›ƒ ì•½ê°„ ì¦ê°€ (ìµœëŒ€ 200ms)
                    if avg_priority > 0.8 and len(futures_with_index) <= 3:
                        timeout_seconds = min(max_timeout, timeout_seconds + 0.05)
                else:
                    avg_priority = 0.0
            else:
                avg_priority = 0.0
            
            logging.debug(f"ì–¼êµ´ ì¸ì‹ ë¹„ë™ê¸° ì²˜ë¦¬: íƒ€ì„ì•„ì›ƒ={timeout_seconds:.3f}s (í”„ë ˆì„ ë“œë ë°©ì§€ ìµœì í™”), ì‘ì—… ìˆ˜={len(futures_with_index)}, ì›Œì»¤ ìˆ˜={num_workers}, í‰ê·  ìš°ì„ ìˆœìœ„={avg_priority:.3f}")
            
            try:
                
                # íƒ€ì„ì•„ì›ƒ ë‚´ ì™„ë£Œëœ ì‘ì—…ë§Œ ìˆ˜ì§‘ (í”„ë ˆì„ ë“œë ë°©ì§€)
                # ë°°ì¹˜ FAISS ê²€ìƒ‰ì„ ìœ„í•´ ëª¨ë“  ì„ë² ë”©ì„ ëª¨ì•„ì„œ ì²˜ë¦¬
                completed_count = 0
                completed_futures = set()
                fallback_embeddings = []  # Fallback ê°œë³„ ì²˜ë¦¬ ê²°ê³¼ ì„ë² ë”© ìˆ˜ì§‘
                try:
                    for future in as_completed([f for _, _, f in futures_with_index], timeout=timeout_seconds):
                        completed_futures.add(future)
                        try:
                            # ì™„ë£Œëœ ì‘ì—… ê²°ê³¼ ìˆ˜ì§‘ (íƒ€ì„ì•„ì›ƒ ì—†ìŒ: ì´ë¯¸ ì™„ë£Œë¨)
                            _, _, embedding, face_bbox = future.result(timeout=0.1)
                            
                            # ë§¤í•‘ëœ ì¸ë±ìŠ¤ ì°¾ê¸°
                            mapped_idx, tracker_id = next(((idx, tid) for idx, tid, f in futures_with_index if f is future), (None, None))
                            if mapped_idx is not None and embedding is not None:
                                # Fast Path ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ì™€ Fallback ê°œë³„ ì²˜ë¦¬ ê²°ê³¼ ëª¨ë‘ ìˆ˜ì§‘
                                embeddings_for_batch.append((mapped_idx, tracker_id, embedding, face_bbox))
                                fallback_embeddings.append((mapped_idx, tracker_id, embedding, face_bbox))
                            else:
                                # ì„ë² ë”©ì´ ì—†ìœ¼ë©´ Unknownìœ¼ë¡œ ì„¤ì •
                                person_data_list[mapped_idx]['name'] = "Unknown"
                                person_data_list[mapped_idx]['similarity'] = 0.0
                                person_data_list[mapped_idx]['embedding'] = None
                                person_data_list[mapped_idx]['face_bbox'] = face_bbox
                            completed_count += 1
                        except FaceRecognitionError as e:
                            # FaceRecognitionErrorëŠ” ë¡œê¹…ë§Œ í•˜ê³  ê³„ì† ì§„í–‰ (Unknown ìœ ì§€)
                            # ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨í•´ë„ person_boxëŠ” ìˆìœ¼ë¯€ë¡œ face_bboxëŠ” Noneìœ¼ë¡œ ì„¤ì •
                            logging.debug(f"ì–¼êµ´ ì¸ì‹ ì˜¤ë¥˜ (ë¬´ì‹œ): {e.message} (error_code={e.error_code})")
                            # ë§¤í•‘ëœ ì¸ë±ìŠ¤ ì°¾ê¸°
                            mapped_idx, tracker_id = next(((idx, tid) for idx, tid, f in futures_with_index if f is future), (None, None))
                            if mapped_idx is not None and mapped_idx < len(person_data_list):
                                # ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨í•´ë„ person_boxëŠ” ìˆìœ¼ë¯€ë¡œ Unknownìœ¼ë¡œ ì„¤ì •
                                person_data_list[mapped_idx]['name'] = "Unknown"
                                person_data_list[mapped_idx]['similarity'] = 0.0
                                person_data_list[mapped_idx]['embedding'] = None
                                person_data_list[mapped_idx]['face_bbox'] = None  # ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨ ì‹œ None
                            completed_count += 1
                        except Exception as e:
                            logging.debug(f"ì–¼êµ´ ì¸ì‹ ì‘ì—… ì‹¤íŒ¨: {e}")
                            # ë§¤í•‘ëœ ì¸ë±ìŠ¤ ì°¾ê¸°
                            mapped_idx, tracker_id = next(((idx, tid) for idx, tid, f in futures_with_index if f is future), (None, None))
                            if mapped_idx is not None and mapped_idx < len(person_data_list):
                                # ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨í•´ë„ person_boxëŠ” ìˆìœ¼ë¯€ë¡œ Unknownìœ¼ë¡œ ì„¤ì •
                                person_data_list[mapped_idx]['name'] = "Unknown"
                                person_data_list[mapped_idx]['similarity'] = 0.0
                                person_data_list[mapped_idx]['embedding'] = None
                                person_data_list[mapped_idx]['face_bbox'] = None  # ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨ ì‹œ None
                            completed_count += 1
                except FuturesTimeoutError:
                    # íƒ€ì„ì•„ì›ƒ ë°œìƒ: ì™„ë£Œëœ ì‘ì—…ë§Œ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìºì‹œ í™œìš©
                    logging.debug(f"ì–¼êµ´ ì¸ì‹ íƒ€ì„ì•„ì›ƒ: {len(completed_futures)}/{len(futures_with_index)}ê°œ ì™„ë£Œ, ë‚˜ë¨¸ì§€ëŠ” ìºì‹œ ì‚¬ìš©")
                
                # ë¯¸ì™„ë£Œ ì‘ì—… ì²˜ë¦¬: ìºì‹œì—ì„œ ê²°ê³¼ ì°¾ê¸° ë˜ëŠ” Unknown ìœ ì§€
                for person_idx, tracker_id, future in futures_with_index:
                    if future not in completed_futures:
                        # ë¯¸ì™„ë£Œ ì‘ì—…: ìºì‹œì—ì„œ ê²°ê³¼ ì°¾ê¸° ì‹œë„
                        if person_idx < len(person_data_list):
                            person_data = person_data_list[person_idx]
                            # ì´ë¯¸ ìºì‹œì—ì„œ ì´ë¦„ì„ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
                            cached_name = person_data.get('name', 'Unknown')
                            if cached_name == "Unknown":
                                # ìºì‹œì—ì„œ ì¶”ê°€ë¡œ ì°¾ê¸° ì‹œë„ (tracker_id ê¸°ë°˜)
                                try:
                                    # tracker_idê°€ ìˆìœ¼ë©´ ì§ì ‘ ì ‘ê·¼
                                    if tracker_id is not None:
                                        cache = recent_identity_cache.get(cam_id)
                                        if cache is not None:
                                            cached_entry = cache.get(tracker_id)
                                            if cached_entry is not None:
                                                cached_name = cached_entry.get('name', 'Unknown')
                                                cached_score = cached_entry.get('score', 0.0)
                                                if cached_name != "Unknown" and cached_score >= config.Thresholds.SIMILARITY:
                                                    person_data_list[person_idx]['name'] = cached_name
                                                    person_data_list[person_idx]['similarity'] = cached_score
                                                    logging.debug(f"ë¯¸ì™„ë£Œ ì‘ì—… ìºì‹œ ì‚¬ìš© (tracker_id): person_idx={person_idx}, tracker_id={tracker_id}, ì´ë¦„={cached_name}")
                                except Exception as cache_error:
                                    logging.debug(f"ìºì‹œ ê²€ìƒ‰ ì‹¤íŒ¨: {cache_error}")
                            
                            # ìºì‹œì—ì„œë„ ì°¾ì§€ ëª»í•˜ë©´ Unknown ìœ ì§€ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì€ ê³„ì† ì‹¤í–‰)
                            if person_data_list[person_idx].get('name', 'Unknown') == "Unknown":
                                logging.debug(f"ì–¼êµ´ ì¸ì‹ ë¯¸ì™„ë£Œ: person_idx={person_idx}, ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† ì‹¤í–‰ ì¤‘ (í˜„ì¬ í”„ë ˆì„ì€ Unknown)")
                
                # ë°°ì¹˜ FAISS ê²€ìƒ‰ ìˆ˜í–‰ (ì„±ëŠ¥ ìµœì í™”)
                if len(embeddings_for_batch) > 0 and face_database is not None:
                    try:
                        embeddings_array = np.array([emb for _, _, emb, _ in embeddings_for_batch], dtype=np.float32)
                        logging.info(f"[CAM-{cam_id}] ğŸ” ë°°ì¹˜ FAISS ê²€ìƒ‰ ì‹œì‘: {len(embeddings_for_batch)}ê°œ ì„ë² ë”©, ì„ê³„ê°’={config.Thresholds.SIMILARITY}")
                        
                        batch_results = find_best_matches_faiss_batch(
                            embeddings_array,
                            face_database,
                            config.Thresholds.SIMILARITY
                        )
                        
                        for (mapped_idx, tracker_id, embedding, face_bbox), (person_name, similarity_score) in zip(embeddings_for_batch, batch_results):
                            if mapped_idx < len(person_data_list):
                                person_data_list[mapped_idx]['name'] = person_name
                                person_data_list[mapped_idx]['similarity'] = similarity_score
                                
                                # FAISS ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
                                if person_name != "Unknown":
                                    logging.info(f"[CAM-{cam_id}] âœ… FAISS ë§¤ì¹­ ì„±ê³µ: person_idx={mapped_idx}, ì´ë¦„={person_name}, ìœ ì‚¬ë„={similarity_score:.3f}, ì„ê³„ê°’={config.Thresholds.SIMILARITY}")
                                else:
                                    logging.warning(f"[CAM-{cam_id}] âš ï¸ FAISS ë§¤ì¹­ ì‹¤íŒ¨: person_idx={mapped_idx}, ìœ ì‚¬ë„={similarity_score:.3f} < ì„ê³„ê°’={config.Thresholds.SIMILARITY} (ì°¨ì´: {config.Thresholds.SIMILARITY - similarity_score:.3f})")
                                
                                # ìºì‹œ ì—…ë°ì´íŠ¸ (tracker_id ê¸°ì¤€)
                                if tracker_id is not None:
                                    recent_identity_cache[cam_id][tracker_id] = {
                                        'name': person_name,
                                        'score': similarity_score,
                                        'ts': time.time()
                                    }
                                    logging.debug(f"[CAM-{cam_id}] ìºì‹œ ì—…ë°ì´íŠ¸: tracker_id={tracker_id}, name={person_name}")
                    except Exception as e:
                        logging.error(f"âŒ ë°°ì¹˜ FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
                        
                        # ê²€ìƒ‰ ì‹¤íŒ¨í•´ë„ ì¿¨ë‹¤ìš´ ì—…ë°ì´íŠ¸ (ì¬ì‹œë„ ë°©ì§€)
                        if len(embeddings_for_batch) > 0:
                            face_recognition_cooldown_ts[cam_id] = time.time()
                        # ë°°ì¹˜ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê°œë³„ ê²€ìƒ‰ìœ¼ë¡œ í´ë°± (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                        for mapped_idx, tracker_id, embedding, face_bbox in embeddings_for_batch:
                            if mapped_idx < len(person_data_list):
                                try:
                                    person_name, similarity_score = utils.find_best_match_faiss(
                                        embedding, face_database, config.Thresholds.SIMILARITY
                                    )
                                    person_data_list[mapped_idx]['name'] = person_name
                                    person_data_list[mapped_idx]['similarity'] = similarity_score
                                    person_data_list[mapped_idx]['embedding'] = embedding
                                    person_data_list[mapped_idx]['face_bbox'] = face_bbox
                                except Exception as fallback_error:
                                    logging.error(f"âŒ ê°œë³„ FAISS ê²€ìƒ‰ í´ë°± ì‹¤íŒ¨: {fallback_error}")
                                    person_data_list[mapped_idx]['name'] = "Unknown"
                                    person_data_list[mapped_idx]['similarity'] = 0.0
                                    person_data_list[mapped_idx]['embedding'] = embedding
                                    person_data_list[mapped_idx]['face_bbox'] = face_bbox
                        
                        # ê°œë³„ ê²€ìƒ‰ í´ë°± ì™„ë£Œ í›„ì—ë„ ì¿¨ë‹¤ìš´ ì—…ë°ì´íŠ¸
                        if len(embeddings_for_batch) > 0:
                            face_recognition_cooldown_ts[cam_id] = time.time()
                            logging.debug(f"[CAM-{cam_id}] ì–¼êµ´ ì¸ì‹ ì¿¨ë‹¤ìš´ ì—…ë°ì´íŠ¸: ê°œë³„ ê²€ìƒ‰ í´ë°± ì™„ë£Œ ({len(embeddings_for_batch)}ê°œ)")
            except Exception as e:
                # ì˜ˆì™¸ ë°œìƒ ì‹œ ì²˜ë¦¬: ì™„ë£Œëœ ì‘ì—…ë§Œ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ìºì‹œ í™œìš©
                logging.debug(f"ì–¼êµ´ ì¸ì‹ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {e}")
                # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ì™„ë£Œëœ ì‘ì—…ì€ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
            
            # ì–¼êµ´ ì¸ì‹ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ (íƒ€ì„ì•„ì›ƒ ì‹œê°„ë§Œ ì¸¡ì •: í”„ë ˆì„ ë“œë ë°©ì§€)
            face_recognition_elapsed = (time.time() - face_recognition_start) * 1000  # ms
            # í”„ë ˆì„ ë“œë ë°©ì§€: íƒ€ì„ì•„ì›ƒ ì‹œê°„ë§Œ ì¸¡ì • (ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† ì‹¤í–‰)
            perf_timings['face_recognition'] = min(face_recognition_elapsed, timeout_seconds * 1000)
            
            # ì–¼êµ´ ì¸ì‹ ìƒì„¸ ë¡œê¹… (ë³‘ëª© ë¶„ì„ìš©)
            if face_recognition_elapsed > 1000:  # 1ì´ˆ ì´ìƒ ê±¸ë¦¬ë©´ ê²½ê³ 
                logging.warning(f"[PERF CAM-{cam_id}] âš ï¸ ì–¼êµ´ ì¸ì‹ ì‹œê°„ ì´ˆê³¼: {face_recognition_elapsed:.1f}ms (ì œì¶œ ì‘ì—… ìˆ˜: {len(futures_with_index)}, ì™„ë£Œ: {completed_count}, íƒ€ì„ì•„ì›ƒ: {timeout_seconds:.3f}s)")

            # ê²°ê³¼ë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
            # í”„ë ˆì„ ë‚´ ë™ì¼ ì´ë¦„ ì¤‘ë³µ ë°©ì§€: ì´ë¦„ë³„ë¡œ ë°•ìŠ¤ì™€ similarity ì €ì¥
            name_to_boxes: Dict[str, List[Tuple[Tuple[int,int,int,int], float, int]]] = {}  # (box, score, person_index)
            person_final_names: Dict[int, str] = {}  # person_index -> ìµœì¢… ì´ë¦„
            
            # 1ë‹¨ê³„: ëª¨ë“  person_dataë¥¼ ìˆœíšŒí•˜ì—¬ name_to_boxes ìˆ˜ì§‘ (ì›ë³¸ ì´ë¦„ ì‚¬ìš©)
            # ìµœì í™”: ìºì‹œ ê²€ìƒ‰ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì¤‘ì²© ë£¨í”„ ì œê±°
            hold_sec = config.Thresholds.RECOGNITION_HOLD_SECONDS
            up_th = config.Thresholds.SIMILARITY
            down_th = max(0.0, up_th - config.Thresholds.RECOGNITION_HYSTERESIS_DELTA)
            now_ts = time.time()
            
            # TTLCacheì—ì„œ ëª¨ë“  í•­ëª© ê°€ì ¸ì˜¤ê¸° (tracker_id ê¸°ë°˜)
            cache = recent_identity_cache.get(cam_id)
            cache_entries = []
            if cache is not None:
                # TTLCacheëŠ” ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ ì‚¬ìš© ê°€ëŠ¥, ëª¨ë“  í•­ëª© ìˆœíšŒ
                for tracker_id_key, entry in cache.items():
                    # ë§Œë£Œëœ í•­ëª©ì€ ìë™ìœ¼ë¡œ ì œê±°ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìœ íš¨í•œ í•­ëª©ë§Œ ì²˜ë¦¬
                    entry_ts = entry.get('ts', 0)
                    age = now_ts - entry_ts
                    if age <= hold_sec:
                        # tracker_idì™€ í•¨ê»˜ ì €ì¥
                        entry_with_tracker = entry.copy()
                        entry_with_tracker['tracker_id'] = tracker_id_key
                        cache_entries.append(entry_with_tracker)
            
            # ë°°ì¹˜ IoU ê³„ì‚°ì„ ìœ„í•œ ë°•ìŠ¤ ë°°ì—´ ì¤€ë¹„ (tracker_id ê¸°ë°˜ìœ¼ë¡œ ì§ì ‘ ë§¤ì¹­)
            if len(cache_entries) > 0 and len(person_data_list) > 0:
                # ìºì‹œ ë°•ìŠ¤ ë°°ì—´ ì¤€ë¹„
                cache_boxes = []
                valid_cache_indices = []
                for idx, entry in enumerate(cache_entries):
                    entry_box = entry.get('box', (0,0,0,0))
                    if len(entry_box) == 4:
                        cache_boxes.append(entry_box)
                        valid_cache_indices.append(idx)
                
                if len(cache_boxes) > 0:
                    cache_boxes_array = np.array(cache_boxes, dtype=np.float32)
                    
                    # person_data ë°•ìŠ¤ ë°°ì—´ ì¤€ë¹„
                    person_boxes = []
                    person_indices = []
                    for person_data in person_data_list:
                        person_boxes.append(person_data['box'])
                        person_indices.append(person_data['index'])
                    
                    person_boxes_array = np.array(person_boxes, dtype=np.float32)
                    
                    # ë°°ì¹˜ IoU ê³„ì‚°
                    iou_matrix = calculate_iou_batch(person_boxes_array, cache_boxes_array)
                    
                    # ê° person_dataì— ëŒ€í•´ ìµœì ì˜ ìºì‹œ í•­ëª© ì°¾ê¸°
                    matched_entries_map = {}  # person_index -> matched_entry
                    for p_idx, person_idx in enumerate(person_indices):
                        person_box = person_boxes[p_idx]
                        x1, y1, x2, y2 = person_box
                        current_box_center_x = (x1 + x2) / 2
                        current_box_center_y = (y1 + y2) / 2
                        current_box_diagonal = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5
                        max_distance = current_box_diagonal * 1.5
                        
                        best_iou = 0.0
                        best_cache_idx = None
                        
                        # IoU í–‰ë ¬ì—ì„œ ìµœì  í•­ëª© ì°¾ê¸° (ê±°ë¦¬ í•„í„°ë§ë„ ì ìš©)
                        for c_idx, cache_idx in enumerate(valid_cache_indices):
                            iou = float(iou_matrix[p_idx, c_idx])
                            if iou < 0.5:  # IoU ì„ê³„ê°’
                                continue
                            
                            entry_box = cache_boxes[c_idx]
                            entry_center_x = (entry_box[0] + entry_box[2]) / 2
                            entry_center_y = (entry_box[1] + entry_box[3]) / 2
                            center_distance = ((current_box_center_x - entry_center_x) ** 2 + 
                                             (current_box_center_y - entry_center_y) ** 2) ** 0.5
                            
                            if center_distance <= max_distance and iou > best_iou:
                                best_iou = iou
                                best_cache_idx = cache_idx
                        
                        if best_cache_idx is not None:
                            matched_entries_map[person_idx] = cache_entries[best_cache_idx]
            else:
                matched_entries_map = {}
            
            for person_data in person_data_list:
                i = person_data['index']
                person_id_text = person_data['person_id']
                x1, y1, x2, y2 = person_data['box']
                person_name = person_data.get('name', 'Unknown')  # ì›ë³¸ ì´ë¦„
                similarity_score = person_data.get('similarity', 0.0)

                # --- ì–¼êµ´ ì¸ì‹ ì•ˆì •í™”: íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ + í™€ë“œ ---
                matched_entry = matched_entries_map.get(i)  # ë°°ì¹˜ ê³„ì‚° ê²°ê³¼ ì‚¬ìš©
                try:

                    # ìƒìŠ¹/í•˜ê°• ì„ê³„ ì ìš©
                    if person_name != "Unknown" and similarity_score >= up_th:
                        # í™•ì • ë˜ëŠ” ê°±ì‹ (EMA)
                        if matched_entry is not None:
                            prev_score = float(matched_entry.get('score', similarity_score))
                            smoothed = 0.7 * similarity_score + 0.3 * prev_score
                            # ìºì‹œ í•­ëª© ê°±ì‹  (IdentityCacheì— ë‹¤ì‹œ ì¶”ê°€)
                            matched_entry['box'] = (x1, y1, x2, y2)
                            matched_entry['name'] = person_name
                            matched_entry['score'] = smoothed
                            matched_entry['ts'] = now_ts
                            recent_identity_cache.add(cam_id, matched_entry)
                        else:
                            # ìƒˆ í•­ëª© ì¶”ê°€ (IdentityCacheê°€ ìë™ìœ¼ë¡œ í¬ê¸° ì œí•œ)
                            recent_identity_cache.add(cam_id, {
                                'box': (x1, y1, x2, y2),
                                'name': person_name,
                                'score': similarity_score
                                # 'ts'ëŠ” IdentityCache.add()ì—ì„œ ìë™ ì¶”ê°€
                            })
                    else:
                        # Unknown ë˜ëŠ” ë‚®ì€ ì ìˆ˜: í™€ë“œ ì¡°ê±´ ì¶©ì¡± ì‹œ ì§ì „ ë¼ë²¨ ìœ ì§€
                        if matched_entry is not None:
                            age = now_ts - matched_entry.get('ts', 0)
                            last_score = float(matched_entry.get('score', 0.0))
                            if age <= hold_sec and last_score >= down_th:
                                person_name = matched_entry.get('name', person_name)
                                similarity_score = last_score
                                # ë°•ìŠ¤/ì‹œê°„ ê°±ì‹ 
                                matched_entry.update({'box': (x1, y1, x2, y2), 'ts': now_ts})
                except Exception as _stb_e:
                    # ì•ˆì •í™” ë¡œì§ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ì› ê²°ê³¼ ì‚¬ìš©
                    pass
                
                # now_tsê°€ try ë¸”ë¡ ì•ˆì—ì„œë§Œ ì •ì˜ë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¬ì •ì˜
                if 'now_ts' not in locals():
                    now_ts = time.time()

                # ì„¼íŠ¸ë¡œì´ë“œ ì„ë² ë”©: ì—¬ëŸ¬ í”„ë ˆì„ì˜ ì„ë² ë”©ì„ í‰ê· ë‚´ì–´ ì•ˆì •ì„± í–¥ìƒ (final ê°œì„  ê¸°ë²•)
                # matched_entryë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ê³„ì‚° ì œê±° (ì„±ëŠ¥ ìµœì í™”)
                embedding = person_data.get('embedding')
                
                if embedding is not None:
                    # person_box_key ìƒì„± (í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
                    person_box_key = _generate_person_box_key(cam_id, matched_entry, x1, y1, x2, y2)
                    
                    # ì„ë² ë”©ì„ ë²„í¼ì— ì¶”ê°€ (ì•ˆì „í•œ ì ‘ê·¼)
                    if cam_id not in embedding_buffers:
                        embedding_buffers[cam_id] = {}
                    if person_box_key not in embedding_buffers[cam_id]:
                        embedding_buffers[cam_id][person_box_key] = {'embeddings': [], 'last_update': 0.0}
                    buffer_data = embedding_buffers[cam_id][person_box_key]
                    # ì„ë² ë”© ë³µì‚¬ ìµœì í™”: ë²„í¼ì— ì¶”ê°€í•  ë•ŒëŠ” ë³µì‚¬ (ì•ˆì •ì„± ìš°ì„ )
                    # ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ì‹œì—ëŠ” ì´ë¯¸ ë³µì‚¬ëœ ë°°ì—´ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì¶”ê°€ ë³µì‚¬ ë¶ˆí•„ìš”
                    buffer_data['embeddings'].append(embedding.copy())
                    buffer_data['last_update'] = now_ts
                    
                    # ë²„í¼ í¬ê¸° ì œí•œ (ìµœëŒ€ EMBEDDING_BUFFER_SIZE)
                    if len(buffer_data['embeddings']) > EMBEDDING_BUFFER_SIZE:
                        buffer_data['embeddings'] = buffer_data['embeddings'][-EMBEDDING_BUFFER_SIZE:]
                    
                    # ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ìµœì í™”: ë²„í¼ í¬ê¸°ê°€ ì¶©ë¶„íˆ í´ ë•Œë§Œ ê³„ì‚° (5ê°œ ì´ìƒ)
                    # ìµœì í™”: ê³„ì‚° ì£¼ê¸°ë¥¼ ëŠ˜ë ¤ CPU ì‚¬ìš©ëŸ‰ ê°ì†Œ (5ê°œ ì´ìƒì¼ ë•Œë§Œ ê³„ì‚°)
                    if len(buffer_data['embeddings']) >= 5:
                        logging.debug(f"ğŸ” {person_id_text} ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ì‹œì‘: ë²„í¼ í¬ê¸°={len(buffer_data['embeddings'])}, person_box_key={person_box_key}")
                        # ìºì‹œ í™•ì¸ (ìµœê·¼ 2ì´ˆ ë‚´ ê²°ê³¼ ì¬ì‚¬ìš©)
                        cached_centroid = centroid_cache[cam_id].get(person_box_key)
                        if cached_centroid:
                            person_name_centroid = cached_centroid.get('name', 'Unknown')
                            similarity_score_centroid = cached_centroid.get('score', 0.0)
                            logging.debug(f"ğŸ” {person_id_text} ì„¼íŠ¸ë¡œì´ë“œ ìºì‹œ ì‚¬ìš©: {person_name_centroid} (ìœ ì‚¬ë„={similarity_score_centroid:.3f})")
                        else:
                            # ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° ìµœì í™”: numpy ë°°ì—´ì„ í•œ ë²ˆì— ì²˜ë¦¬
                            # ë²„í¼ì— ì´ë¯¸ ë³µì‚¬ëœ ë°°ì—´ì´ë¯€ë¡œ ì¶”ê°€ ë³µì‚¬ ë¶ˆí•„ìš” (ë©”ëª¨ë¦¬ ìµœì í™”)
                            embeddings_array = np.array(buffer_data['embeddings'], dtype=np.float32)
                            if len(embeddings_array) > 0:
                                avg_embedding = np.mean(embeddings_array, axis=0)
                                norm = np.linalg.norm(avg_embedding)
                                if norm > 1e-6:
                                    normalized_avg_embedding = (avg_embedding / norm).astype('float32')
                                    # ì„¼íŠ¸ë¡œì´ë“œ ì„ë² ë”©ìœ¼ë¡œ ì¬ê²€ìƒ‰ (ì•½ê°„ ì—„ê²©í•œ ì„ê³„ê°’ ì ìš©)
                                    # ì„¼íŠ¸ë¡œì´ë“œëŠ” ì—¬ëŸ¬ í”„ë ˆì„ í‰ê· ì´ë¯€ë¡œ ê¸°ë³¸ë³´ë‹¤ +0.03ë§Œ ìƒí–¥
                                    centroid_threshold = config.Thresholds.SIMILARITY + 0.03  # ê¸°ë³¸ ì„ê³„ê°’ + 0.03
                                    logging.debug(f"ğŸ” {person_id_text} ì„¼íŠ¸ë¡œì´ë“œ FAISS ê²€ìƒ‰: ì„ê³„ê°’={centroid_threshold:.3f}")
                                    person_name_centroid, similarity_score_centroid = find_best_match_faiss(
                                        normalized_avg_embedding, face_database, centroid_threshold
                                    )
                                    logging.debug(f"ğŸ” {person_id_text} ì„¼íŠ¸ë¡œì´ë“œ ê²°ê³¼: {person_name_centroid} (ìœ ì‚¬ë„={similarity_score_centroid:.3f})")
                                    # ìºì‹œì— ì €ì¥ (TTLCacheê°€ ìë™ìœ¼ë¡œ ë§Œë£Œ ì²˜ë¦¬)
                                    centroid_cache[cam_id].put(person_box_key, {
                                        'name': person_name_centroid,
                                        'score': similarity_score_centroid
                                    })
                                else:
                                    person_name_centroid = "Unknown"
                                    similarity_score_centroid = 0.0
                                    logging.debug(f"âš ï¸ {person_id_text} ì„¼íŠ¸ë¡œì´ë“œ ì •ê·œí™” ì‹¤íŒ¨: norm={norm}")
                            else:
                                person_name_centroid = "Unknown"
                                similarity_score_centroid = 0.0
                        
                        # ì„¼íŠ¸ë¡œì´ë“œ ê²°ê³¼ê°€ ë” ì¢‹ìœ¼ë©´ ì‚¬ìš© (Unknownì´ ì•„ë‹ˆê³  similarityê°€ ë” ë†’ìœ¼ë©´)
                        if person_name_centroid != "Unknown" and (person_name == "Unknown" or similarity_score_centroid > similarity_score):
                            logging.debug(f"âœ… {person_id_text} ì„¼íŠ¸ë¡œì´ë“œ ê²°ê³¼ ì ìš©: {person_name_centroid} (ìœ ì‚¬ë„={similarity_score_centroid:.3f}, ê¸°ì¡´={person_name}, ìœ ì‚¬ë„={similarity_score:.3f})")
                            person_name = person_name_centroid
                            similarity_score = similarity_score_centroid
                            # ë²„í¼ ì´ˆê¸°í™” (ì¸ì‹ ì„±ê³µ ì‹œ)
                            buffer_data['embeddings'] = []
                        else:
                            logging.debug(f"ğŸ” {person_id_text} ì„¼íŠ¸ë¡œì´ë“œ ê²°ê³¼ ë¯¸ì‚¬ìš©: ì„¼íŠ¸ë¡œì´ë“œ={person_name_centroid} (ìœ ì‚¬ë„={similarity_score_centroid:.3f}), ê¸°ì¡´={person_name} (ìœ ì‚¬ë„={similarity_score:.3f})")
                
                # â­ ìœ„ì¹˜ ê¸°ë°˜ íˆ¬í‘œë¡œ ì´ë¦„ ì•ˆì •í™” (ê¹œë¹¡ê±°ë¦¼ ë°©ì§€)
                if person_name != "Unknown":
                    from state import vote_for_name
                    voted_name, voted_score = vote_for_name(
                        cam_id, int(x1), int(y1), int(x2), int(y2), 
                        person_name, float(similarity_score)
                    )
                    if voted_name != person_name:
                        logging.debug(f"[CAM-{cam_id}] íˆ¬í‘œë¡œ ì´ë¦„ ì•ˆì •í™”: {person_name} â†’ {voted_name}")
                    person_name = voted_name
                    similarity_score = voted_score
                
                # ë™ì¼ ì´ë¦„ ì¤‘ë³µ ë°©ì§€: ê°™ì€ ì´ë¦„ì´ ì—¬ëŸ¬ ë°•ìŠ¤ì— í• ë‹¹ë˜ë©´ ê°€ì¥ ë†’ì€ similarityë§Œ ìœ ì§€
                if person_name != "Unknown":
                    if person_name not in name_to_boxes:
                        name_to_boxes[person_name] = []
                    name_to_boxes[person_name].append(((x1, y1, x2, y2), float(similarity_score), i))
            
            # ì˜¤ë˜ëœ ì„ë² ë”© ë²„í¼ ë° ìºì‹œ ì •ë¦¬ (ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°œì„ )
            # ì£¼ê¸°ì  ì •ë¦¬ë¡œ ìµœì í™”: ë§¤ í”„ë ˆì„ë§ˆë‹¤ ì‹¤í–‰í•˜ì§€ ì•Šê³  10ì´ˆë§ˆë‹¤ ì‹¤í–‰
            if not hasattr(process_single_frame, '_last_cleanup_time'):
                process_single_frame._last_cleanup_time = 0.0
            
            current_time_cleanup = time.time()
            CLEANUP_INTERVAL = 10.0  # 10ì´ˆë§ˆë‹¤ ì •ë¦¬
            
            if current_time_cleanup - process_single_frame._last_cleanup_time > CLEANUP_INTERVAL:
                cleanup_threshold = 5.0  # 5ì´ˆ ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•Šì€ ë²„í¼ ì œê±°
                
                # embedding_buffers ì •ë¦¬ (ìµœì í™”: list() ë³€í™˜ ìµœì†Œí™”)
                for cam_id_cleanup in list(embedding_buffers.keys()):
                    if cam_id_cleanup not in embedding_buffers:  # ì•ˆì „ì„± ì²´í¬
                        continue
                    
                    # ì¹´ë©”ë¼ë³„ ë²„í¼ ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ ìµœì í™”)
                    if len(embedding_buffers[cam_id_cleanup]) > MAX_EMBEDDING_BUFFERS_PER_CAM:
                        # ê°€ì¥ ì˜¤ë˜ëœ ë²„í¼ë¶€í„° ì œê±°
                        sorted_keys = sorted(
                            embedding_buffers[cam_id_cleanup].keys(),
                            key=lambda k: embedding_buffers[cam_id_cleanup][k].get('last_update', 0)
                        )
                        # ì´ˆê³¼ë¶„ ì œê±°
                        for key_to_remove in sorted_keys[:-MAX_EMBEDDING_BUFFERS_PER_CAM]:
                            if key_to_remove in embedding_buffers[cam_id_cleanup]:
                                del embedding_buffers[cam_id_cleanup][key_to_remove]
                            # ê´€ë ¨ ìºì‹œë„ ì œê±°
                            if cam_id_cleanup in centroid_cache:
                                centroid_cache[cam_id_cleanup].remove(key_to_remove)
                        logging.debug(f"CAM-{cam_id_cleanup} ë²„í¼ ìˆ˜ ì œí•œ: {MAX_EMBEDDING_BUFFERS_PER_CAM}ê°œë¡œ ì¶•ì†Œ")
                    
                    # í‚¤ ëª©ë¡ì„ í•œ ë²ˆë§Œ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
                    keys_to_check = list(embedding_buffers[cam_id_cleanup].keys())
                    for key in keys_to_check:
                        if key not in embedding_buffers[cam_id_cleanup]:  # ì‚­ì œë˜ì—ˆì„ ìˆ˜ ìˆìŒ
                            continue
                        buffer_data = embedding_buffers[cam_id_cleanup].get(key)
                        if buffer_data is None:
                            continue
                        # ë²„í¼ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ì œê±°
                        if len(buffer_data.get('embeddings', [])) == 0 or (current_time_cleanup - buffer_data.get('last_update', 0)) > cleanup_threshold:
                            if key in embedding_buffers[cam_id_cleanup]:
                                del embedding_buffers[cam_id_cleanup][key]
                            # ê´€ë ¨ ìºì‹œë„ ì œê±° (ì•ˆì „í•œ ì ‘ê·¼)
                            if cam_id_cleanup in centroid_cache:
                                centroid_cache[cam_id_cleanup].remove(key)
                
                # ì˜¤ë˜ëœ ì„¼íŠ¸ë¡œì´ë“œ ìºì‹œ ì •ë¦¬ (TTLCacheê°€ ìë™ìœ¼ë¡œ ë§Œë£Œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê°„ì†Œí™”)
                # ì£¼ê¸°ì ìœ¼ë¡œ ë§Œë£Œëœ í•­ëª©ë§Œ ì œê±° (ì„±ëŠ¥ ìµœì í™”)
                for cam_id_cleanup in list(centroid_cache.keys()):
                    if cam_id_cleanup in centroid_cache:
                        # TTLCacheì˜ clear_expired() í˜¸ì¶œí•˜ì—¬ ë§Œë£Œëœ í•­ëª© ì œê±°
                        centroid_cache[cam_id_cleanup].clear_expired()
                
                # ì •ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
                process_single_frame._last_cleanup_time = current_time_cleanup
            
            # ì˜¤ë˜ëœ ë„˜ì–´ì§ ê°ì§€ ì‹œê°„ ì¶”ì  ì •ë¦¬ (ìµœì í™”: list() ë³€í™˜ ìµœì†Œí™”)
            for cam_id_cleanup in list(fall_start_times.keys()):
                if cam_id_cleanup not in fall_start_times:  # ì•ˆì „ì„± ì²´í¬
                    continue
                # í‚¤ ëª©ë¡ì„ í•œ ë²ˆë§Œ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
                keys_to_check = list(fall_start_times[cam_id_cleanup].keys())
                for key in keys_to_check:
                    if key not in fall_start_times[cam_id_cleanup]:  # ì‚­ì œë˜ì—ˆì„ ìˆ˜ ìˆìŒ
                        continue
                    fall_time = fall_start_times[cam_id_cleanup].get(key)
                    if fall_time is None:
                        continue
                    if (current_time_cleanup - fall_time) > FALL_DURATION_THRESHOLD * 3:
                        if key in fall_start_times[cam_id_cleanup]:
                            del fall_start_times[cam_id_cleanup][key]
            
            # ì˜¤ë˜ëœ ì–¼êµ´ ë°”ìš´ë”©ë°•ìŠ¤ ìºì‹œ ì •ë¦¬ (TTLCacheê°€ ìë™ìœ¼ë¡œ ë§Œë£Œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê°„ì†Œí™”)
            # ì£¼ê¸°ì ìœ¼ë¡œ ë§Œë£Œëœ í•­ëª©ë§Œ ì œê±° (ì„±ëŠ¥ ìµœì í™”)
            for cam_id_cleanup in list(face_bbox_cache.keys()):
                if cam_id_cleanup in face_bbox_cache:
                    # TTLCacheì˜ clear_expired() í˜¸ì¶œí•˜ì—¬ ë§Œë£Œëœ í•­ëª© ì œê±°
                    face_bbox_cache[cam_id_cleanup].clear_expired()
            
            # 2ë‹¨ê³„: name_to_boxesë¥¼ ì²˜ë¦¬í•˜ì—¬ ê° person_indexì˜ ìµœì¢… ì´ë¦„ ê²°ì •
            for name, boxes_scores_indices in name_to_boxes.items():
                if len(boxes_scores_indices) == 1:
                    # ì´ë¦„ì´ 1ê°œë§Œ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    (x1, y1, x2, y2), score, person_idx = boxes_scores_indices[0]
                    person_final_names[person_idx] = name
                else:
                    # ê°™ì€ ì´ë¦„ì´ ì—¬ëŸ¬ ë°•ìŠ¤ì— í• ë‹¹ë¨: ëª¨ë‘ ê°™ì€ ì´ë¦„ í—ˆìš©
                    # (ì‹¤ì œë¡œ ì—¬ëŸ¬ ì‚¬ëŒì´ ê°™ì€ ì´ë¦„ìœ¼ë¡œ ì¸ì‹ë  ìˆ˜ ìˆìŒ - íˆ¬í‘œ ì‹œìŠ¤í…œì´ ì²˜ë¦¬)
                    for box, score, idx in boxes_scores_indices:
                        person_final_names[idx] = name
            
            # 3ë‹¨ê³„: ìµœì¢… ì´ë¦„ìœ¼ë¡œ ë Œë”ë§ ë° ì²˜ë¦¬
            # person_statusë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
            person_status_map: Dict[int, str] = {}
            
            for person_data in person_data_list:
                i = person_data['index']
                person_id_text = person_data['person_id']
                x1, y1, x2, y2 = person_data['box']
                # ìµœì¢… ì´ë¦„ ì‚¬ìš© (ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼)
                person_name = person_final_names.get(i, person_data.get('name', 'Unknown'))
                similarity_score = person_data.get('similarity', 0.0)

                # PPE ìœ„ë°˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (í†µí•© í•¨ìˆ˜ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)
                ppe_violations = person_data.get('ppe_violations', [])
                
                # ìƒíƒœ ì´ˆê¸°í™”
                person_status = "SAFE"
                status_details = []
                current_violations = list(ppe_violations)  # PPE ìœ„ë°˜ ë³µì‚¬
                
                # PPE ìœ„ë°˜ì´ ìˆìœ¼ë©´ VIOLATION ìƒíƒœ
                if ppe_violations:
                    person_status = "VIOLATION"
                    for rule in ppe_violations:
                        status_details.append(f"{rule}: VIOLATION")

                # ìœ„í—˜ í–‰ë™ ê°ì§€ ê²°ê³¼ ì‚¬ìš© (ì´ë¯¸ ì–¼êµ´ ì¸ì‹ ì „ì— ìˆ˜í–‰ë¨)
                is_dangerous_detected = person_data.get('is_dangerous', False)
                violation_type = person_data.get('violation_type', '')
                
                # person_box_key ìƒì„± (í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
                person_box_key = _generate_person_box_key(cam_id, matched_entry, x1, y1, x2, y2)
                
                # ìœ„í—˜í•  ë•Œë§Œ ìƒíƒœ ë³€ê²½ ë° ìœ„ë°˜ ëª©ë¡ì— ì¶”ê°€
                if is_dangerous_detected and violation_type:
                    person_status = "FALL"
                    status_details.append("ë„˜ì–´ì§ ê°ì§€")
                    current_violations.append("ë„˜ì–´ì§")
                    logging.warning(f"âš ï¸ ìœ„í—˜ í–‰ë™ ê°ì§€: {person_box_key} - {violation_type}")
                
                # person_status ì €ì¥ (recognized_faces í•„í„°ë§ìš©)
                person_status_map[i] = person_status

                # ë Œë”ë§ ì •ì±…: person_boxëŠ” ê·¸ë¦¬ì§€ ì•Šê³ , PPE ê°ì§€ ë°•ìŠ¤ë§Œ ê·¸ë¦¼
                # ì–¼êµ´ ì¸ì‹ì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† ì§„í–‰ (í…ìŠ¤íŠ¸ë¡œë§Œ í‘œì‹œ)
                ppe_boxes_list = person_data.get('ppe_boxes', [])
                face_bbox = person_data.get('face_bbox')
                
                # ì–¼êµ´ ë°”ìš´ë”©ë°•ìŠ¤ ìºì‹œ ì²˜ë¦¬ (ê¹œë¹¡ì„ ë°©ì§€)
                current_time = time.time()
                cached_face_bbox = None
                
                # í˜„ì¬ í”„ë ˆì„ì—ì„œ ì–¼êµ´ì´ ê°ì§€ë˜ì—ˆìœ¼ë©´ ìºì‹œ ì—…ë°ì´íŠ¸
                if face_bbox is not None:
                    # TTLCacheì— ì €ì¥ (ìë™ ë§Œë£Œ ì²˜ë¦¬)
                    face_bbox_cache[cam_id].put(person_box_key, {
                        'face_bbox': face_bbox,
                        'person_box': (x1, y1, x2, y2)
                    })
                    cached_face_bbox = face_bbox
                else:
                    # ìºì‹œì—ì„œ ì´ì „ ì–¼êµ´ ë°”ìš´ë”©ë°•ìŠ¤ ì°¾ê¸° (IoU ê¸°ë°˜ ë§¤ì¹­)
                    # ë¨¼ì € person_box_keyë¡œ ì§ì ‘ ì°¾ê¸°
                    cached_entry = face_bbox_cache[cam_id].get(person_box_key)
                    if cached_entry:
                        cached_person_box = cached_entry.get('person_box', (0, 0, 0, 0))
                        iou = utils.calculate_iou((x1, y1, x2, y2), cached_person_box)
                        
                        # IoUê°€ 0.3 ì´ìƒì´ë©´ ê°™ì€ ì‚¬ëŒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ìºì‹œëœ ë°”ìš´ë”©ë°•ìŠ¤ ì‚¬ìš©
                        if iou >= 0.3:
                            cached_face_bbox = cached_entry.get('face_bbox')
                            # ìºì‹œ ì—…ë°ì´íŠ¸ (TTLCacheì— ë‹¤ì‹œ ì €ì¥í•˜ì—¬ TTL ê°±ì‹ )
                            face_bbox_cache[cam_id].put(person_box_key, {
                                'face_bbox': cached_face_bbox,
                                'person_box': (x1, y1, x2, y2)
                            })
                    else:
                        # person_box_keyë¡œ ì°¾ì§€ ëª»í•˜ë©´ IoU ê¸°ë°˜ìœ¼ë¡œ ëª¨ë“  ìºì‹œ í•­ëª© ê²€ìƒ‰
                        # TTLCacheëŠ” keys()ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ë°©ë²• ì‚¬ìš©
                        # ëŒ€ì‹  person_box_key ê¸°ë°˜ ë§¤ì¹­ë§Œ ì‚¬ìš© (ì„±ëŠ¥ ìµœì í™”)
                        # IoU ê¸°ë°˜ ì „ì²´ ê²€ìƒ‰ì€ ì œê±° (TTLCache íŠ¹ì„±ìƒ ì–´ë ¤ì›€)
                        cached_face_bbox = None
                
                # ìºì‹œëœ ì–¼êµ´ ë°”ìš´ë”©ë°•ìŠ¤ ì‚¬ìš© (ì—†ìœ¼ë©´ None)
                face_bbox_to_draw = face_bbox if face_bbox is not None else cached_face_bbox
                
                    # PPE ë°•ìŠ¤ê°€ ìˆê±°ë‚˜ ìœ„ë°˜ì´ ìˆê±°ë‚˜ ì–¼êµ´ì´ ê°ì§€ë˜ë©´ ë Œë”ë§
                if ppe_boxes_list or current_violations or person_status != "SAFE" or person_name != "Unknown" or face_bbox_to_draw is not None:
                    # í—¬ë©§ ë°•ìŠ¤ ì°¾ê¸° (ì–¼êµ´ ì¸ì‹ ê²°ê³¼ í‘œì‹œìš©)
                    helmet_box = None
                    for ppe_box_info in ppe_boxes_list:
                        ppe_class = ppe_box_info['class']
                        # Hardhat ë˜ëŠ” NO-Hardhat í´ë˜ìŠ¤ ì°¾ê¸°
                        if "Hardhat" in ppe_class:
                            helmet_box = ppe_box_info
                            break
                    
                    # ì–¼êµ´ ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì–¼êµ´ì´ ê°ì§€ë˜ê±°ë‚˜ ìºì‹œì— ìˆìœ¼ë©´ í‘œì‹œ)
                    if face_bbox_to_draw is not None:
                        # person_img_for_detectionì˜ ì¢Œí‘œë¥¼ ì›ë³¸ í”„ë ˆì„ ì¢Œí‘œë¡œ ë³€í™˜
                        # person_img_for_detectionì€ person_box ì˜ì—­ì„ ì¶”ì¶œí•œ ì´ë¯¸ì§€
                        # face_bboxëŠ” person_img_for_detection ë‚´ì˜ ì¢Œí‘œ
                        fx1, fy1, fx2, fy2 = face_bbox_to_draw
                        
                        # person_img_for_detectionì´ ë¦¬ì‚¬ì´ì¦ˆë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì›ë³¸ person_box í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§
                        # person_img_for_detectionì˜ ì›ë³¸ í¬ê¸° í™•ì¸
                        person_img = person_data.get('img')
                        if person_img is not None:
                            person_img_h, person_img_w = person_img.shape[:2]
                            # person_boxì˜ ì‹¤ì œ í¬ê¸°
                            person_box_w = x2 - x1
                            person_box_h = y2 - y1
                            
                            # ìŠ¤ì¼€ì¼ ê³„ì‚°
                            scale_x = person_box_w / person_img_w if person_img_w > 0 else 1.0
                            scale_y = person_box_h / person_img_h if person_img_h > 0 else 1.0
                            
                            # ì›ë³¸ í”„ë ˆì„ ì¢Œí‘œë¡œ ë³€í™˜
                            face_x1 = int(x1 + fx1 * scale_x)
                            face_y1 = int(y1 + fy1 * scale_y)
                            face_x2 = int(x1 + fx2 * scale_x)
                            face_y2 = int(y1 + fy2 * scale_y)
                            
                            # ì–¼êµ´ ë°•ìŠ¤ ì¢Œí‘œ ì €ì¥ (í†µí•© ë°•ìŠ¤ìš©)
                            # (ê·¸ë¦¬ì§€ëŠ” ì•ŠìŒ)

                    # person_box ê¸°ì¤€ìœ¼ë¡œ ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ê° ì‚¬ëŒ ë…ë¦½ì ìœ¼ë¡œ)
                    # ìƒ‰ìƒ ë° íˆ¬ëª…ë„ ê²°ì •
                    if person_status == "FALL":
                        unified_color = (0, 50, 255)  # ë°ì€ ë¹¨ê°„ìƒ‰ (ìœ„í—˜)
                        alpha = 0.25
                    elif current_violations:
                        unified_color = (0, 140, 255)  # ë°ì€ ì£¼í™©ìƒ‰ (ìœ„ë°˜)
                        alpha = 0.2
                    else:
                        unified_color = (50, 255, 50)  # ë°ì€ ì´ˆë¡ìƒ‰ (ì¤€ìˆ˜)
                        alpha = 0.15

                    # ë Œë”ë§ì´ í•„ìš”í•œ ì‹œì ì— í”„ë ˆì„ ë³µì‚¬ (ë©”ëª¨ë¦¬ ìµœì í™”: í•„ìš”í•  ë•Œë§Œ ë³µì‚¬)
                    if processed_frame is None:
                        processed_frame = frame.copy()
                    
                    # person_boxë¡œ ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (í˜„ëŒ€ì  ìŠ¤íƒ€ì¼)
                    draw_modern_bbox(processed_frame, x1, y1, x2, y2, unified_color, thickness=3, corner_length=35, alpha=alpha)
                    
                    # ìƒíƒœ í…ìŠ¤íŠ¸ í‘œì‹œ (person_box ìœ„ì— í‘œì‹œ) - í”„ë¡ íŠ¸ì—”ë“œì™€ ë™ì¼í•œ í˜•ì‹
                    # ë¼ë²¨ í…ìŠ¤íŠ¸ êµ¬ì„±: "ì´ë¦„: ìœ„ë°˜ë‚´ì—­" í˜•ì‹
                    display_name = person_name if person_name != "Unknown" else "ì•Œ ìˆ˜ ì—†ìŒ"
                    
                    # ìœ„ë°˜ ì •ë³´ ìˆ˜ì§‘
                    violation_parts = []
                    if "ë„˜ì–´ì§" in current_violations:
                        violation_parts.append("ë„˜ì–´ì§ ê°ì§€")
                    # PPE ìœ„ë°˜ ì •ë³´ ìˆ˜ì§‘
                    ppe_violations_display = []
                    for v in current_violations:
                        if v == "ì•ˆì „ëª¨":
                            ppe_violations_display.append("ì•ˆì „ëª¨")
                        elif v == "ì•ˆì „ì¡°ë¼":
                            ppe_violations_display.append("ì•ˆì „ì¡°ë¼")
                        elif v == "ë§ˆìŠ¤í¬":
                            ppe_violations_display.append("ë§ˆìŠ¤í¬")
                    
                    # ìœ„ë°˜ ë¬¸ìì—´ êµ¬ì„±
                    violation_str = ""
                    if violation_parts or ppe_violations_display:
                        violation_list = violation_parts + ppe_violations_display
                        if "ë„˜ì–´ì§ ê°ì§€" in violation_list and len(violation_list) > 1:
                            # ë„˜ì–´ì§ê³¼ PPE ìœ„ë°˜ì´ í•¨ê»˜ ìˆì„ ë•Œ
                            ppe_only = [v for v in violation_list if v != "ë„˜ì–´ì§ ê°ì§€"]
                            violation_str = f"ë„˜ì–´ì§ ê°ì§€, {', '.join(ppe_only)} ë¯¸ì°©ìš©"
                        elif "ë„˜ì–´ì§ ê°ì§€" in violation_list:
                            violation_str = "ë„˜ì–´ì§ ê°ì§€"
                        else:
                            violation_str = f"{', '.join(ppe_violations_display)} ë¯¸ì°©ìš©"
                    
                    # ìµœì¢… ë¼ë²¨ í…ìŠ¤íŠ¸ êµ¬ì„±
                    if violation_str:
                        status_text = f"{display_name}: {violation_str}"
                    else:
                        status_text = display_name
                    
                    # ë¼ë²¨ í‘œì‹œ ì¡°ê±´: ì–¼êµ´ ì¸ì‹ì´ ì„±ê³µí•˜ë©´ íŠ¸ë˜í‚¹ìœ¼ë¡œ ê³„ì† ì´ë¦„ í‘œì‹œ
                    # ìœ„ë°˜ì´ ìˆìœ¼ë©´ ì–¼êµ´ì´ ì—†ì–´ë„ ìœ„ë°˜ ì •ë³´ëŠ” í‘œì‹œ
                    should_show_label = False
                    if current_violations:
                        # ìœ„ë°˜ì´ ìˆìœ¼ë©´ í•­ìƒ í‘œì‹œ
                        should_show_label = True
                    elif person_name != "Unknown":
                        # ì–¼êµ´ ì¸ì‹ì´ ì„±ê³µí•œ ê²½ìš° (í•œ ë²ˆ ì¸ì‹ë˜ë©´ íŠ¸ë˜í‚¹ìœ¼ë¡œ ê³„ì† í‘œì‹œ)
                        # person_nameì´ "Unknown"ì´ ì•„ë‹ˆë©´ ì´ë¯¸ ì¸ì‹ëœ ì´ë¦„ì´ë¯€ë¡œ ê³„ì† í‘œì‹œ
                        # í˜„ì¬ í”„ë ˆì„ì—ì„œ ì–¼êµ´ì´ ê°ì§€ë˜ì—ˆê±°ë‚˜, ì´ì „ì— ì¸ì‹ëœ ì´ë¦„ì´ ìˆìœ¼ë©´ í‘œì‹œ
                        if face_bbox is not None:
                            # í˜„ì¬ í”„ë ˆì„ì—ì„œ ì–¼êµ´ ê°ì§€: í•­ìƒ í‘œì‹œ
                            should_show_label = True
                        else:
                            # ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šì•„ë„, person_nameì´ "Unknown"ì´ ì•„ë‹ˆë©´
                            # ì´ì „ì— ì¸ì‹ëœ ì´ë¦„ì´ë¯€ë¡œ íŠ¸ë˜í‚¹ìœ¼ë¡œ ê³„ì† í‘œì‹œ
                            should_show_label = True
                    
                    if should_show_label:
                        # person_box ìœ„ì¹˜ì— í…ìŠ¤íŠ¸ í‘œì‹œ
                        text_x, text_y = x1, y1

                        # ìƒ‰ìƒ ê²°ì • (ë°•ìŠ¤ì™€ ë™ì¼)
                        text_color = unified_color

                        # person_box ìœ„ì— í…ìŠ¤íŠ¸ í‘œì‹œ
                        renderer.add_text(status_text, (text_x, text_y - 10), text_color)

                # ìœ„ë°˜ ì‚¬í•­ ê¸°ë¡ (ì¤‘ë³µ ì œê±°: ê°™ì€ ì‚¬ëŒ ë°•ìŠ¤ì— ëŒ€í•´ í•œ ë²ˆë§Œ ê¸°ë¡)
                if current_violations:
                    # ë°°ì¹˜ IoU ê³„ì‚°ìœ¼ë¡œ ì¤‘ë³µ í™•ì¸ ìµœì í™”
                    is_duplicate = False
                    if len(violations_found) > 0:
                        # ê¸°ì¡´ ìœ„ë°˜ ë°•ìŠ¤ ë°°ì—´ ì¤€ë¹„
                        existing_boxes = []
                        for existing_violation in violations_found:
                            ex_box = existing_violation.get("person_box", [])
                            if len(ex_box) == 4:
                                existing_boxes.append(ex_box)
                        
                        if len(existing_boxes) > 0:
                            # ë°°ì¹˜ IoU ê³„ì‚°
                            current_box_array = np.array([(x1, y1, x2, y2)], dtype=np.float32)
                            existing_boxes_array = np.array(existing_boxes, dtype=np.float32)
                            iou_matrix = calculate_iou_batch(current_box_array, existing_boxes_array)
                            
                            # ìµœëŒ€ IoUê°€ 0.6 ì´ìƒì´ë©´ ì¤‘ë³µ
                            max_iou = float(np.max(iou_matrix))
                            if max_iou > 0.6:
                                is_duplicate = True
                    
                    if not is_duplicate:
                        # cam_idë¥¼ areaë¡œ ë§¤í•‘ (0â†’A-1, 1â†’A-2, 2â†’B-1, 3â†’B-2)
                        area_map = {0: "A-1", 1: "A-2", 2: "B-1", 3: "B-2"}
                        area = area_map.get(cam_id, f"A-{cam_id+1}")
                        
                        # ìœ„ë°˜ ë‚´ìš©ì„ hazard ë¬¸ìì—´ë¡œ ë³€í™˜
                        # ì˜ˆ: "PPE ìœ„ë°˜ë‚´ì—­: ì•ˆì „ëª¨, ë§ˆìŠ¤í¬, ì•ˆì „ì¡°ë¼"
                        # ì¤‘ë³µ ì œê±°: ìœ„ë°˜ ìœ í˜•ì„ setìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¤‘ë³µ ì œê±°
                        # ìµœì í™”: current_violationsê°€ ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ set ë³€í™˜ í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
                        if isinstance(current_violations, (set, tuple)):
                            unique_violations = list(current_violations)
                        else:
                            unique_violations = list(set(current_violations))  # ì¤‘ë³µ ì œê±°
                        ppe_violations = []
                        other_violations = []
                        
                        for violation_type in unique_violations:
                            if violation_type == "ë„˜ì–´ì§":
                                other_violations.append("ë„˜ì–´ì§ ê°ì§€")
                            elif violation_type == "ì•ˆì „ëª¨":
                                ppe_violations.append("ì•ˆì „ëª¨")
                            elif violation_type == "ë§ˆìŠ¤í¬":
                                ppe_violations.append("ë§ˆìŠ¤í¬")
                            elif violation_type == "ì•ˆì „ì¡°ë¼":
                                ppe_violations.append("ì•ˆì „ì¡°ë¼")
                            else:
                                other_violations.append(f"ìœ„ë°˜: {violation_type}")
                        
                        # PPE ìœ„ë°˜ì´ ìˆìœ¼ë©´ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ í‘œì‹œ
                        if ppe_violations:
                            hazard = f"PPE ìœ„ë°˜ë‚´ì—­: {', '.join(ppe_violations)}"
                            if other_violations:
                                hazard += f", {', '.join(other_violations)}"
                        elif other_violations:
                            hazard = ", ".join(other_violations)
                        else:
                            hazard = "ìœ„ë°˜ ê°ì§€"
                        
                        # worker ì´ë¦„: recognized_nameì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ "ì•Œ ìˆ˜ ì—†ìŒ"
                        worker = person_name if person_name != "Unknown" else "ì•Œ ìˆ˜ ì—†ìŒ"
                        
                        violations_found.append({
                            "person_box": [x1, y1, x2, y2],
                            "violations": unique_violations,  # ì¤‘ë³µ ì œê±°ëœ ìœ„ë°˜ ëª©ë¡
                            "recognized_name": person_name,
                            "worker": worker,
                            "area": area,
                            "level": "WARNING",
                            "hazard": hazard
                        })

            # í”„ë ˆì„ ë‚´ ë™ì¼ ì´ë¦„ ì¤‘ë³µ ì œê±°: person_final_namesì—ì„œ ìµœì¢… ì´ë¦„ìœ¼ë¡œ recognized_faces êµ¬ì„±
            # ìœ„ë°˜ì´ ìˆëŠ” ì‚¬ëŒë§Œ recognized_facesì— ì¶”ê°€ (ì–¼êµ´ ì¸ì‹ì€ ê³„ì† ì‹¤í–‰í•˜ë˜ ìœ„ë°˜ ì‹œì—ë§Œ ì „ì†¡)
            # ì¤‘ìš”: ê° ì‚¬ëŒë§ˆë‹¤ ë³„ë„ì˜ í•­ëª©ì„ ë³´ë‚´ì•¼ í•¨ (ê°™ì€ ì´ë¦„ì´ì–´ë„ ë‹¤ë¥¸ person_boxë¥¼ ê°€ì§€ë©´ ë‹¤ë¥¸ ì‚¬ëŒ)
            
            # ë””ë²„ê¹…: person_data_list ê°œìˆ˜ í™•ì¸
            logging.debug(f"[CAM-{cam_id}] person_data_list: {len(person_data_list)}ëª… ê°ì§€")
            
            added_boxes = set()  # ë°•ìŠ¤ íŠœí”Œ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (IoU ê³„ì‚°ìš©)
            skipped_count = 0  # ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µëœ ì‚¬ëŒ ìˆ˜
            added_count = 0  # ì¶”ê°€ëœ ì‚¬ëŒ ìˆ˜
            
            for person_data in person_data_list:
                i = person_data['index']
                x1, y1, x2, y2 = person_data['box']
                final_name = person_final_names.get(i, person_data.get('name', 'Unknown'))
                similarity_score = person_data.get('similarity', 0.0)
                person_status = person_status_map.get(i, "SAFE")
                
                # ë°•ìŠ¤ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (IoU ê¸°ë°˜ìœ¼ë¡œ ë§¤ìš° ì—„ê²©í•˜ê²Œ)
                # IoU 0.98 ì´ìƒì´ë©´ ê°™ì€ ì‚¬ëŒìœ¼ë¡œ ê°„ì£¼ (ê±°ì˜ ì™„ì „íˆ ê²¹ì¹˜ëŠ” ê²½ìš°ë§Œ)
                # ë©€ë¦¬ ìˆëŠ” ì‚¬ëŒì€ IoUê°€ ë‚®ìœ¼ë¯€ë¡œ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼ë˜ì§€ ì•ŠìŒ
                is_duplicate = False
                for seen_box in added_boxes:
                    if len(seen_box) == 4:
                        # ê±°ë¦¬ ê¸°ë°˜ í•„í„°ë§ ë¨¼ì € ìˆ˜í–‰ (ì„±ëŠ¥ í–¥ìƒ ë° ì •í™•ë„ í–¥ìƒ)
                        current_center_x = (x1 + x2) / 2
                        current_center_y = (y1 + y2) / 2
                        seen_center_x = (seen_box[0] + seen_box[2]) / 2
                        seen_center_y = (seen_box[1] + seen_box[3]) / 2
                        
                        # í˜„ì¬ ë°•ìŠ¤ì˜ ëŒ€ê°ì„  ê¸¸ì´ ê³„ì‚°
                        current_diagonal = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5
                        center_distance = ((current_center_x - seen_center_x) ** 2 + (current_center_y - seen_center_y) ** 2) ** 0.5
                        
                        # ê±°ë¦¬ê°€ ëŒ€ê°ì„  ê¸¸ì´ì˜ 0.5ë°° ì´ìƒì´ë©´ ì¤‘ë³µì´ ì•„ë‹˜ (IoU ê³„ì‚° ìƒëµ)
                        if center_distance > current_diagonal * 0.5:
                            continue  # ë©€ë¦¬ ìˆìœ¼ë©´ ì¤‘ë³µ ì•„ë‹˜
                        
                        # ê±°ë¦¬ê°€ ê°€ê¹Œìš°ë©´ IoU ê³„ì‚°
                        iou = utils.calculate_iou((x1, y1, x2, y2), seen_box)
                        # IoU 0.98 ì´ìƒì´ë©´ ê°™ì€ ì‚¬ëŒìœ¼ë¡œ ê°„ì£¼ (ê±°ì˜ ì™„ì „íˆ ê²¹ì¹˜ëŠ” ê²½ìš°ë§Œ)
                        # ë©€ë¦¬ ìˆëŠ” ì‚¬ëŒì€ IoUê°€ ë‚®ìœ¼ë¯€ë¡œ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼ë˜ì§€ ì•ŠìŒ
                        if iou > 0.98:  # 0.95 -> 0.98ë¡œ ë” ì—„ê²©í•˜ê²Œ (ê±°ì˜ ì™„ì „íˆ ê²¹ì¹˜ëŠ” ê²½ìš°ë§Œ)
                            is_duplicate = True
                            logging.debug(f"[CAM-{cam_id}] ì¤‘ë³µ ì œê±°: person_idx={i}, box=({x1}, {y1}, {x2}, {y2}), IoU={iou:.3f}, seen_box={seen_box}, ê±°ë¦¬={center_distance:.1f}")
                            break
                        elif iou > 0.5:  # IoUê°€ 0.5 ì´ìƒì´ë©´ ë””ë²„ê¹… ë¡œê·¸ ì¶œë ¥
                            logging.debug(f"[CAM-{cam_id}] IoU ì²´í¬: person_idx={i}, box=({x1}, {y1}, {x2}, {y2}), IoU={iou:.3f}, seen_box={seen_box}, ê±°ë¦¬={center_distance:.1f} (ì¤‘ë³µ ì•„ë‹˜)")
                
                if is_duplicate:
                    # ê±°ì˜ ì™„ì „íˆ ê²¹ì¹˜ëŠ” ì‚¬ëŒì€ ì´ë¯¸ ì¶”ê°€ë¨ (ì¤‘ë³µ ë°©ì§€)
                    skipped_count += 1
                    continue
                
                # PPE ìœ„ë°˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œìš©)
                ppe_violations = person_data.get('ppe_violations', [])
                
                # ë””ë²„ê¹…: PPE ìœ„ë°˜ ì •ë³´ í™•ì¸
                if ppe_violations:
                    logging.info(f"[CAM-{cam_id}] PPE ìœ„ë°˜ í™•ì¸: person_idx={i}, ìœ„ë°˜={ppe_violations}, ìƒíƒœ={person_status}")
                
                # ìœ„ë°˜ì´ ìˆëŠ” ì‚¬ëŒë§Œ recognized_facesì— ì¶”ê°€ (ì–¼êµ´ ì¸ì‹ì€ ê³„ì† ì‹¤í–‰í•˜ë˜ ìœ„ë°˜ ì‹œì—ë§Œ ì „ì†¡)
                # ìœ„ë°˜ì´ ìˆìœ¼ë©´ ì–¼êµ´ ì¸ì‹ì´ ì‹¤íŒ¨í•´ë„ "Unknown"ìœ¼ë¡œ ë³´ë‚´ê¸°
                if person_status != "SAFE" or ppe_violations:
                    # ì–¼êµ´ ì¸ì‹ ê²°ê³¼ ì‚¬ìš© (ì‹¤íŒ¨í•´ë„ "Unknown"ìœ¼ë¡œ ì„¤ì •ë¨)
                    face_name = final_name if final_name != "Unknown" else "Unknown"
                    
                    # â­â­ ì¤‘ë³µ ë°•ìŠ¤ ë°©ì§€: í•˜ë‚˜ì˜ Personì— í•˜ë‚˜ì˜ ë°•ìŠ¤ë§Œ ì¶”ê°€
                    # ìœ„ë°˜ ì •ë³´ì™€ ì–¼êµ´ ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ê°ì²´ì— í†µí•©
                    recognized_faces.append({
                        "box": [x1, y1, x2, y2],  # person_box ì‚¬ìš©
                        "bbox": [x1, y1, x2, y2],  # í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ bboxë„ ì¶”ê°€
                        "name": face_name,  # ì–¼êµ´ ì¸ì‹ ê²°ê³¼ (ì‹¤íŒ¨ ì‹œ "Unknown")
                        "worker": face_name,  # í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„± (worker í•„ë“œ)
                        "similarity": float(similarity_score),
                        "status": person_status,  # ìƒíƒœ ì •ë³´ ì¶”ê°€ (KPI ê³„ì‚°ìš©)
                        "ppe_violations": ppe_violations,  # PPE ìœ„ë°˜ ì •ë³´ ì¶”ê°€ (í”„ë¡ íŠ¸ì—”ë“œ í‘œì‹œìš©)
                        "isFace": True,  # ì–¼êµ´ ì¸ì‹ ì‹œë„í–ˆìŒì„ í‘œì‹œ
                        "isViolation": len(ppe_violations) > 0,  # ìœ„ë°˜ í”Œë˜ê·¸ (í”„ë¡ íŠ¸ì—”ë“œ í•„ìˆ˜)
                    })
                    added_boxes.add((x1, y1, x2, y2))  # ë°•ìŠ¤ ì¤‘ë³µ ë°©ì§€
                    added_count += 1
            
            # ë””ë²„ê¹…: recognized_faces ì¶”ê°€ ê²°ê³¼ í™•ì¸
            logging.debug(f"[CAM-{cam_id}] recognized_faces ì¶”ê°€: person_data_list={len(person_data_list)}ëª…, ì¶”ê°€={added_count}ëª…, ì¤‘ë³µì œê±°={skipped_count}ëª…, ìµœì¢…={len(recognized_faces)}ëª…")

        # 8. ê¸°íƒ€ ê°ì²´ ê·¸ë¦¬ê¸° (ì•ˆì „ ì¥ë¹„ëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ì œì™¸)
        # violationsì™€ recognized_facesì—ì„œ ì‚¬ëŒ ë°•ìŠ¤ ì¶”ì¶œ
        person_boxes_for_filter = []
        for v in violations:
            if 'person_box' in v and v['person_box']:
                person_boxes_for_filter.append(v['person_box'])
        for rf in recognized_faces:
            if 'box' in rf and rf['box']:
                person_boxes_for_filter.append(rf['box'])
        
        for class_name, detections in all_detections.items():
            # 'person' í´ë˜ìŠ¤ëŠ” pose_resultsì—ì„œ ì´ë¯¸ ì²˜ë¦¬í•˜ë¯€ë¡œ ì œì™¸
            if class_name.lower() == 'person':
                continue
            # Safety Con ë“± ì˜¤íƒì§€ í´ë˜ìŠ¤ í•„í„°ë§
            if class_name in config.Thresholds.IGNORED_CLASSES:
                continue
            # ì•ˆì „ ì¥ë¹„ í´ë˜ìŠ¤ëŠ” ì‚¬ëŒ ë°•ìŠ¤ì™€ í•¨ê»˜ ìœ„ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì œì™¸
            is_safety_gear = any(class_name in item.values() for item in config.Constants.SAFETY_RULES_MAP.values())
            if not is_safety_gear and detections:
                color = (255, 0, 0)  # íŒŒë€ìƒ‰ (BGR)
                for det in detections:
                    if det and 'bbox' in det and det['bbox'] and len(det['bbox']) == 4:
                        x1_obj, y1_obj, x2_obj, y2_obj = map(int, det['bbox'])

                        # ì†/ì‘ì€ ê°ì²´ í•„í„°ë§: ì‚¬ëŒ ë°•ìŠ¤ì™€ ê²¹ì¹˜ëŠ” ì‘ì€ ê°ì²´ëŠ” ë¬´ì‹œ
                        obj_area = (x2_obj - x1_obj) * (y2_obj - y1_obj)
                        obj_center_x = (x1_obj + x2_obj) / 2
                        obj_center_y = (y1_obj + y2_obj) / 2

                        # ì‚¬ëŒ ë°•ìŠ¤ì™€ì˜ IOU í™•ì¸ ë° í•„í„°ë§ (ìµœì í™”: ê±°ë¦¬ ê¸°ë°˜ í•„í„°ë§ ë¨¼ì €)
                        should_filter = False
                        for person_box in person_boxes_for_filter:
                            px1, py1, px2, py2 = person_box
                            person_area = (px2 - px1) * (py2 - py1)
                            
                            # ê±°ë¦¬ ê¸°ë°˜ í•„í„°ë§ ë¨¼ì € ìˆ˜í–‰ (IoU ê³„ì‚°ë³´ë‹¤ ë¹ ë¦„)
                            person_center_x = (px1 + px2) / 2
                            person_center_y = (py1 + py2) / 2
                            center_distance = ((obj_center_x - person_center_x) ** 2 + (obj_center_y - person_center_y) ** 2) ** 0.5
                            person_diagonal = ((px2 - px1) ** 2 + (py2 - py1) ** 2) ** 0.5
                            
                            # ê±°ë¦¬ê°€ ë„ˆë¬´ ë©€ë©´ IoU ê³„ì‚° ìƒëµ (ì„±ëŠ¥ í–¥ìƒ)
                            if center_distance > person_diagonal * 1.5:
                                continue

                            # ì‘ì€ ê°ì²´ê°€ ì‚¬ëŒ ë°•ìŠ¤ ë‚´ë¶€ë‚˜ ê°€ê¹Œì´ ìˆìœ¼ë©´ í•„í„°ë§
                            if (px1 <= obj_center_x <= px2 and py1 <= obj_center_y <= py2) or \
                               (x1_obj < px2 and x2_obj > px1 and y1_obj < py2 and y2_obj > py1):
                                # IOU ê³„ì‚° (ê±°ë¦¬ í•„í„°ë§ í†µê³¼í•œ ê²½ìš°ë§Œ)
                                iou = utils.calculate_iou((px1, py1, px2, py2), (x1_obj, y1_obj, x2_obj, y2_obj))

                                # ì‘ì€ ê°ì²´(machinery, hand ë“±)ì´ê³  ì‚¬ëŒ ë°•ìŠ¤ì™€ ê²¹ì¹˜ë©´ í•„í„°ë§
                                # ë˜ëŠ” ê°ì²´ê°€ ì‚¬ëŒ ë°•ìŠ¤ ë©´ì ì˜ 10% ë¯¸ë§Œì´ê³  IOUê°€ 0.1 ì´ìƒì´ë©´ í•„í„°ë§
                                if obj_area < person_area * 0.1 and iou > 0.05:
                                    should_filter = True
                                    break

                        # machinery í´ë˜ìŠ¤ëŠ” íŠ¹íˆ ì—„ê²©í•˜ê²Œ í•„í„°ë§ (ì‚¬ëŒ ë°•ìŠ¤ì™€ ê²¹ì¹˜ë©´ ë¬´ì‹œ)
                        if class_name.lower() in ['machinery', 'hand', 'hands'] and should_filter:
                            logging.debug(f"ì‘ì€ ê°ì²´ í•„í„°ë§: {class_name} (ì‚¬ëŒ ë°•ìŠ¤ì™€ ê²¹ì¹¨)")
                            continue

                        # ë Œë”ë§ì´ í•„ìš”í•œ ì‹œì ì— í”„ë ˆì„ ë³µì‚¬ (ë©”ëª¨ë¦¬ ìµœì í™”)
                        if processed_frame is None:
                            processed_frame = frame.copy()
                        
                        # ì›ë³¸ í”„ë ˆì„ì— ì§ì ‘ ê·¸ë¦¬ê¸° (í˜„ëŒ€ì  ìŠ¤íƒ€ì¼, ì´ë¯¸ ìŠ¤ì¼€ì¼ë§ëœ ì¢Œí‘œ)
                        draw_modern_bbox(processed_frame, x1_obj, y1_obj, x2_obj, y2_obj, color, thickness=1, corner_length=15, alpha=0.15)
                        display_name = class_name[:10]
                        renderer.add_text(f"{display_name}", (x1_obj, y1_obj - 5), color)

        # ìŠ¤í‚µ/ëˆ„ë½ ìƒí™©ì—ì„œ ë°•ìŠ¤/ë¼ë²¨ ìœ ì§€: ìºì‹œë¡œ ë³´ê°• (ê°•í™” ë²„ì „)
        # ë Œë”ë§ ì „ì— recognized_facesê°€ ë¹„ì–´ìˆìœ¼ë©´ ìºì‹œì—ì„œ ê°•ì œë¡œ ê°€ì ¸ì™€ì„œ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        try:
            hold_sec = config.Thresholds.RECOGNITION_HOLD_SECONDS
            now_ts = time.time()
            
            # TTLCacheì—ì„œ ìµœê·¼ í•­ëª© ê°€ì ¸ì˜¤ê¸° (ìë™ ë§Œë£Œ ì²˜ë¦¬)
            cache = recent_identity_cache.get(cam_id)
            cache_entries = []
            if cache is not None:
                # TTLCacheëŠ” ë”•ì…”ë„ˆë¦¬ì²˜ëŸ¼ ì‚¬ìš© ê°€ëŠ¥, ëª¨ë“  í•­ëª© ìˆœíšŒ
                for tracker_id_key, entry in cache.items():
                    # ë§Œë£Œëœ í•­ëª©ì€ ìë™ìœ¼ë¡œ ì œê±°ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìœ íš¨í•œ í•­ëª©ë§Œ ì²˜ë¦¬
                    entry_ts = entry.get('ts', 0)
                    age = now_ts - entry_ts
                    if age <= hold_sec:
                        # tracker_idì™€ í•¨ê»˜ ì €ì¥
                        entry_with_tracker = entry.copy()
                        entry_with_tracker['tracker_id'] = tracker_id_key
                        cache_entries.append(entry_with_tracker)
            
            if cache_entries:
                # recognized_facesê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¶€ì¡±í•˜ë©´ ìºì‹œì—ì„œ ë³´ê°• (ë‹¨, ìµœê·¼ í•­ëª©ë§Œ)
                preserved = []
                
                for entry in cache_entries:
                    age = now_ts - entry.get('ts', 0)
                    if age <= hold_sec:  # í™€ë“œ ì‹œê°„ê¹Œì§€ë§Œ ìœ ì§€ (ì”ìƒ ë°©ì§€)
                        x1, y1, x2, y2 = entry.get('box', (0,0,0,0))
                        name = entry.get('name', 'Unknown')
                        score = float(entry.get('score', 0.0))
                        
                        # Unknownì´ ì•„ë‹ˆê³  ìœ íš¨í•œ ë°•ìŠ¤ë©´ ì¶”ê°€
                        if name != "Unknown" and (x2 > x1 and y2 > y1):
                            # ê¸°ì¡´ recognized_facesì— ê°™ì€ ë°•ìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
                            # ê²¹ì¹˜ëŠ” ì‚¬ëŒ êµ¬ë¶„ì„ ìœ„í•´ IoU ì„ê³„ê°’ ìƒí–¥
                            is_duplicate = False
                            for existing in recognized_faces:
                                ex_box = existing.get("box", [])
                                if len(ex_box) == 4:
                                    ex_iou = utils.calculate_iou((x1, y1, x2, y2), tuple(ex_box))
                                    if ex_iou > 0.5:  # IoU 0.5 ì´ìƒì´ë©´ ì¤‘ë³µ (ê²¹ì¹˜ëŠ” ì‚¬ëŒ êµ¬ë¶„)
                                        is_duplicate = True
                                        break
                            
                            if not is_duplicate:
                                # ìºì‹œì—ì„œ ê°€ì ¸ì˜¨ í•­ëª©ì€ VIOLATION ìƒíƒœë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ì§€ ì•ŠìŒ
                                # SAFE ë°”ìš´ë”© ë°•ìŠ¤ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ (ì‚¬ìš©ì ìš”ì²­)
                                # preservedì—ë§Œ ì¶”ê°€í•˜ê³  ë Œë”ë§ì€ í•˜ì§€ ì•ŠìŒ
                                preserved.append({
                                    "box": [int(x1), int(y1), int(x2), int(y2)],
                                    "name": name,
                                    "similarity": score
                                })
                                # ì´ˆë¡ìƒ‰ ë°•ìŠ¤ ì œê±°: VIOLATION ìƒíƒœë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” ìºì‹œ í•­ëª©ì€ ë Œë”ë§í•˜ì§€ ì•ŠìŒ
                
                # ë³´ê°•ëœ í•­ëª©ì„ recognized_facesì— ì¶”ê°€
                if preserved:
                    recognized_faces.extend(preserved)

            # ë Œë”ë§ ìºì‹œ ë³´ê°• ì œê±°: SAFE ë°”ìš´ë”© ë°•ìŠ¤ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ (ì‚¬ìš©ì ìš”ì²­)
            # VIOLATION ìƒíƒœë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” ìºì‹œ í•­ëª©ì€ ë Œë”ë§í•˜ì§€ ì•ŠìŒ
        except Exception as e:
            logging.debug(f"ìºì‹œ ë³´ê°• ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

        # ì¤‘ë³µ ì œê±°: ë°•ìŠ¤ ê¸°ë°˜ìœ¼ë¡œë§Œ ì¤‘ë³µ ì œê±° (ê°™ì€ person_boxë¥¼ ê°€ì§„ í•­ëª©ë§Œ ì œê±°)
        # ì¤‘ìš”: ê° ì‚¬ëŒë§ˆë‹¤ ë³„ë„ì˜ í•­ëª©ì„ ìœ ì§€í•´ì•¼ í•¨ (ê°™ì€ ì´ë¦„ì´ì–´ë„ ë‹¤ë¥¸ person_boxë¥¼ ê°€ì§€ë©´ ë‹¤ë¥¸ ì‚¬ëŒ)
        # ë°•ìŠ¤ ê¸°ë°˜ ì¤‘ë³µ ì œê±°ëŠ” ì´ë¯¸ ìœ„ì—ì„œ ìˆ˜í–‰í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶”ê°€ ì¤‘ë³µ ì œê±°í•˜ì§€ ì•ŠìŒ
        try:
            if recognized_faces:
                # ë°•ìŠ¤ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (IoU ê¸°ë°˜)
                unique_faces = []
                seen_boxes = set()
                
                for face in recognized_faces:
                    box = face.get("box", [])
                    if len(box) == 4:
                        box_tuple = tuple(box)
                        # ê°™ì€ ë°•ìŠ¤ë¥¼ ê°€ì§„ í•­ëª©ì´ ì´ë¯¸ ìˆìœ¼ë©´ IoUë¡œ í™•ì¸
                        is_duplicate = False
                        for seen_box in seen_boxes:
                            if len(seen_box) == 4:
                                iou = utils.calculate_iou(box_tuple, seen_box)
                                if iou > 0.7:  # IoU 0.7 ì´ìƒì´ë©´ ê°™ì€ ì‚¬ëŒìœ¼ë¡œ ê°„ì£¼ (0.98 -> 0.7ë¡œ ì™„í™”í•˜ì—¬ ì¤‘ë³µ ì œê±° ê°•í™”)
                                    is_duplicate = True
                                    break
                        
                        if not is_duplicate:
                            unique_faces.append(face)
                            seen_boxes.add(box_tuple)
                    else:
                        # ë°•ìŠ¤ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€
                        unique_faces.append(face)
                
                recognized_faces = unique_faces
                
                # ë””ë²„ê¹…: recognized_faces ê°œìˆ˜ í™•ì¸
                logging.debug(f"[CAM-{cam_id}] ìµœì¢… recognized_faces: {len(recognized_faces)}ê°œ")
        except Exception as e:
            logging.debug(f"ì¤‘ë³µ ì œê±° ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

        # ë°”ìš´ë”© ë°•ìŠ¤ ìŠ¤ë¬´ë”© ì œê±°: ì”ìƒ ë°©ì§€ë¥¼ ìœ„í•´ ì´ì „ í”„ë ˆì„ ê²°ê³¼ë¥¼ í˜„ì¬ í”„ë ˆì„ì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        # ì¢Œí‘œ ìŠ¤ë¬´ë”©ë§Œ ìœ ì§€ (ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë¨)
        # ì´ì „ í”„ë ˆì„ì˜ ë°•ìŠ¤ë¥¼ í˜„ì¬ í”„ë ˆì„ì— ì¶”ê°€í•˜ëŠ” ë¡œì§ì€ ì”ìƒì„ ìœ ë°œí•˜ë¯€ë¡œ ì œê±°

        # 8. ë Œë”ë§ (í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´)
        rendering_start = time.time()
        # ë Œë”ë§ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ í”„ë ˆì„ ë³µì‚¬ (ë©”ëª¨ë¦¬ ìµœì í™”)
        if processed_frame is None:
            processed_frame = frame.copy()
        processed_frame = renderer.render_on(processed_frame)

        # ì´ë²ˆ í”„ë ˆì„ ë Œë”ë§ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥ (ë‹¤ìŒ í”„ë ˆì„ ë³´ê°•ìš©)
        try:
            if recognized_faces:
                # TTLCacheì— ì €ì¥ (ìë™ ë§Œë£Œ ì²˜ë¦¬)
                last_render_cache[cam_id].put('render', {
                    'items': [{'box': tuple(face.get('box', (0,0,0,0))), 'name': face.get('name', 'Unknown'), 'similarity': face.get('similarity', 0.0)} for face in recognized_faces]
                })
        except Exception:
            pass
        perf_timings['rendering'] = (time.time() - rendering_start) * 1000  # ms

        # 9. ì²˜ë¦¬ëœ í”„ë ˆì„ì„ JPEG ë°”ì´íŠ¸ë¡œ ì¸ì½”ë”© (í”„ë¡œë•ì…˜ ìµœì í™”)
        # ìŠ¤íŠ¸ë¦¬ë° ì§€ì—° í•´ê²°: ì´ë¯¸ì§€ í¬ê¸° ë¦¬ì‚¬ì´ì¦ˆ (FHD -> HDê¸‰ìœ¼ë¡œ ì¶•ì†Œí•˜ì—¬ ì „ì†¡ëŸ‰ 50% ì ˆê°)
        # ì›ë³¸ ë¶„ì„ì€ ê³ í•´ìƒë„ë¡œ í–ˆìœ¼ë¯€ë¡œ ì •í™•ë„ëŠ” ìœ ì§€ë¨
        stream_width = 1280  # 1280px (HD) ì •ë„ë©´ ì¶©ë¶„íˆ ì„ ëª…í•¨
        processed_frame_resized = processed_frame
        # ë¦¬ì‚¬ì´ì¦ˆ ìµœì í™”: í•„ìš”í•  ë•Œë§Œ ë¦¬ì‚¬ì´ì¦ˆ (í”„ë ˆì„ í¬ê¸°ê°€ ê°™ìœ¼ë©´ ìŠ¤í‚µ)
        if processed_frame.shape[1] > stream_width:
            try:
                aspect_ratio = processed_frame.shape[0] / processed_frame.shape[1]
                stream_height = int(stream_width * aspect_ratio)
                # ë¹ ë¥¸ ë¦¬ì‚¬ì´ì¦ˆ (INTER_LINEARê°€ ì†ë„ì™€ í’ˆì§ˆì˜ ê· í˜•)
                processed_frame_resized = cv2.resize(processed_frame, (stream_width, stream_height), interpolation=cv2.INTER_LINEAR)
            except Exception:
                processed_frame_resized = processed_frame

        encoding_start = time.time()
        # í”„ë¡œë•ì…˜ í’ˆì§ˆ ì¡°ì •: 95 (ê³ í™”ì§ˆ)
        ret, buffer = cv2.imencode('.jpg', processed_frame_resized, [cv2.IMWRITE_JPEG_QUALITY, 95])
        perf_timings['encoding'] = (time.time() - encoding_start) * 1000  # ms
        if not ret:
            logging.error("JPEG ì¸ì½”ë”© ì‹¤íŒ¨")
            empty_frame = np.zeros((orig_h, orig_w, 3), dtype=np.uint8)
            _, buffer = cv2.imencode('.jpg', empty_frame)
            return buffer.tobytes(), {"timestamp": time.time(), "recognized_faces": [], "violations": [], "violation_count": 0}

        processed_frame_bytes = buffer.tobytes()

        # í”„ë ˆì„ ë³´ì¥ ë°©ì‹: best_resultëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì°¾ì•˜ê³ , ë Œë”ë§ë„ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ
        # ì—¬ê¸°ì„œëŠ” ì¸ì½”ë”©ëœ í”„ë ˆì„ë§Œ ë°˜í™˜

        # ì „ì²´ ì‹œê°„ ê³„ì‚°
        perf_timings['total'] = (time.time() - total_start) * 1000  # ms
        
        # ì„±ëŠ¥ ë°ì´í„° ë¡œê¹… (ì£¼ê¸°ì ìœ¼ë¡œë§Œ ì¶œë ¥ - ì„±ëŠ¥ ìµœì í™”)
        if not hasattr(process_single_frame, '_perf_log_count'):
            process_single_frame._perf_log_count = {}
        if cam_id not in process_single_frame._perf_log_count:
            process_single_frame._perf_log_count[cam_id] = 0
        process_single_frame._perf_log_count[cam_id] += 1
        
        # FPS í–¥ìƒì„ ìœ„í•´ ì„±ëŠ¥ ë¡œê¹… ë¹ˆë„ ê°ì†Œ (ë§¤ í”„ë ˆì„ -> 10í”„ë ˆì„ë§ˆë‹¤)
        if process_single_frame._perf_log_count[cam_id] % 10 == 0:
            # ê°œë³„ ëª¨ë¸ ì²˜ë¦¬ ì‹œê°„ í¬í•¨ (GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ìš©)
            perf_msg = f"[PERF CAM-{cam_id}] ì´ ì²˜ë¦¬: {perf_timings['total']:.1f}ms | "
            perf_msg += f"Decode={perf_timings['decode']:.1f}ms | "
            perf_msg += f"Resize={perf_timings['resize']:.1f}ms | "
            if 'yolo_violation_actual' in perf_timings:
                perf_msg += f"Violation={perf_timings['yolo_violation_actual']:.1f}ms | "
            else:
                perf_msg += f"Violation={perf_timings['yolo_violation']:.1f}ms | "
            if 'yolo_pose_actual' in perf_timings:
                perf_msg += f"Pose={perf_timings['yolo_pose_actual']:.1f}ms | "
            else:
                perf_msg += f"Pose={perf_timings['yolo_pose']:.1f}ms | "
            if 'yolo_face_actual' in perf_timings:
                perf_msg += f"Face={perf_timings['yolo_face_actual']:.1f}ms | "
            perf_msg += f"Parse={perf_timings.get('parse_results', 0):.1f}ms | "
            perf_msg += f"ì–¼êµ´ì¸ì‹={perf_timings.get('face_recognition', 0):.1f}ms | "
            perf_msg += f"Render={perf_timings.get('rendering', 0):.1f}ms | "
            perf_msg += f"Encode={perf_timings.get('encoding', 0):.1f}ms"
            logging.info(perf_msg)  # debug -> info (ë³‘ëª© ë¶„ì„ìš©)
            
            # GPU ì‚¬ìš© ì—¬ë¶€ ì¶”ì • (ì²˜ë¦¬ ì‹œê°„ ê¸°ì¤€)
            if 'yolo_violation_actual' in perf_timings and 'yolo_pose_actual' in perf_timings:
                v_time = perf_timings['yolo_violation_actual']
                p_time = perf_timings['yolo_pose_actual']
                if v_time < 50 and p_time < 50:
                    logging.info(f"[PERF CAM-{cam_id}] âœ… GPU ì‚¬ìš© ì¶”ì •: Violation={v_time:.1f}ms, Pose={p_time:.1f}ms (GPU ì†ë„ ë²”ìœ„)")
                elif v_time > 150 or p_time > 150:
                    logging.warning(f"[PERF CAM-{cam_id}] âš ï¸ CPU ì‚¬ìš© ê°€ëŠ¥ì„±: Violation={v_time:.1f}ms, Pose={p_time:.1f}ms (CPU ì†ë„ ë²”ìœ„)")
            
            # ë³‘ëª© ì§€ì  ì‹ë³„ (ë§¤ í”„ë ˆì„ë§ˆë‹¤, ìƒìœ„ 3ê°œ í‘œì‹œ)
            if perf_timings['total'] > 0:
                bottlenecks = []
                for key, value in perf_timings.items():
                    if key != 'total' and value > 0:
                        percentage = (value / perf_timings['total']) * 100
                        if percentage > 10:  # 10% ì´ìƒ ì°¨ì§€í•˜ëŠ” ë‹¨ê³„ë§Œ í‘œì‹œ
                            bottlenecks.append((key, value, percentage))
                
                # ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
                bottlenecks.sort(key=lambda x: x[1], reverse=True)
                if bottlenecks:
                    bottleneck_msg = f"[BOTTLENECK CAM-{cam_id}] "
                    for i, (key, value, pct) in enumerate(bottlenecks[:3]):  # ìƒìœ„ 3ê°œë§Œ
                        bottleneck_msg += f"{key}={value:.1f}ms({pct:.0f}%) "
                    logging.warning(bottleneck_msg)

        # 10. ê²°ê³¼ ë°ì´í„° êµ¬ì„±
        # ë¡œê¹… ìµœì†Œí™” (ì„±ëŠ¥ ìµœì í™”)
        faces_count = len(recognized_faces)
        violations_count = len(violations_found)
        
        # PPE ìœ„ë°˜ì´ ìˆëŠ” ì‚¬ëŒ ìˆ˜ í™•ì¸ (ë””ë²„ê¹…ìš©)
        ppe_violation_count = sum(1 for face in recognized_faces if face.get('ppe_violations', []))
        
        if faces_count > 0 or violations_count > 0:
            logging.debug(f"[CAM-{cam_id}] AI ê²°ê³¼: ì–¼êµ´={faces_count}ê°œ, ìœ„ë°˜={violations_count}ê°œ, PPEìœ„ë°˜ ìˆëŠ” ì‚¬ëŒ={ppe_violation_count}ê°œ")
        
        result_data = {
            "timestamp": time.time(),  # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ (WebSocket ì „ì†¡ìš©)
            "recognized_faces": recognized_faces,
            "violations": violations_found,
            "violation_count": len(violations_found),
            "performance": perf_timings,  # ì„±ëŠ¥ ì¸¡ì • ë°ì´í„° í¬í•¨
            "frame_width": orig_w,  # ì›ë³¸ í”„ë ˆì„ ë„ˆë¹„ (ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ê¸°ì¤€)
            "frame_height": orig_h,  # ì›ë³¸ í”„ë ˆì„ ë†’ì´ (ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ê¸°ì¤€)
            "cam_id": cam_id  # ì¹´ë©”ë¼ ID ì¶”ê°€ (í”„ë¡ íŠ¸ì—”ë“œ ë””ë²„ê¹…ìš©)
        }

        total_elapsed = (time.time() - total_start) * 1000
        # ì‹¤ì œ FPSëŠ” camera_workerì—ì„œ ê³„ì‚°í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ì²˜ë¦¬ ì‹œê°„ë§Œ ë¡œê¹…
        logging.debug(f"[CAM-{cam_id}] ì²˜ë¦¬ ì™„ë£Œ: ì–¼êµ´={faces_count}ê°œ, ìœ„ë°˜={violations_count}ê°œ, ì²˜ë¦¬ì‹œê°„={total_elapsed:.1f}ms")
        
        # ì£¼ê¸°ì  GPU/ì›Œì»¤ ìƒíƒœ ë¡œê¹… (10ì´ˆë§ˆë‹¤)
        current_time = time.time()
        if not hasattr(process_single_frame, '_last_status_log_time'):
            process_single_frame._last_status_log_time = {}
        if cam_id not in process_single_frame._last_status_log_time:
            process_single_frame._last_status_log_time[cam_id] = current_time
        
        if current_time - process_single_frame._last_status_log_time[cam_id] >= 10.0:  # 10ì´ˆë§ˆë‹¤
            process_single_frame._last_status_log_time[cam_id] = current_time
            
            # GPU ì‚¬ìš©ë¥  í™•ì¸
            gpu_stats = {}
            if torch.cuda.is_available():
                try:
                    for gpu_id in range(torch.cuda.device_count()):
                        props = torch.cuda.get_device_properties(gpu_id)
                        memory_allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3  # GB
                        memory_reserved = torch.cuda.memory_reserved(gpu_id) / 1024**3  # GB
                        memory_total = props.total_memory / 1024**3  # GB
                        memory_util = (memory_reserved / memory_total) * 100 if memory_total > 0 else 0
                        
                        gpu_stats[gpu_id] = {
                            "name": props.name,
                            "memory_allocated_gb": memory_allocated,
                            "memory_reserved_gb": memory_reserved,
                            "memory_total_gb": memory_total,
                            "memory_util_percent": memory_util
                        }
                except Exception as e:
                    logging.debug(f"GPU í†µê³„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            
            # ì›Œì»¤ ìƒíƒœ í™•ì¸
            try:
                # ì›Œì»¤ ìˆ˜ (ìµœëŒ€ ì›Œì»¤ ìˆ˜)
                face_max_workers = face_recognition_executor._max_workers
                yolo_max_workers = yolo_executor._max_workers
                danger_max_workers = dangerous_behavior_executor._max_workers
                frame_max_workers = frame_processing_executor._max_workers
                
                # Executorì˜ ë‚´ë¶€ í í¬ê¸° ì¶”ì • (ì•ˆì „í•œ ë°©ë²•)
                face_queue_size = 0
                yolo_queue_size = 0
                try:
                    # ThreadPoolExecutorì˜ ë‚´ë¶€ íëŠ” ì§ì ‘ ì ‘ê·¼ ë¶ˆê°€, ëŒ€ì‹  í†µê³„ë¡œ ì¶”ì •
                    # _work_queueëŠ” ë‚´ë¶€ êµ¬í˜„ì´ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                    if hasattr(face_recognition_executor, '_work_queue'):
                        try:
                            face_queue_size = face_recognition_executor._work_queue.qsize()
                        except:
                            pass
                    if hasattr(yolo_executor, '_work_queue'):
                        try:
                            yolo_queue_size = yolo_executor._work_queue.qsize()
                        except:
                            pass
                except:
                    pass
                
                # í”„ë ˆì„ í í¬ê¸°
                frame_queue_size = 0
                try:
                    if cam_id in state.frame_queues:
                        frame_queue_size = state.frame_queues[cam_id].qsize()
                except:
                    pass
                
                # FPS ê³„ì‚°
                with frame_stats_lock:
                    cam_stat = frame_stats.get(cam_id, {})
                    recent_frames = cam_stat.get('recent_frame_times', [])
                    if len(recent_frames) >= 2:
                        time_span = recent_frames[-1] - recent_frames[0]
                        current_fps = (len(recent_frames) - 1) / time_span if time_span > 0 else 0
                    else:
                        current_fps = 0
                
                # í‰ê·  ì²˜ë¦¬ ì‹œê°„
                avg_processing_time = total_elapsed
                if 'processing_times' in cam_stat:
                    times = cam_stat['processing_times']
                    if len(times) > 0:
                        avg_processing_time = sum(times) / len(times)
                
                # ìƒíƒœ ë¡œê¹…
                logging.info(f"ğŸ“Š [CAM-{cam_id}] ì‹œìŠ¤í…œ ìƒíƒœ (10ì´ˆ ì£¼ê¸°):")
                logging.info(f"   FPS: {current_fps:.1f} | í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_processing_time:.1f}ms")
                logging.info(f"   ì›Œì»¤: Face={face_max_workers}, YOLO={yolo_max_workers}, Danger={danger_max_workers}, Frame={frame_max_workers}")
                logging.info(f"   í: Face={face_queue_size}, YOLO={yolo_queue_size}, Frame={frame_queue_size}")
                
                if gpu_stats:
                    for gpu_id, stat in gpu_stats.items():
                        logging.info(f"   GPU {gpu_id} ({stat['name']}): ë©”ëª¨ë¦¬={stat['memory_reserved_gb']:.2f}GB/{stat['memory_total_gb']:.2f}GB ({stat['memory_util_percent']:.1f}%)")
                        logging.warning(f"   âš ï¸ ì°¸ê³ : TensorRTëŠ” PyTorch CUDA ë©”ëª¨ë¦¬ì™€ ë³„ë„ë¡œ ì‘ë™í•©ë‹ˆë‹¤. ì‹¤ì œ GPU ì‚¬ìš©ë¥ ì€ nvidia-smië¡œ í™•ì¸í•˜ì„¸ìš”.")
            except Exception as e:
                logging.debug(f"ì›Œì»¤ ìƒíƒœ ë¡œê¹… ì˜¤ë¥˜: {e}")
        
        # ë Œë”ë§ëœ í”„ë ˆì„ ìºì‹œ ì €ì¥ (ë‹¤ìŒ ìŠ¤í‚µ í”„ë ˆì„ì—ì„œ ì¬ì‚¬ìš©í•˜ì—¬ ë°”ìš´ë”© ë°•ìŠ¤ ìœ ì§€)
        _last_rendered_frames[cam_id] = (processed_frame_bytes, result_data)
        
        return processed_frame_bytes, result_data

    except Exception as e:
        total_failed = (time.time() - total_start) * 1000
        error_msg = str(e)
        logging.error(f"AI ì²˜ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ (CAM-{cam_id}, ëˆ„ì  {total_failed:.2f}ms): {e}", exc_info=True)
        
        # ì˜¤ë¥˜ í”„ë ˆì„ ìƒì„± (ë” ìì„¸í•œ ì •ë³´ í¬í•¨)
        error_frame = frame if frame is not None else np.zeros((orig_h, orig_w, 3), dtype=np.uint8)
        
        # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ì—¬ëŸ¬ ì¤„ë¡œ í‘œì‹œ
        error_lines = [
            "Processing Error",
            error_msg[:50] + ("..." if len(error_msg) > 50 else ""),
            "Check backend logs"
        ]
        
        y_offset = 30
        for i, line in enumerate(error_lines):
            cv2.putText(error_frame, line, (10, y_offset + i * 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        ret, buffer = cv2.imencode('.jpg', error_frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
        return buffer.tobytes(), {"error": error_msg, "recognized_faces": [], "violations": []}