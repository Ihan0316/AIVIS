# core.py - SafetySystem í´ë˜ìŠ¤ (ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬)
import os
import sys
import cv2
import torch
import logging
import numpy as np
import platform
from collections import defaultdict
from typing import List, Dict, Tuple, Optional, Any
from ultralytics import YOLO
from ultralytics.engine.results import Keypoints

# NOTE: conda í™˜ê²½ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ìˆ˜ë™ ê²½ë¡œ ì¶”ê°€ ë¶ˆí•„ìš” (ì¶©ëŒ ë°©ì§€)
# conda activate aivis-gpu í›„ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ site-packagesê°€ ì„¤ì •ë¨

# onnxruntimeì„ ë¨¼ì € import (InsightFaceë³´ë‹¤ ë¨¼ì € ë¡œë“œí•˜ì—¬ DLL ì¶©ëŒ ë°©ì§€)
ONNXRUNTIME_AVAILABLE = False
try:
    import onnxruntime
    ONNXRUNTIME_AVAILABLE = True
    logging.info(f"âœ… onnxruntime {onnxruntime.__version__} import ì„±ê³µ")
    logging.info(f"   Providers: {onnxruntime.get_available_providers()}")
except ImportError as e:
    logging.warning(f"âš ï¸ onnxruntime import ì‹¤íŒ¨: {e}")
except Exception as e:
    logging.warning(f"âš ï¸ onnxruntime import ì¤‘ ì˜ˆì™¸: {type(e).__name__}: {e}")

# faiss ì„í¬íŠ¸ (conda ê²½ë¡œ ì¶”ê°€ í›„)
try:
    import faiss
except ImportError:
    faiss = None
    logging.warning("âš ï¸ FAISSë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì–¼êµ´ ì¸ì‹ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# InsightFaceëŠ” ì„ íƒì  (ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ ì–¼êµ´ ì¸ì‹ ê¸°ëŠ¥ ë¹„í™œì„±í™”)
# ì£¼ì˜: onnxruntimeì´ ë¨¼ì € ë¡œë“œë˜ì–´ì•¼ InsightFaceê°€ ì •ìƒ ì‘ë™í•  ìˆ˜ ìˆìŒ
try:
    print("[DEBUG] InsightFace import ì‹œë„...")
    from insightface.app import FaceAnalysis
    INSIGHTFACE_AVAILABLE = True
    print("[DEBUG] âœ… InsightFace FaceAnalysis import ì„±ê³µ")
    logging.info("âœ… InsightFace FaceAnalysis import ì„±ê³µ")
except ImportError as e:
    INSIGHTFACE_AVAILABLE = False
    FaceAnalysis = None
    print(f"[DEBUG] âŒ InsightFace ImportError: {e}")
    logging.warning(f"insightface ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ImportError: {e}). ì–¼êµ´ ì¸ì‹ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    logging.warning("ì„¤ì¹˜ ë°©ë²•: .\\install_insightface.bat")
except Exception as e:
    INSIGHTFACE_AVAILABLE = False
    FaceAnalysis = None
    print(f"[DEBUG] âŒ InsightFace Exception: {type(e).__name__}: {e}")
    import traceback
    traceback.print_exc()
    logging.error(f"InsightFace import ì¤‘ ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e}")
    logging.warning("ì„¤ì¹˜ ë°©ë²•: .\\install_insightface.bat")

# TensorRTëŠ” ì„ íƒì  (ì—”ì§„ íŒŒì¼ ë¡œë“œ ì‹œ í•„ìš”)
# conda ê²½ë¡œ ì¶”ê°€ í›„ì—ë„ TensorRTë¥¼ ì°¾ì§€ ëª»í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì¶”ê°€ ê²½ë¡œ í™•ì¸
TENSORRT_AVAILABLE = False
try:
    import tensorrt
    TENSORRT_AVAILABLE = True
    logging.info("âœ… TensorRT Python íŒ¨í‚¤ì§€ ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    # conda í™˜ê²½ì˜ TensorRT í™•ì¸ (sys.pathì— ì¶”ê°€ë˜ê¸° ì „ì¼ ìˆ˜ ìˆìŒ)
    try:
        # conda ê²½ë¡œì—ì„œ ì§ì ‘ í™•ì¸
        conda_site_packages = os.path.join(os.environ.get('USERPROFILE', ''), 'anaconda3', 'Lib', 'site-packages')
        if os.path.exists(conda_site_packages):
            tensorrt_path = os.path.join(conda_site_packages, 'tensorrt')
            if os.path.exists(tensorrt_path):
                # conda ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€í•˜ê³  ì¬ì‹œë„
                if conda_site_packages not in sys.path:
                    sys.path.insert(0, conda_site_packages)
                import tensorrt
                TENSORRT_AVAILABLE = True
                logging.info("âœ… TensorRT Python íŒ¨í‚¤ì§€ ì‚¬ìš© ê°€ëŠ¥ (conda í™˜ê²½ì—ì„œ ì°¾ìŒ)")
    except Exception as e:
        TENSORRT_AVAILABLE = False
    
    if not TENSORRT_AVAILABLE:
        logging.info("â„¹ï¸ TensorRT Python íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. TensorRT ì—”ì§„ íŒŒì¼ ëŒ€ì‹  ONNX ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        logging.info("  TensorRTë¥¼ ì‚¬ìš©í•˜ë ¤ë©´: pip install nvidia-tensorrt (CUDAì™€ í˜¸í™˜ë˜ëŠ” ë²„ì „ í•„ìš”)")

import config
from utils import calculate_iou, clip_bbox_xyxy, is_person_horizontal, log_violation



class SafetySystem:
    def __init__(self):
        # 0. ì„±ëŠ¥ ìµœì í™” ì„¤ì • (ONNX Runtime ê¸°ë°˜)
        # ONNX Runtimeê³¼ PyTorch ëª¨ë‘ í™•ì¸
        try:
            import onnxruntime as ort
            onnx_providers = ort.get_available_providers()
            has_cuda_provider = 'CUDAExecutionProvider' in onnx_providers
        except:
            has_cuda_provider = False
            onnx_providers = []
        
        # PyTorch CUDA í™•ì¸ (í˜¸í™˜ì„±ì„ ìœ„í•´)
        pytorch_cuda_available = False
        gpu_count = 0
        try:
            if torch.cuda.is_available():
                pytorch_cuda_available = True
                gpu_count = torch.cuda.device_count()
        except:
            pass
        
        # ONNX Runtime GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if has_cuda_provider:
            logging.info(f"âœ… ONNX Runtime CUDA Provider ì‚¬ìš© ê°€ëŠ¥")
            logging.info(f"  - ì‚¬ìš© ê°€ëŠ¥í•œ Providers: {onnx_providers}")
            if pytorch_cuda_available:
                logging.info(f"  - PyTorch CUDAë„ ì‚¬ìš© ê°€ëŠ¥ (GPU ê°œìˆ˜: {gpu_count})")
                # PyTorch ìµœì í™” ì„¤ì • (í˜¸í™˜ì„±ì„ ìœ„í•´)
                torch.backends.cudnn.benchmark = True
                if gpu_count >= 2:
                    for i in range(gpu_count):
                        with torch.cuda.device(i):
                            torch.cuda.empty_cache()
                    logging.info(f"âœ… ë©€í‹° GPU ({gpu_count}ê°œ) ìµœì í™” ì„¤ì • ì™„ë£Œ")
        else:
            logging.warning("âš ï¸ ONNX Runtime CUDA Providerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            logging.info(f"  - ì‚¬ìš© ê°€ëŠ¥í•œ Providers: {onnx_providers}")
            if pytorch_cuda_available:
                logging.info(f"  - PyTorch CUDAëŠ” ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ, ONNX Runtimeì€ CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            else:
                logging.warning("  - CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

        # 1. ì¥ì¹˜ ì„¤ì • (ë©€í‹° GPU ì§€ì›)
        self.device_config = config.SystemConfig.get_device_config()
        self.device = self.device_config['device']  # GPU 0: YOLO Violation, Pose
        self.device_face = self.device_config.get('device_face', self.device)  # GPU 1: YOLO Face, InsightFace
        self.gpu_count = self.device_config.get('gpu_count', 0)
        
        # GPU ê°•ì œ ì‚¬ìš© ì‹œë„ (ONNX Runtime ê¸°ë°˜)
        # ONNX Runtime CUDA Provider í™•ì¸
        try:
            import onnxruntime as ort
            onnx_providers = ort.get_available_providers()
            has_onnx_cuda = 'CUDAExecutionProvider' in onnx_providers
        except:
            has_onnx_cuda = False
        
        if self.device == 'cpu':
            # GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ì§€ë§Œ GPU ì‚¬ìš© ì‹œë„
            try:
                # ONNX Runtime CUDA Provider í™•ì¸
                if has_onnx_cuda:
                    self.device = 'cuda:0'
                    self.device_face = 'cuda:0' if self.gpu_count < 2 else 'cuda:1'
                    logging.warning("âš ï¸ ONNX Runtime CUDA Provider ì‚¬ìš© ê°€ëŠ¥. GPUë¡œ ê°•ì œ ì „í™˜í•©ë‹ˆë‹¤.")
                elif torch.cuda.is_available():
                    # PyTorch CUDAë„ í™•ì¸ (í˜¸í™˜ì„±)
                    self.device = 'cuda:0'
                    self.device_face = 'cuda:0' if self.gpu_count < 2 else 'cuda:1'
                    logging.warning("âš ï¸ PyTorch CUDA ì‚¬ìš© ê°€ëŠ¥. GPUë¡œ ê°•ì œ ì „í™˜í•©ë‹ˆë‹¤.")
                else:
                    # CUDAê°€ ì—†ìœ¼ë©´ CPU ìœ ì§€
                    logging.warning("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                    self.device = 'cpu'
                    self.device_face = 'cpu'
            except Exception as e:
                logging.warning(f"âš ï¸ GPU ê°•ì œ ì‚¬ìš© ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}. CPU ëª¨ë“œë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                self.device = 'cpu'
                self.device_face = 'cpu'
        
        logging.info(f"SafetySystem: YOLO Violation/Pose ì¥ì¹˜: {self.device.upper()}")
        logging.info(f"SafetySystem: ì–¼êµ´ ì¸ì‹ ì¥ì¹˜: {self.device_face.upper()}")

        # 2. ëª¨ë¸ ë¡œë”©
        (self.violation_model,
         self.pose_model,
         self.violation_uses_trt,
         self.pose_uses_trt) = self._initialize_tracking_models()
        (self.face_model,
         self.face_analyzer,
         self.face_database,
         self.face_uses_trt,
         self.use_adaface,
         self.adaface_model_path) = self._initialize_face_recognition_models()
        
        # ë„˜ì–´ì§ ê°ì§€ ëª¨ë¸ ë¡œë“œ (ì„ íƒì ) - TensorRT ì—”ì§„ ìš°ì„ 
        self.fall_model = None
        self.fall_uses_trt = False
        try:
            fall_engine_path = config.Paths.YOLO_FALL_DETECTION_ENGINE
            fall_model_path = config.Paths.YOLO_FALL_DETECTION_MODEL
            
            # TensorRT ì—”ì§„ ìš°ì„  ë¡œë“œ (2ë°° ë¹ ë¦„: 24ms â†’ 12ms)
            if os.path.exists(fall_engine_path):
                logging.info(f"ğŸ” Fall TensorRT Engine ë¡œë“œ ì‹œë„: {fall_engine_path}")
                self.fall_model = YOLO(fall_engine_path, task='detect')
                self.fall_uses_trt = True
                logging.info("âœ… ë„˜ì–´ì§ ê°ì§€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (TensorRT)")
            elif os.path.exists(fall_model_path):
                logging.info(f"ğŸ” Fall Detection ëª¨ë¸ ë¡œë“œ ì‹œë„: {fall_model_path}")
                self.fall_model = YOLO(fall_model_path, task='detect')
                if torch.cuda.is_available():
                    self.fall_model.to('cuda:0')
                    logging.info("âœ… ë„˜ì–´ì§ ê°ì§€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (GPU)")
                else:
                    logging.info("âœ… ë„˜ì–´ì§ ê°ì§€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (CPU)")
            else:
                logging.info(f"â„¹ï¸ ë„˜ì–´ì§ ê°ì§€ ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
                logging.info("   (í‚¤í¬ì¸íŠ¸ ê¸°ë°˜ ë¶„ì„ë§Œ ì‚¬ìš©)")
        except Exception as e:
            logging.warning(f"âš ï¸ ë„˜ì–´ì§ ê°ì§€ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
            self.fall_model = None
        
        # FastIndustrialRecognizer ì´ˆê¸°í™” (ëœë“œë§ˆí¬ ê¸°ë°˜ ê³ ì† ì²˜ë¦¬ìš©)
        self.fast_recognizer = None
        # CUDA ë””ë°”ì´ìŠ¤ì— ë§ëŠ” ctx_id ì„¤ì •
        if 'cuda' in str(self.device_face):
            ctx_id_face = int(self.device_face.split(':')[-1]) if ':' in str(self.device_face) else 0
        else:
            ctx_id_face = -1  # CPU
        
        # AdaFace ëª¨ë¸ì´ ìˆìœ¼ë©´ InsightFace ì—†ì´ë„ FastIndustrialRecognizer ì‚¬ìš© ê°€ëŠ¥
        if self.use_adaface and self.adaface_model_path and os.path.exists(self.adaface_model_path):
            try:
                from fast_face_recognizer import FastIndustrialRecognizer
                self.fast_recognizer = FastIndustrialRecognizer(
                    model_path=self.adaface_model_path,
                    ctx_id=ctx_id_face,
                    use_adaface=True
                )
                logging.info(f"âœ… FastIndustrialRecognizer ì´ˆê¸°í™” ì™„ë£Œ (AdaFace ëª¨ë¸: {self.adaface_model_path})")
            except Exception as e:
                logging.warning(f"âš ï¸ FastIndustrialRecognizer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.fast_recognizer = None
        elif self.face_analyzer is not None:
            # AdaFaceê°€ ì—†ê³  InsightFaceê°€ ìˆìœ¼ë©´ buffalo_l ì‚¬ìš©
            try:
                from fast_face_recognizer import FastIndustrialRecognizer
                self.fast_recognizer = FastIndustrialRecognizer(
                    model_path=None,  # InsightFace ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
                    ctx_id=ctx_id_face,
                    use_adaface=False
                )
                logging.info(f"âœ… FastIndustrialRecognizer ì´ˆê¸°í™” ì™„ë£Œ (buffalo_l ëª¨ë¸, ëœë“œë§ˆí¬ ê¸°ë°˜ ì²˜ë¦¬)")
            except Exception as e:
                logging.warning(f"âš ï¸ FastIndustrialRecognizer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.fast_recognizer = None
        else:
            logging.warning("âš ï¸ FastIndustrialRecognizer ì‚¬ìš© ë¶ˆê°€: AdaFace ëª¨ë¸ ë˜ëŠ” InsightFace í•„ìš”")

        if self.violation_model is None or self.pose_model is None:
            logging.error("í•„ìˆ˜ ëª¨ë¸(Violation or Pose) ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
             logging.info("YOLO ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

        # ğŸ¦¬ buffalo_lë§Œ ì‚¬ìš©: face_modelì€ Noneì´ì–´ë„ ë¨!
        # face_analyzerì™€ face_databaseë§Œ ìˆìœ¼ë©´ ì–¼êµ´ ì¸ì‹ ê°€ëŠ¥
        if self.face_analyzer is None or self.face_database is None:
            logging.warning("=" * 80)
            logging.warning("âš ï¸  ì–¼êµ´ ì¸ì‹ ëª¨ë¸ ë˜ëŠ” DB ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            logging.warning("âš ï¸  ì–¼êµ´ ì¸ì‹ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            if not INSIGHTFACE_AVAILABLE:
                logging.warning("âš ï¸  InsightFace ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                logging.warning("âš ï¸  ì„¤ì¹˜ ë°©ë²•: .\\install_insightface.bat")
            logging.warning("=" * 80)
        else:
            logging.info("âœ… ì–¼êµ´ ì¸ì‹ ëª¨ë¸ ë° DB ë¡œë”© ì™„ë£Œ (buffalo_l + InsightFace ì„ë² ë”©).")

    def _load_yolo_variant(self, weight_path: str, engine_path: str, task_description: str, task_type: str) -> Tuple[Optional[YOLO], bool]:
        """
        YOLO ëª¨ë¸ ë¡œë“œ (TensorRT Engine ìš°ì„ , ì—†ìœ¼ë©´ ONNX ëª¨ë¸ ì‚¬ìš©)
        
        :param weight_path: ONNX ëª¨ë¸ ê²½ë¡œ (.onnx)
        :param engine_path: TensorRT ì—”ì§„ ê²½ë¡œ (.engine)
        :param task_description: ì‘ì—… ì„¤ëª… (ë¡œê¹…ìš©)
        :param task_type: ì‘ì—… íƒ€ì… ('detect', 'pose', 'segment')
        :return: (ëª¨ë¸, TensorRT ì‚¬ìš© ì—¬ë¶€) íŠœí”Œ
        """
        # 1. TensorRT Engine íŒŒì¼ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        # TensorRT Python íŒ¨í‚¤ì§€ê°€ ì—†ì–´ë„ Ultralyticsê°€ ë‚´ë¶€ì ìœ¼ë¡œ ë¡œë“œ ì‹œë„í•  ìˆ˜ ìˆìŒ
        if engine_path and os.path.exists(engine_path):
            logging.info(f"ğŸ” {task_description} ì—”ì§„ íŒŒì¼ ë°œê²¬: {engine_path}")
            try:
                logging.info(f"{task_description} TensorRT Engine ë¡œë“œ ì‹œë„: {engine_path}")
                model = YOLO(engine_path, task=task_type)
                
                # ë¡œë“œ ì„±ê³µ í™•ì¸ (ì†ì„± ì ‘ê·¼)
                _ = model.names
                
                # TensorRT ì—”ì§„ì€ GPUì—ì„œë§Œ ì‹¤í–‰ë˜ë¯€ë¡œ device ì„¤ì • ë¶ˆí•„ìš”
                logging.info(f"âœ… {task_description} TensorRT Engine ë¡œë“œ ì™„ë£Œ (GPU ìµœì í™”)")
                
                # TensorRT GPU ì‚¬ìš© í™•ì¸ (ì‹¤ì œ GPU ë©”ëª¨ë¦¬ í™•ì¸)
                try:
                    if torch.cuda.is_available():
                        gpu_id = 0 if task_description != "Face" else (1 if torch.cuda.device_count() >= 2 else 0)
                        gpu_mem = torch.cuda.memory_allocated(gpu_id) / 1024**3  # GB
                        gpu_reserved = torch.cuda.memory_reserved(gpu_id) / 1024**3  # GB
                        logging.info(f"ğŸ” {task_description} TensorRT ë¡œë“œ í›„ GPU {gpu_id} ë©”ëª¨ë¦¬: í• ë‹¹={gpu_mem:.3f}GB, ì˜ˆì•½={gpu_reserved:.3f}GB")
                        logging.warning(f"âš ï¸ ì°¸ê³ : TensorRTëŠ” PyTorch CUDA ë©”ëª¨ë¦¬ì™€ ë³„ë„ë¡œ ì‘ë™í•˜ë¯€ë¡œ, ì‹¤ì œ GPU ì‚¬ìš©ë¥ ì€ nvidia-smië¡œ í™•ì¸í•˜ì„¸ìš”.")
                except Exception as mem_e:
                    logging.debug(f"{task_description} GPU ë©”ëª¨ë¦¬ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {mem_e}")
                
                return model, True  # TensorRT ì‚¬ìš©
            except Exception as e:
                error_msg = str(e)
                logging.warning(f"âš ï¸ {task_description} TensorRT Engine ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
                logging.info(f"   {task_description} ONNX ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        else:
            logging.info(f"â„¹ï¸ {task_description} ì—”ì§„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {engine_path}")
        
        # 2. ONNX ëª¨ë¸ ë¡œë“œ (Engineì´ ì—†ê±°ë‚˜ ë¡œë“œ ì‹¤íŒ¨ ì‹œ)
        # weight_pathê°€ .onnxë¡œ ëë‚˜ì§€ ì•Šìœ¼ë©´ .onnx í™•ì¥ì ì¶”ê°€
        if not weight_path.endswith('.onnx'):
            onnx_path = os.path.splitext(weight_path)[0] + ".onnx"
        else:
            onnx_path = weight_path
        
        if not os.path.exists(onnx_path):
            raise FileNotFoundError(f"{task_description} ëª¨ë¸ íŒŒì¼ ì—†ìŒ (Engine: {engine_path}, ONNX: {onnx_path})")
        
        try:
            logging.info(f"{task_description} ONNX ëª¨ë¸ ë¡œë“œ: {onnx_path}")
            
            # GPU ì‚¬ìš©ì„ ìœ„í•œ ONNX Runtime ì„¸ì…˜ ì˜µì…˜ ì„¤ì •
            try:
                import onnxruntime as ort
                available_providers = ort.get_available_providers()
                
                # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
                if 'CUDAExecutionProvider' in available_providers:
                    # GPU ID ê²°ì •
                    if task_description == "Face":
                        gpu_id = 1 if torch.cuda.device_count() >= 2 else 0
                    else:
                        gpu_id = 0
                    
                    # ONNX Runtime í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (YOLOê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©)
                    os.environ['ORT_DEVICE'] = 'cuda'
                    os.environ['ORT_EXECUTION_PROVIDERS'] = 'CUDAExecutionProvider;CPUExecutionProvider'
                    os.environ['ORT_CUDA_DEVICE_ID'] = str(gpu_id)
                    
                    logging.info(f"{task_description} ONNX Runtime GPU ì˜µì…˜ ì„¤ì • ì™„ë£Œ (GPU {gpu_id})")
                else:
                    logging.warning(f"{task_description} ONNX Runtime: CUDA Providerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            except Exception as opt_e:
                logging.warning(f"{task_description} ONNX Runtime ì˜µì…˜ ì„¤ì • ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {opt_e}")
            
            # YOLO ONNX ëª¨ë¸ ë¡œë“œ (YOLOê°€ ë‚´ë¶€ì ìœ¼ë¡œ ONNX Runtime ì‚¬ìš©)
            model = YOLO(onnx_path, task=task_type)
            
            # YOLOê°€ ë‚´ë¶€ì ìœ¼ë¡œ deviceë¥¼ ì„¤ì •í•˜ì§€ ì•Šë„ë¡ ëª…ì‹œì ìœ¼ë¡œ device ì†ì„± ì„¤ì •
            # ONNX ëª¨ë¸ì€ ONNX Runtimeì´ deviceë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ, YOLOì˜ device ì†ì„±ì„ None ë˜ëŠ” 'cpu'ë¡œ ì„¤ì •
            try:
                # YOLO ëª¨ë¸ì˜ device ì†ì„±ì„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë‚´ë¶€ device ì„¤ì • ë°©ì§€
                if hasattr(model, 'device'):
                    # ONNX ëª¨ë¸ì€ deviceë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ Noneìœ¼ë¡œ ì„¤ì •
                    model.device = None
                elif hasattr(model, 'overrides'):
                    # YOLO v8+ ìŠ¤íƒ€ì¼: overridesì— deviceë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ì§€ ì•ŠìŒ
                    if 'device' in model.overrides:
                        del model.overrides['device']
                # YOLO ëª¨ë¸ì˜ predictor device ì„¤ì •ë„ í™•ì¸
                if hasattr(model, 'predictor') and hasattr(model.predictor, 'device'):
                    model.predictor.device = None
            except Exception as device_set_e:
                logging.debug(f"{task_description} YOLO device ì†ì„± ì„¤ì • ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {device_set_e}")
            
            logging.info(f"{task_description} ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # âœ… GPU ì‚¬ìš© ì—¬ë¶€ ë° ì…ë ¥/ì¶œë ¥ í™•ì¸
            try:
                import onnxruntime as ort
                
                # ë¨¼ì € ONNX Runtimeì˜ ì‚¬ìš© ê°€ëŠ¥í•œ Providers í™•ì¸
                available_providers = ort.get_available_providers()
                logging.info(f"ğŸ” {task_description} ONNX Runtime ì‚¬ìš© ê°€ëŠ¥í•œ Providers: {available_providers}")
                
                if 'CUDAExecutionProvider' not in available_providers:
                    logging.warning(f"âš ï¸ {task_description} ONNX Runtime: CUDAExecutionProviderê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    logging.warning(f"   GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´: pip install onnxruntime-gpu")
                    logging.warning(f"   í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ Providers: {available_providers}")
                
                # YOLO ëª¨ë¸ì˜ ë‚´ë¶€ ONNX Runtime ì„¸ì…˜ ì°¾ê¸°
                session_obj = None
                session_path = None
                
                # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ê²½ë¡œë¡œ ì„¸ì…˜ ì°¾ê¸°
                if hasattr(model, 'model'):
                    # ê²½ë¡œ 1: model.model.session
                    if hasattr(model.model, 'session'):
                        try:
                            session_obj = model.model.session
                            session_path = 'model.model.session'
                        except:
                            pass
                    
                    # ê²½ë¡œ 2: model.model.predictor.session
                    if session_obj is None and hasattr(model.model, 'predictor'):
                        try:
                            if hasattr(model.model.predictor, 'session'):
                                session_obj = model.model.predictor.session
                                session_path = 'model.model.predictor.session'
                        except:
                            pass
                    
                    # ê²½ë¡œ 3: model.model.overrides['session']
                    if session_obj is None and hasattr(model.model, 'overrides'):
                        try:
                            if isinstance(model.model.overrides, dict) and 'session' in model.model.overrides:
                                session_obj = model.model.overrides.get('session')
                                if session_obj:
                                    session_path = 'model.model.overrides["session"]'
                        except:
                            pass
                    
                    # ê²½ë¡œ 4: ì¬ê·€ì ìœ¼ë¡œ session ì†ì„± ì°¾ê¸°
                    if session_obj is None:
                        def find_session(obj, path="", depth=0):
                            if depth > 3:  # ìµœëŒ€ 3ë‹¨ê³„ ê¹Šì´
                                return None, None
                            if hasattr(obj, 'get_providers'):
                                return obj, path
                            for attr_name in dir(obj):
                                if attr_name.startswith('_') or attr_name in ['session']:
                                    continue
                                try:
                                    attr = getattr(obj, attr_name)
                                    if attr is None or isinstance(attr, (str, int, float, bool)):
                                        continue
                                    if 'session' in attr_name.lower():
                                        found_session, found_path = find_session(attr, f"{path}.{attr_name}" if path else attr_name, depth+1)
                                        if found_session:
                                            return found_session, found_path
                                except:
                                    pass
                            return None, None
                        
                        found_session, found_path = find_session(model.model, "model.model")
                        if found_session:
                            session_obj = found_session
                            session_path = found_path
                
                if session_obj:
                    # 1. GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
                    actual_providers = session_obj.get_providers()
                    logging.info(f"ğŸ” {task_description} ONNX ëª¨ë¸ ì„¸ì…˜ ì •ë³´:")
                    logging.info(f"   ì„¸ì…˜ ê²½ë¡œ: {session_path}")
                    logging.info(f"   í™œì„±í™”ëœ Providers: {actual_providers}")
                    
                    if 'CUDAExecutionProvider' in actual_providers:
                        # CUDA Providerê°€ ì²« ë²ˆì§¸ì¸ì§€ í™•ì¸ (ìš°ì„ ìˆœìœ„)
                        if actual_providers[0] == 'CUDAExecutionProvider':
                            logging.info(f"âœ… {task_description} ONNX ëª¨ë¸: GPU ì‚¬ìš© ì¤‘ (CUDAExecutionProvider ìš°ì„ ìˆœìœ„ 1)")
                        else:
                            logging.warning(f"âš ï¸ {task_description} ONNX ëª¨ë¸: CUDA ProviderëŠ” ìˆì§€ë§Œ ìš°ì„ ìˆœìœ„ê°€ ë‚®ìŒ (í˜„ì¬: {actual_providers[0]})")
                    else:
                        logging.warning(f"âš ï¸ {task_description} ONNX ëª¨ë¸: CPUë¡œ ì‹¤í–‰ ì¤‘ (CUDA Provider ì—†ìŒ)")
                    
                    # 2. ì…ë ¥/ì¶œë ¥ shape í™•ì¸
                    try:
                        inputs = session_obj.get_inputs()
                        outputs = session_obj.get_outputs()
                        
                        logging.info(f"ğŸ” {task_description} ONNX ëª¨ë¸ ì…ë ¥/ì¶œë ¥ ì •ë³´:")
                        for i, inp in enumerate(inputs):
                            logging.info(f"   ì…ë ¥[{i}]: name={inp.name}, shape={inp.shape}, type={inp.type}")
                        
                        for i, out in enumerate(outputs):
                            logging.info(f"   ì¶œë ¥[{i}]: name={out.name}, shape={out.shape}, type={out.type}")
                        
                        # ì…ë ¥ shape ê²€ì¦
                        if len(inputs) > 0:
                            input_shape = inputs[0].shape
                            if len(input_shape) == 4:  # [batch, channels, height, width]
                                expected_h = 832 if task_description != "Face" else 640
                                expected_w = 832 if task_description != "Face" else 640
                                # ë™ì  shapeì¸ ê²½ìš° (None ë˜ëŠ” -1)ëŠ” ì²´í¬í•˜ì§€ ì•ŠìŒ
                                if input_shape[2] not in [None, -1] and input_shape[3] not in [None, -1]:
                                    if input_shape[2] != expected_h or input_shape[3] != expected_w:
                                        logging.warning(f"âš ï¸ {task_description} ëª¨ë¸ ì…ë ¥ í¬ê¸° ë¶ˆì¼ì¹˜: ì˜ˆìƒ={expected_h}x{expected_w}, ì‹¤ì œ={input_shape[2]}x{input_shape[3]}")
                                    else:
                                        logging.info(f"âœ… {task_description} ëª¨ë¸ ì…ë ¥ í¬ê¸° í™•ì¸: {input_shape[2]}x{input_shape[3]}")
                    except Exception as io_e:
                        logging.warning(f"âš ï¸ {task_description} ëª¨ë¸ ì…ë ¥/ì¶œë ¥ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {io_e}")
                    
                    # 3. GPU ë©”ëª¨ë¦¬ ë³€í™” í™•ì¸ (ì‹¤ì œ GPU ì‚¬ìš© ì—¬ë¶€ ê²€ì¦)
                    if 'CUDAExecutionProvider' in actual_providers and torch.cuda.is_available():
                        try:
                            gpu_id = 0 if task_description != "Face" else (1 if torch.cuda.device_count() >= 2 else 0)
                            gpu_mem_before = torch.cuda.memory_allocated(gpu_id) / 1024**2  # MB
                            
                            # ë”ë¯¸ ì¶”ë¡  ì‹¤í–‰
                            dummy_input = np.random.randn(1, 3, 832 if task_description != "Face" else 640, 832 if task_description != "Face" else 640).astype(np.float32)
                            input_name = inputs[0].name if inputs else 'images'
                            _ = session_obj.run(None, {input_name: dummy_input})
                            
                            gpu_mem_after = torch.cuda.memory_allocated(gpu_id) / 1024**2  # MB
                            mem_increase = gpu_mem_after - gpu_mem_before
                            
                            if mem_increase > 0.1:  # 0.1MB ì´ìƒ ì¦ê°€í•˜ë©´ GPU ì‚¬ìš© ì¤‘
                                logging.info(f"âœ… {task_description} ONNX ëª¨ë¸: GPU ì‹¤ì œ ì‚¬ìš© í™•ì¸ë¨ (ë©”ëª¨ë¦¬ ì¦ê°€: {mem_increase:.2f}MB)")
                            else:
                                logging.warning(f"âš ï¸ {task_description} ONNX ëª¨ë¸: GPU ë©”ëª¨ë¦¬ ë³€í™” ì—†ìŒ ({mem_increase:.2f}MB) - CPUì—ì„œ ì‹¤í–‰ ì¤‘ì¼ ìˆ˜ ìˆìŒ")
                        except Exception as mem_check_error:
                            logging.debug(f"{task_description} GPU ë©”ëª¨ë¦¬ í™•ì¸ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {mem_check_error}")
                else:
                    logging.warning(f"âš ï¸ {task_description} ONNX ëª¨ë¸: ì„¸ì…˜ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (YOLO ë‚´ë¶€ êµ¬ì¡° í™•ì¸ í•„ìš”)")
            except Exception as check_e:
                logging.warning(f"âš ï¸ {task_description} ONNX ëª¨ë¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {check_e}")
            
            return model, False  # TensorRT ì‚¬ìš© ì•ˆ í•¨
        except Exception as e:
            logging.error(f"{task_description} ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
            return None, False
    
    @staticmethod
    def _is_onnx_model(model_path: str) -> bool:
        """ëª¨ë¸ ê²½ë¡œê°€ ONNX ëª¨ë¸ì¸ì§€ í™•ì¸"""
        if model_path.endswith('.onnx'):
            return True
        onnx_path = os.path.splitext(model_path)[0] + ".onnx"
        return os.path.exists(onnx_path)

    def _initialize_tracking_models(self) -> Tuple[Optional[YOLO], Optional[YOLO], bool, bool]:
        try:
            violation_model, violation_trt = self._load_yolo_variant(
                config.Paths.YOLO_VIOLATION_MODEL,
                config.Paths.YOLO_VIOLATION_ENGINE,
                "Violation",
                "detect"
            )
            pose_model, pose_trt = self._load_yolo_variant(
                config.Paths.YOLO_POSE_MODEL,
                config.Paths.YOLO_POSE_ENGINE,
                "Pose",
                "pose"
            )

            if violation_model is None or pose_model is None:
                raise RuntimeError("í•„ìˆ˜ YOLO ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            # CUDA ë””ë°”ì´ìŠ¤ ì‚¬ìš© (GPU ê°•ì œ ì‚¬ìš©)
            pose_device = self.device
            
            # GPUë¡œ ê°•ì œ ì´ë™ ì‹œë„
            target_device = self.device
            if target_device == 'cpu':
                # CPUë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ë„ GPU ì‚¬ìš© ì‹œë„
                if torch.cuda.is_available():
                    target_device = 'cuda:0'
                    pose_device = 'cuda:0'
                    logging.info("ğŸ”„ GPU ê°ì§€ë¨. YOLO Violation/Pose ëª¨ë¸ì„ GPUë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                else:
                    # CUDAê°€ ì‹¤ì œë¡œ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•˜ë©´ CPUë¡œ ìœ ì§€
                    target_device = 'cpu'
                    pose_device = 'cpu'
                    logging.warning("âš ï¸ CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # ONNX ëª¨ë¸ì€ YOLOê°€ ë‚´ë¶€ì ìœ¼ë¡œ ONNX Runtimeì„ ì‚¬ìš©í•˜ë¯€ë¡œ
            # .to() ë©”ì„œë“œ í˜¸ì¶œì´ í•„ìš” ì—†ê³ , í˜¸ì¶œí•˜ë©´ YOLOê°€ ë‚´ë¶€ì ìœ¼ë¡œ deviceë¥¼ ì„¤ì •í•˜ì—¬ ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥
            # ONNX Runtimeì€ ëª¨ë¸ ë¡œë“œ ì‹œ ì´ë¯¸ GPU/CPU Providerë¥¼ ì„¤ì •í–ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ì„¤ì • ë¶ˆí•„ìš”
            if not violation_trt:
                try:
                    # ONNX ëª¨ë¸ì€ .to() ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ (YOLOê°€ ë‚´ë¶€ì ìœ¼ë¡œ device ì„¤ì • ì‹œë„í•˜ì—¬ ì˜¤ë¥˜ ë°œìƒ)
                    # eval()ë§Œ í˜¸ì¶œ (ìˆì„ ê²½ìš°)
                    if hasattr(violation_model, 'eval'):
                        violation_model.eval()
                    logging.info(f"âœ… Violation ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ONNX Runtimeì´ ìë™ìœ¼ë¡œ GPU/CPU ì²˜ë¦¬)")
                except Exception as e:
                    logging.debug(f"Violation ëª¨ë¸ ì„¤ì •: {e} (ONNX ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬ë¨)")

            if not pose_trt:
                try:
                    # ONNX ëª¨ë¸ì€ .to() ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
                    if hasattr(pose_model, 'eval'):
                        pose_model.eval()
                    logging.info(f"âœ… Pose ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ONNX Runtimeì´ ìë™ìœ¼ë¡œ GPU/CPU ì²˜ë¦¬)")
                except Exception as e:
                    logging.debug(f"Pose ëª¨ë¸ ì„¤ì •: {e} (ONNX ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬ë¨)")

            if 'cuda' in str(self.device) and (not violation_trt or not pose_trt):
                if torch.cuda.is_available():
                    # GPU 0 ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    gpu_id = int(self.device.split(':')[-1]) if ':' in str(self.device) else 0
                    gpu_name = torch.cuda.get_device_name(gpu_id)
                    gpu_memory = torch.cuda.get_device_properties(gpu_id).total_memory / (1024**3)  # GB
                    logging.info(f"GPU {gpu_id} ({gpu_name}) ìµœì í™”: YOLO Violation/Pose ëª¨ë¸ ì‹¤í–‰ (ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB)")
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cudnn.deterministic = False
                    logging.info("âœ… cuDNN ìµœì í™” í™œì„±í™”")

            # ëª¨ë¸ íƒ€ì… ë¡œê¹…
            violation_device_str = str(self.device).upper()
            if 'cuda' in violation_device_str:
                violation_device_info = "CUDA GPU"
            else:
                violation_device_info = "CPU"
            
            pose_device_str = str(pose_device).upper()
            if 'cuda' in pose_device_str:
                pose_device_info = "CUDA GPU"
            else:
                pose_device_info = "CPU"
            
            logging.info(f"âœ… Violation ëª¨ë¸: PyTorch ({violation_device_info})")
            logging.info(f"âœ… Pose ëª¨ë¸: PyTorch ({pose_device_info})")

            return violation_model, pose_model, violation_trt, pose_trt
        except Exception as e:
            logging.error(f"YOLO ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
            return None, None, False, False

    def _initialize_face_recognition_models(self):
        face_model = None
        face_analyzer = None
        face_database = None
        face_uses_trt = False
        use_adaface = False
        adaface_model_path = None
        face_model_name = 'buffalo_l'  # buffalo_l ì‚¬ìš© (InsightFace)
        face_model = None  # YOLO Face ëŒ€ì‹  buffalo_l ì‚¬ìš©
        
        # â­ buffalo_l ì‚¬ìš© (ì–¼êµ´ ê°ì§€ + ì„ë² ë”© í†µí•©!)
        # AdaFace, YOLO Face ëŒ€ì‹  InsightFace buffalo_l ì‚¬ìš©
        logging.info("ğŸ¦¬ buffalo_l ëª¨ë¸ ì‚¬ìš© (ì–¼êµ´ ê°ì§€ + ì„ë² ë”© í†µí•©)")
        use_adaface = False
        adaface_model_path = None

        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ (InsightFace ì—†ì´ë„ ì‚¬ìš© ê°€ëŠ¥)
        face_database = None
        try:
            index_path = config.Paths.FAISS_INDEX
            face_database = self._load_face_database(index_path)
            if face_database and face_database[0] is not None:
                logging.info(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {index_path}")
            else:
                logging.warning("âš ï¸ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ì—†ìŒ")
                face_database = None
        except Exception as e:
            logging.warning(f"âš ï¸ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            face_database = None

        # InsightFaceê°€ ì—†ìœ¼ë©´ AdaFace + FAISSë¡œ ì–¼êµ´ ì¸ì‹ ì‹œë„
        if not INSIGHTFACE_AVAILABLE:
            logging.warning("InsightFaceê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ buffalo_l ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            if use_adaface and face_database:
                logging.info("âœ… AdaFace + FAISSë¡œ ì–¼êµ´ ì¸ì‹ ê¸°ëŠ¥ì„ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            else:
                logging.warning("ì–¼êµ´ ê°ì§€ëŠ” YOLO ëª¨ë¸ë¡œ ì‘ë™í•©ë‹ˆë‹¤. ì´ë¦„ ì¸ì‹ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            return face_model, None, face_database, face_uses_trt, use_adaface, adaface_model_path

        try:
            # InsightFace buffalo_l ê´€ë ¨ ì´ˆê¸°í™” (ì–¼êµ´ ê°ì§€ + ì„ë² ë”© í†µí•©)
            # NOTE: buffalo_lë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ YOLO Face ëª¨ë¸ ì²´í¬ ì œê±°
            # face_modelì€ Noneì´ì–´ë„ ë¨ (buffalo_lì´ ì–¼êµ´ ê°ì§€ + ì„ë² ë”© ëª¨ë‘ ì²˜ë¦¬)
            
            # PyTorch ëª¨ë¸ ìµœì í™” (YOLO Face ì‚¬ìš© ì‹œì—ë§Œ)
            underlying_face = getattr(face_model, "model", None) if face_model else None
            if underlying_face is not None:
                # float() ë©”ì„œë“œë¡œ ëª¨ë¸ì„ float32ë¡œ ë³€í™˜ (CUDA ìµœì í™”)
                if not isinstance(underlying_face, str) and hasattr(underlying_face, "float"):
                    try:
                        underlying_face.float()
                    except (AttributeError, TypeError):
                        pass
            
            # ONNX ëª¨ë¸ì€ YOLOê°€ ë‚´ë¶€ì ìœ¼ë¡œ ONNX Runtimeì„ ì‚¬ìš©í•˜ë¯€ë¡œ
            # device ì„¤ì •ì€ YOLO API í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
            target_device_face = self.device_face
            
            # ONNX Runtime CUDA Provider í™•ì¸
            try:
                import onnxruntime as ort
                onnx_providers = ort.get_available_providers()
                has_onnx_cuda = 'CUDAExecutionProvider' in onnx_providers
            except:
                has_onnx_cuda = False
            
            if target_device_face == 'cpu' and has_onnx_cuda:
                target_device_face = 'cuda:0'
                logging.info("ğŸ”„ ONNX Runtime CUDA Provider ê°ì§€ë¨. YOLO Face ëª¨ë¸ì´ GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            # YOLO Face ëª¨ë¸ ì„¤ì • (buffalo_lë§Œ ì‚¬ìš© ì‹œ ìŠ¤í‚µ)
            if face_model is not None:
                try:
                    # ONNX ëª¨ë¸ë„ YOLO ë˜í¼ë¥¼ í†µí•´ .to() ë©”ì„œë“œ ì§€ì› ê°€ëŠ¥
                    face_model.to(target_device_face)
                    if hasattr(face_model, 'eval'):
                        face_model.eval()
                    logging.info(f"âœ… Face ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {target_device_face})")
                except (AttributeError, TypeError) as e:
                    # ONNX ëª¨ë¸ì€ .to() ë©”ì„œë“œê°€ ì—†ì„ ìˆ˜ ìˆìŒ (ì •ìƒ)
                    logging.debug(f"Face ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì„¤ì •: {e} (ONNX ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬ë¨)")
                except Exception as e:
                    logging.warning(f"âš ï¸ Face ëª¨ë¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {e}")
            else:
                logging.info("ğŸ¦¬ YOLO Face ëª¨ë¸ ì—†ìŒ - buffalo_lë¡œ ì–¼êµ´ ê°ì§€ ì²˜ë¦¬")
            
            # âœ… Face ëª¨ë¸ë„ GPU ì‚¬ìš© ì—¬ë¶€ ë° ì…ë ¥/ì¶œë ¥ í™•ì¸ (ì´ë¯¸ _load_yolo_variantì—ì„œ í™•ì¸í–ˆì§€ë§Œ ì¬í™•ì¸)
            try:
                import onnxruntime as ort
                session_obj = None
                if hasattr(face_model, 'model'):
                    if hasattr(face_model.model, 'session'):
                        session_obj = face_model.model.session
                    elif hasattr(face_model.model, 'predictor') and hasattr(face_model.model.predictor, 'session'):
                        session_obj = face_model.model.predictor.session
                
                if session_obj:
                    face_providers = session_obj.get_providers()
                    if 'CUDAExecutionProvider' in face_providers and face_providers[0] == 'CUDAExecutionProvider':
                        logging.info(f"âœ… Face ONNX ëª¨ë¸: GPU ì‚¬ìš© í™•ì¸ë¨")
                    else:
                        logging.warning(f"âš ï¸ Face ONNX ëª¨ë¸: GPU ë¯¸ì‚¬ìš© ë˜ëŠ” ìš°ì„ ìˆœìœ„ ë‚®ìŒ (Providers: {face_providers})")
            except Exception as face_check_e:
                logging.debug(f"Face ëª¨ë¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {face_check_e}")
            
            # ë””ë°”ì´ìŠ¤ ì •ë³´ ë¡œê¹…
            device_str = str(self.device_face).upper()
            if 'cuda' in device_str:
                gpu_id_face = int(self.device_face.split(':')[-1]) if ':' in str(self.device_face) else 0
                device_info = f"CUDA GPU {gpu_id_face}"
            else:
                device_info = "CPU"
            
            logging.info(f"âœ… YOLO ì–¼êµ´ ê°ì§€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: PyTorch ({device_info})")
            
            # 2. InsightFace ëª¨ë¸ ê²½ë¡œ ì„¤ì •
            # InsightFaceëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ~/.insightface/models/ ê²½ë¡œë¥¼ ì°¾ìŒ
            # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œê°€ ìˆìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
            insightface_models_dir = os.path.normpath(os.path.join(config.BASE_DIR, "../../models/insightface"))
            if os.path.exists(insightface_models_dir):
                # í™˜ê²½ ë³€ìˆ˜ë¡œ InsightFace ëª¨ë¸ ê²½ë¡œ ì„¤ì •
                os.environ['INSIGHTFACE_ROOT'] = os.path.normpath(os.path.join(config.BASE_DIR, "../../models"))
                logging.info(f"InsightFace ëª¨ë¸ ê²½ë¡œ ì„¤ì •: {insightface_models_dir}")
            else:
                # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš© (ì‚¬ìš©ì í™ˆ ë””ë ‰í† ë¦¬)
                default_insightface_dir = os.path.expanduser("~/.insightface/models")
                logging.info(f"ë¡œì»¬ InsightFace ëª¨ë¸ ê²½ë¡œ ì—†ìŒ. ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©: {default_insightface_dir}")
            
            # 2. buffalo_l ëª¨ë¸ë§Œ ì‚¬ìš© (AdaFace ë¹„í™œì„±í™”)
            # ğŸ¦¬ buffalo_lë¡œ í†µí•©: ì–¼êµ´ ê°ì§€ + ì„ë² ë”© ëª¨ë‘ buffalo_l ì‚¬ìš©
            use_adaface = False
            adaface_model_path = None
            logging.info("ğŸ¦¬ buffalo_l ëª¨ë¸ë¡œ ì–¼êµ´ ê°ì§€ + ì„ë² ë”© í†µí•©")
            
            # InsightFaceëŠ” í•­ìƒ 'buffalo_l' ëª¨ë¸ì„ ì‚¬ìš© (detection ëª¨ë“ˆìš©)
            # ì‹¤ì œ ì„ë² ë”© ì¶”ì¶œì€ FastIndustrialRecognizerë¥¼ í†µí•´ AdaFaceë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
            face_model_name = 'buffalo_l'
            
            # 3. buffalo_L ëª¨ë¸ ë¡œë“œ (InsightFace - ì–¼êµ´ ì„ë² ë”© ì¶”ì¶œìš©)
            # ì‹œìŠ¤í…œ íë¦„: yolov11n-face.ptë¡œ ì–¼êµ´ ê°ì§€ â†’ ì–¼êµ´ ìë¥´ê¸° â†’ buffalo_Lë¡œ ì„ë² ë”© ì¶”ì¶œ â†’ FAISS ë§¤ì¹­
            # AdaFaceë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°: yolov11n-face.ptë¡œ ì–¼êµ´ ê°ì§€ + ëœë“œë§ˆí¬ ì¶”ì¶œ â†’ FastIndustrialRecognizerë¡œ AdaFace ì„ë² ë”© ì¶”ì¶œ
            if 'cuda' in str(self.device_face):
                # CUDA ìš°ì„ , ì‹¤íŒ¨ ì‹œ CPU í´ë°±
                # ğŸ¦¬ buffalo_l: GPU 0 ê³ ì • (GPU 1ì—ì„œ CAM-0 ê°ì§€ ì‹¤íŒ¨ ë¬¸ì œ)
                gpu_id_face = 0  # GPU 0 ì‚¬ìš© - ì•ˆì •ì ìœ¼ë¡œ ì‘ë™
                
                # â­ TensorRT ìš°ì„  â†’ CUDA â†’ CPU ìˆœì„œë¡œ Provider ì„¤ì •
                # TensorRT ì—”ì§„(.engine)ì´ ìˆìœ¼ë©´ TensorRT ì‚¬ìš©, ì—†ìœ¼ë©´ CUDA í´ë°±
                trt_options = {
                    'device_id': gpu_id_face,
                    'trt_max_workspace_size': 4 * 1024 * 1024 * 1024,  # 4GB
                    'trt_fp16_enable': True,  # FP16 ê°€ì†
                }
                cuda_options = {
                    'device_id': gpu_id_face,
                    'arena_extend_strategy': 'kNextPowerOfTwo',  # ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”
                    'gpu_mem_limit': 10 * 1024 * 1024 * 1024,  # 10GB ì œí•œ (11GB GPU)
                    'cudnn_conv_algo_search': 'EXHAUSTIVE',  # ìµœì  ì•Œê³ ë¦¬ì¦˜ ê²€ìƒ‰
                    'do_copy_in_default_stream': True,  # ìŠ¤íŠ¸ë¦¼ ìµœì í™”
                }
                
                # TensorRT Providerê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
                import onnxruntime as ort
                available_providers = ort.get_available_providers()
                
                # ğŸ¦¬ buffalo_l: TensorRT ë¹„í™œì„±í™” (ì•ˆì •ì„± + ë¹ ë¥¸ warmup)
                # TensorRTëŠ” ë§¤ë²ˆ ì—”ì§„ì„ ë¹Œë“œí•´ì„œ 30ì´ˆ+ warmupì´ í•„ìš”í•¨
                # CUDAë§Œ ì‚¬ìš©í•˜ë©´ 2-3ì´ˆë¡œ ë‹¨ì¶•ë¨
                providers = []
                # TensorRT ë¹„í™œì„±í™” (ì£¼ì„ ì²˜ë¦¬)
                # if 'TensorrtExecutionProvider' in available_providers:
                #     providers.append(('TensorrtExecutionProvider', trt_options))
                #     logging.info(f"ğŸš€ TensorRT Provider í™œì„±í™” (GPU {gpu_id_face})")
                
                providers.append(('CUDAExecutionProvider', cuda_options))
                providers.append('CPUExecutionProvider')
                logging.info(f"ğŸ¦¬ buffalo_l: CUDA Provider ì‚¬ìš© (TensorRT ë¹„í™œì„±í™” - ì•ˆì •ì„± í–¥ìƒ)")
                
                ctx_id = gpu_id_face  # ctx_idë¡œ íŠ¹ì • GPU ì§€ì •
                logging.info(f"ğŸ¦¬ InsightFace buffalo_l GPU {gpu_id_face} ëª¨ë“œ í™œì„±í™” (ctx_id={gpu_id_face})")
            else:
                providers = ['CPUExecutionProvider']
                ctx_id = -1  # CPU ì‚¬ìš©
                model_info = f"AdaFace ëª¨ë¸" if use_adaface else "buffalo_L ëª¨ë¸"
                logging.info(f"InsightFace CPU ëª¨ë“œ í™œì„±í™” ({model_info} - ì„ë² ë”© ì¶”ì¶œìš©)")
            
            # InsightFace ì´ˆê¸°í™” (buffalo_l ëª¨ë¸ ì‚¬ìš© - detection ëª¨ë“ˆìš©)
            # ì‹¤ì œ ì–¼êµ´ ê°ì§€ëŠ” YOLO(yolov11n-face.pt)ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, 
            # InsightFaceëŠ” detection ëª¨ë“ˆì´ ìˆì–´ì•¼ ì •ìƒ ì‘ë™í•˜ë¯€ë¡œ í¬í•¨
            # AdaFaceë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ë„ InsightFaceëŠ” 'buffalo_l'ì„ ì‚¬ìš©í•˜ê³ , 
            # ì‹¤ì œ ì„ë² ë”© ì¶”ì¶œì€ FastIndustrialRecognizerë¥¼ í†µí•´ AdaFaceë¥¼ ì‚¬ìš©
            face_analyzer = FaceAnalysis(
                name=face_model_name,  # í•­ìƒ 'buffalo_l' (detection ëª¨ë“ˆìš©)
                providers=providers,
                allowed_modules=['detection', 'recognition']  # detection ëª¨ë“ˆ í¬í•¨ (í•„ìˆ˜)
            )
            
            # det_size ì„¤ì • (ì‹¤ì‹œê°„ ì²˜ë¦¬ ìµœì í™”)
            # ctx_idë¡œ íŠ¹ì • GPU ì§€ì • (ë©€í‹° GPU ì§€ì›)
            det_size = config.Thresholds.FACE_DETECTION_SIZE
            face_analyzer.prepare(ctx_id=ctx_id, det_size=det_size)
            
            # ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ Provider í™•ì¸ ë° ì…ë ¥/ì¶œë ¥ í™•ì¸
            try:
                rec_session = face_analyzer.models['recognition'].session
                actual_providers = rec_session.get_providers()
                logging.info(f"{face_model_name} ëª¨ë¸(InsightFace) ë¡œë“œ ì™„ë£Œ (Provider: {actual_providers}, ctx_id={ctx_id})")
                
                # InsightFace ëª¨ë¸ ì…ë ¥/ì¶œë ¥ í™•ì¸
                try:
                    rec_inputs = rec_session.get_inputs()
                    rec_outputs = rec_session.get_outputs()
                    logging.info(f"ğŸ” InsightFace ëª¨ë¸ ì…ë ¥/ì¶œë ¥ ì •ë³´:")
                    for i, inp in enumerate(rec_inputs):
                        logging.info(f"   ì…ë ¥[{i}]: name={inp.name}, shape={inp.shape}, type={inp.type}")
                    for i, out in enumerate(rec_outputs):
                        logging.info(f"   ì¶œë ¥[{i}]: name={out.name}, shape={out.shape}, type={out.type}")
                except Exception as io_e:
                    logging.debug(f"InsightFace ëª¨ë¸ ì…ë ¥/ì¶œë ¥ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {io_e}")
            except Exception as e:
                logging.info(f"{face_model_name} ëª¨ë¸(InsightFace) ë¡œë“œ ì™„ë£Œ (Provider: {providers}, ctx_id={ctx_id})")
            
            # AdaFace ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ ë¡œê¹…
            if use_adaface and adaface_model_path:
                logging.info(f"âœ… AdaFace ëª¨ë¸ í™œì„±í™”: {adaface_model_path}")
                logging.info("ğŸ’¡ ì‹¤ì œ ì„ë² ë”© ì¶”ì¶œì€ FastIndustrialRecognizerë¥¼ í†µí•´ AdaFaceë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            else:
                logging.info("âœ… buffalo_l ëª¨ë¸ ì‚¬ìš© (InsightFace ê¸°ë³¸ ëª¨ë¸)")

            # 3. FAISS ì¸ë±ìŠ¤ ë¡œë“œ (face_index.faiss, face_index.faiss.labels.npy)
            # face_embeddings.npyëŠ” FAISS ì¸ë±ìŠ¤ì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìŒ
            logging.info(f"ğŸ” FAISS ì¸ë±ìŠ¤ ê²½ë¡œ í™•ì¸: {config.Paths.FAISS_INDEX} (ì¡´ì¬: {os.path.exists(config.Paths.FAISS_INDEX)})")
            logging.info(f"ğŸ” FAISS ë ˆì´ë¸” ê²½ë¡œ í™•ì¸: {config.Paths.FAISS_LABELS} (ì¡´ì¬: {os.path.exists(config.Paths.FAISS_LABELS)})")
            face_index, face_labels = self._load_face_database(config.Paths.FAISS_INDEX)
            face_database = (face_index, face_labels)  # íŠœí”Œë¡œ ì €ì¥
            logging.info(f"âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ: ì¸ë±ìŠ¤={face_index.ntotal if face_index else 0}ê°œ, ë ˆì´ë¸”={len(face_labels) if face_labels is not None else 0}ê°œ")
            if face_index is None or (hasattr(face_index, 'ntotal') and face_index.ntotal == 0):
                logging.error(f"âŒ FAISS ì¸ë±ìŠ¤ê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! ì–¼êµ´ ì¸ì‹ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            if face_labels is None or len(face_labels) == 0:
                logging.error(f"âŒ FAISS ë ˆì´ë¸”ì´ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! ì–¼êµ´ ì¸ì‹ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        except Exception as e:
            logging.error(f"ì–¼êµ´ ì¸ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
            face_model = None
            face_analyzer = None # ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì„¤ì •
            face_database = None
            face_uses_trt = False
            use_adaface = False
            adaface_model_path = None

        return face_model, face_analyzer, face_database, face_uses_trt, use_adaface, adaface_model_path

    @staticmethod
    def _load_face_database(index_path: str) -> Tuple[Optional[object], Optional[np.ndarray]]:
        """
        FAISS ì¸ë±ìŠ¤ì™€ ë ˆì´ë¸” íŒŒì¼ì„ í•¨ê»˜ ë¡œë“œ
        
        ë¡œë“œí•˜ëŠ” íŒŒì¼:
        - face_index.faiss: FAISS ì¸ë±ìŠ¤ (face_embeddings.npyì˜ ë°ì´í„°ê°€ í¬í•¨ë¨)
        - face_index.faiss.labels.npy: ì¸ë¬¼ ì´ë¦„ ë ˆì´ë¸”
        
        Returns:
            (faiss_index, labels): FAISS ì¸ë±ìŠ¤ì™€ ë ˆì´ë¸” ë°°ì—´ íŠœí”Œ
        """
        try:
            # configì—ì„œ ì„¤ì •ëœ ê²½ë¡œ ì‚¬ìš© (face/data/face_index.faiss)
            if not os.path.exists(index_path):
                # í´ë°± 1: í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì°¾ê¸° (í•˜ìœ„ í˜¸í™˜ì„±)
                project_root_index = os.path.normpath(os.path.join(config.BASE_DIR, "../..", "face_index.faiss"))
                if os.path.exists(project_root_index):
                    index_path = project_root_index
                    logging.info(f"âœ… FAISS ì¸ë±ìŠ¤ ë°œê²¬ (í”„ë¡œì íŠ¸ ë£¨íŠ¸): {index_path}")
                else:
                    # í´ë°± 2: face/data í´ë”ì—ì„œ ì°¾ê¸° (ìƒˆ ê²½ë¡œ)
                    face_data_index = os.path.normpath(os.path.join(config.BASE_DIR, "../..", "face", "data", "face_index.faiss"))
                    if os.path.exists(face_data_index):
                        index_path = face_data_index
                        logging.info(f"âœ… FAISS ì¸ë±ìŠ¤ ë°œê²¬ (face/data): {index_path}")
                    else:
                        logging.warning(f"Faiss ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.Paths.FAISS_INDEX}")
                        # ë¹ˆ ì¸ë±ìŠ¤ ë°˜í™˜
                        empty_index = faiss.IndexFlatIP(512)
                        empty_labels = np.array([])
                        return empty_index, empty_labels

            # ë ˆì´ë¸” íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            labels_path = config.Paths.FAISS_LABELS
            logging.info(f"FAISS ë ˆì´ë¸” íŒŒì¼ ê²½ë¡œ í™•ì¸: {labels_path} (ì¡´ì¬: {os.path.exists(labels_path)})")
            if not os.path.exists(labels_path):
                # í´ë°± 1: í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì°¾ê¸° (í•˜ìœ„ í˜¸í™˜ì„±)
                project_root_labels = os.path.normpath(os.path.join(config.BASE_DIR, "../..", "face_index.faiss.labels.npy"))
                if os.path.exists(project_root_labels):
                    labels_path = project_root_labels
                    logging.info(f"âœ… FAISS ë ˆì´ë¸” ë°œê²¬ (í”„ë¡œì íŠ¸ ë£¨íŠ¸): {labels_path}")
                else:
                    # í´ë°± 2: face/data í´ë”ì—ì„œ ì°¾ê¸° (ìƒˆ ê²½ë¡œ)
                    face_data_labels = os.path.normpath(os.path.join(config.BASE_DIR, "../..", "face", "data", "face_index.faiss.labels.npy"))
                    if os.path.exists(face_data_labels):
                        labels_path = face_data_labels
                        logging.info(f"âœ… FAISS ë ˆì´ë¸” ë°œê²¬ (face/data): {labels_path}")
                    else:
                        abs_path = os.path.abspath(labels_path)
                        logging.warning(f"Faiss ë ˆì´ë¸” íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {labels_path} (ì ˆëŒ€ ê²½ë¡œ: {abs_path})")
                        labels_path = None

            # ì¸ë±ìŠ¤ ë¡œë“œ
            dimension = 512  # InsightFace ì„ë² ë”© ì°¨ì›
            index = faiss.read_index(index_path)

            # FAISS ì¸ë±ìŠ¤ë¥¼ GPUë¡œ ì´ì „ (ê¸°ë³¸ í™œì„±í™”: ì„±ëŠ¥ í–¥ìƒ)
            # ì–¼êµ´ ì¸ì‹ì´ GPU 1ì„ ì‚¬ìš©í•˜ë¯€ë¡œ FAISSë„ GPU 1ì„ ì‚¬ìš©í•˜ì—¬ ë¶€í•˜ ë¶„ì‚°
            try:
                import torch as _torch
                if config.Thresholds.USE_FAISS_GPU and _torch.cuda.is_available():
                    # ì–¼êµ´ ì¸ì‹ì´ ì‚¬ìš©í•˜ëŠ” GPU ID í™•ì¸ (ë©€í‹° GPUì¸ ê²½ìš° GPU 1)
                    gpu_count = _torch.cuda.device_count()
                    faiss_gpu_id = 1 if gpu_count >= 2 else 0  # GPU 1 ì‚¬ìš© (ë©€í‹° GPUì¸ ê²½ìš°)
                    
                    # faiss-gpu íŒ¨í‚¤ì§€ í™•ì¸
                    try:
                        gpu_res = faiss.StandardGpuResources()
                    except AttributeError:
                        # faiss-cpuë§Œ ì„¤ì¹˜ëœ ê²½ìš°
                        logging.warning("âš ï¸ FAISS GPU ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (faiss-cpuë§Œ ì„¤ì¹˜ë¨)")
                        logging.info("ğŸ’¡ FAISS GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
                        logging.info("   1. Conda ì‚¬ìš©: conda install -c pytorch faiss-gpu")
                        logging.info("   2. CPU ë²„ì „ ê³„ì† ì‚¬ìš© (í˜„ì¬ ì„¤ì •)")
                        logging.info("   3. .env íŒŒì¼ì— USE_FAISS_GPU=0 ì„¤ì •í•˜ì—¬ CPU ëª¨ë“œ ëª…ì‹œ")
                        raise AttributeError("faiss.StandardGpuResources not available (faiss-cpu only)")
                    
                    # ì„ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œí™”ë¡œ ì•ˆì •ì„± í–¥ìƒ
                    try:
                        gpu_res.setTempMemory(0)
                    except Exception:
                        pass
                    index = faiss.index_cpu_to_gpu(gpu_res, faiss_gpu_id, index)
                    logging.info(f"âœ… FAISS ì¸ë±ìŠ¤ë¥¼ GPU {faiss_gpu_id}ë¡œ ì´ì „ ì™„ë£Œ (ì„±ëŠ¥ í–¥ìƒ, USE_FAISS_GPU=1)")
                else:
                    logging.info("FAISS CPU ì¸ë±ìŠ¤ ì‚¬ìš© (USE_FAISS_GPU=0 ë˜ëŠ” CUDA ë¹„í™œì„±)")
            except AttributeError as attr_e:
                # faiss-gpuê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
                logging.warning(f"âš ï¸ FAISS GPU ì´ì „ ì‹¤íŒ¨: {attr_e}")
                logging.info("ğŸ’¡ CPU ëª¨ë“œë¡œ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤ (ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥)")
                logging.info("   FAISS GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ condaë¥¼ í†µí•´ faiss-gpuë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”")
            except Exception as gpu_e:
                logging.warning(f"âš ï¸ FAISS GPU ì´ì „ ì‹¤íŒ¨, CPU ì¸ë±ìŠ¤ ì‚¬ìš©: {gpu_e}")
                logging.info("ğŸ’¡ CPU ëª¨ë“œë¡œ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤ (ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥)")

            # ë ˆì´ë¸” ë¡œë“œ
            labels = np.array([])
            if labels_path and os.path.exists(labels_path):
                labels = np.load(labels_path, allow_pickle=True)
                logging.info(f"âœ… Faiss ì¸ë±ìŠ¤ ë° ë ˆì´ë¸” ë¡œë“œ ì™„ë£Œ. ì¸ë±ìŠ¤={index.ntotal}ê°œ ì„ë² ë”©, ë ˆì´ë¸”={len(labels)}ê°œ í¬í•¨.")
                if len(labels) == 0:
                    logging.error(f"âŒ ë ˆì´ë¸” íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! ì–¼êµ´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë‹¤ì‹œ êµ¬ì¶•í•´ì•¼ í•©ë‹ˆë‹¤.")
                elif index.ntotal != len(labels):
                    logging.warning(f"âš ï¸ ì¸ë±ìŠ¤ í¬ê¸°({index.ntotal})ì™€ ë ˆì´ë¸” í¬ê¸°({len(labels)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
            else:
                logging.error(f"âŒ Faiss ë ˆì´ë¸” íŒŒì¼ ì—†ìŒ. ì¸ë±ìŠ¤ë§Œ ë¡œë“œ: {index.ntotal}ê°œ ì„ë² ë”©. ì–¼êµ´ ì¸ì‹ì´ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")

            return index, labels
        except Exception as e:
            logging.error(f"Faiss ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
            # ë¹ˆ ì¸ë±ìŠ¤ ë°˜í™˜
            empty_index = faiss.IndexFlatIP(512)
            empty_labels = np.array([])
            return empty_index, empty_labels

    def cleanup(self):
        logging.info("SafetySystem ì •ë¦¬ë¨.")

    # --- í—¬í¼ í•¨ìˆ˜ (Static Methods) ---
    # ì´ í•¨ìˆ˜ë“¤ì€ server.pyì˜ process_single_frameì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.

    @staticmethod
    def _scale_boxes(
        boxes: Any, 
        w_scale: float, 
        h_scale: float, 
        names: Dict[int, str]
    ) -> Dict[str, List[Dict[str, Any]]]:
        scaled: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        if boxes is None or len(boxes) == 0:
            return scaled

        for box in boxes:
            try:
                class_name = names[int(box.cls[0])]
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                confidence = float(box.conf[0].cpu().numpy())
                scaled[class_name].append({
                    'bbox': (x1 * w_scale, y1 * h_scale, x2 * w_scale, y2 * h_scale),
                    'confidence': confidence
                })
            except Exception as e:
                logging.warning(f"ë°•ìŠ¤ ìŠ¤ì¼€ì¼ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        return scaled

    @staticmethod
    def _scale_poses(
        pose_result: Any, 
        w_scale: float, 
        h_scale: float, 
        orig_shape: Tuple[int, int]
    ) -> List[Dict[str, Any]]:
        scaled: List[Dict[str, Any]] = []
        if pose_result.keypoints and pose_result.boxes is not None and pose_result.boxes.id is not None:
            tracker_ids = pose_result.boxes.id.int().cpu().numpy()
            for idx, (kpts, tracker_id) in enumerate(zip(pose_result.keypoints, tracker_ids)):
                try:
                    if torch.sum(kpts.conf > config.Thresholds.POSE_CONFIDENCE) >= config.Thresholds.MIN_VISIBLE_KEYPOINTS:
                        kpts_data = kpts.data.clone()
                        kpts_data[..., 0] *= w_scale
                        kpts_data[..., 1] *= h_scale
                        box = pose_result.boxes[idx].xyxy[0].cpu().numpy()
                        scaled_box = (box[0] * w_scale, box[1] * h_scale, box[2] * w_scale, box[3] * h_scale)

                        scaled.append({'keypoints': Keypoints(kpts_data, orig_shape), 'bbox_xyxy': scaled_box,
                                       'tracker_id': tracker_id})
                except Exception as e:
                    logging.warning(f"í¬ì¦ˆ ìŠ¤ì¼€ì¼ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        return scaled

    # ( ... _get_frame_batch, cleanup, _update_people_states, _draw_results, _create_grid_display ë“±ë“± ... ëª¨ë‘ ì‚­ì œ ...)
    # ( ... _match_and_update_people, _check_fall_status, _check_safety_gear_status ë“±ë“± ... ëª¨ë‘ ì‚­ì œ ...)
    # SafetySystem í´ë˜ìŠ¤ëŠ” ì´ì œ ëª¨ë¸ ë¡œë”©ê³¼ í—¬í¼ í•¨ìˆ˜(_scale_boxes, _scale_poses)ë§Œ ì œê³µí•©ë‹ˆë‹¤.
