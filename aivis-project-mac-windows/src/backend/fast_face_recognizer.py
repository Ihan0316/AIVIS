# fast_face_recognizer.py - ê³ ì† ì–¼êµ´ ì¸ì‹ ëª¨ë“ˆ
"""
ëœë“œë§ˆí¬ ê¸°ë°˜ ê³ ì† ì–¼êµ´ ì¸ì‹
YOLO Faceì˜ ëœë“œë§ˆí¬ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ InsightFaceì˜ Detection ë‹¨ê³„ ìƒëµ
"""
import logging
import cv2
import numpy as np
from typing import Optional, Tuple, Any
import os

try:
    from insightface.utils import face_align
    INSIGHTFACE_UTILS_AVAILABLE = True
except ImportError:
    INSIGHTFACE_UTILS_AVAILABLE = False
    logging.warning("insightface.utilsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. face_alignì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

try:
    import onnxruntime
    ONNXRUNTIME_AVAILABLE = True
except ImportError:
    ONNXRUNTIME_AVAILABLE = False
    logging.warning("onnxruntimeë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ì ‘ ONNX ì¶”ë¡ ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# TensorRT ì§€ì› (ONNXë³´ë‹¤ 12ë°° ë¹ ë¦„)
try:
    import tensorrt as trt
    import torch
    TENSORRT_AVAILABLE = True
except ImportError:
    TENSORRT_AVAILABLE = False


class FastIndustrialRecognizer:
    """
    ê³ ì† ì–¼êµ´ ì¸ì‹ í´ë˜ìŠ¤
    YOLO Faceì˜ ëœë“œë§ˆí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ InsightFaceì˜ Detection ë‹¨ê³„ë¥¼ ìƒëµ
    """
    
    def __init__(
        self, 
        model_path: Optional[str] = None,
        ctx_id: int = 0,
        use_adaface: bool = False
    ):
        """
        :param model_path: ONNX ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ InsightFace ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©)
        :param ctx_id: GPU ID (0, 1, ... ë˜ëŠ” -1 for CPU)
        :param use_adaface: AdaFace ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ (Trueë©´ AdaFace, Falseë©´ ê¸°ì¡´ buffalo_l)
        """
        self.ctx_id = ctx_id
        self.use_adaface = use_adaface
        self.session = None
        self.input_name = None
        self.output_name = None
        self.use_direct_onnx = False
        
        # TensorRT ê´€ë ¨
        self.use_tensorrt = False
        self.trt_context = None
        self.trt_input_tensor = None
        self.trt_output_tensor = None
        self.trt_output_div = None
        
        # TensorRT ì—”ì§„ ìš°ì„  ë¡œë“œ (ONNX ëŒ€ë¹„ 2ë°° ë¹ ë¦„: 21ms â†’ 10ms)
        if model_path and TENSORRT_AVAILABLE:
            # .engine íŒŒì¼ì´ ì§ì ‘ ì „ë‹¬ëœ ê²½ìš°
            if model_path.endswith('.engine'):
                engine_path = model_path
            else:
                engine_path = model_path.replace('.onnx', '.engine')
            
            if os.path.exists(engine_path):
                try:
                    self._init_tensorrt(engine_path, ctx_id)
                except Exception as e:
                    logging.warning(f"âš ï¸ TensorRT ì´ˆê¸°í™” ì‹¤íŒ¨, ONNXë¡œ í´ë°±: {e}")
        
        # TensorRT ì‹¤íŒ¨ ì‹œ ONNX ì¶”ë¡  ì‚¬ìš©
        if not self.use_tensorrt and model_path and os.path.exists(model_path) and ONNXRUNTIME_AVAILABLE:
            try:
                # ONNX Runtime ì„¸ì…˜ ìƒì„±
                available_providers = onnxruntime.get_available_providers()
                providers = []
                
                # CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° (GPU 2ëŒ€ ìµœëŒ€ í™œìš©)
                if ctx_id >= 0 and 'CUDAExecutionProvider' in available_providers:
                    # CUDA Execution Provider ìµœì í™” ì˜µì…˜
                    cuda_options = {
                        'device_id': ctx_id,
                        'arena_extend_strategy': 'kNextPowerOfTwo',  # ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”
                        'gpu_mem_limit': 10 * 1024 * 1024 * 1024,  # 10GB ì œí•œ (11GB GPU)
                        'cudnn_conv_algo_search': 'DEFAULT',  # EXHAUSTIVE -> DEFAULT (ì•ˆì •ì„± ë° ì´ˆê¸°í™” ì†ë„ í–¥ìƒ)
                        'do_copy_in_default_stream': True,  # ìŠ¤íŠ¸ë¦¼ ìµœì í™”
                    }
                    providers.append(('CUDAExecutionProvider', cuda_options))
                    logging.info(f"âœ… CUDA GPU {ctx_id} ê°ì§€: CUDA Execution Provider ì‚¬ìš© (ìµœì í™” í™œì„±í™”)")
                
                # CPUëŠ” í•­ìƒ í´ë°±ìœ¼ë¡œ ì¶”ê°€
                providers.append('CPUExecutionProvider')
                
                # ì„¸ì…˜ ì˜µì…˜ ì„¤ì • (GPU 2ëŒ€ ìµœëŒ€ í™œìš© - GPU ì‚¬ìš©ë¥  ê·¹ëŒ€í™”)
                sess_options = onnxruntime.SessionOptions()
                # GPU ì‚¬ìš© ì‹œ ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” (GPU ì‚¬ìš©ë¥  ê·¹ëŒ€í™”)
                if ctx_id >= 0 and 'CUDAExecutionProvider' in available_providers:
                    sess_options.execution_mode = onnxruntime.ExecutionMode.ORT_PARALLEL  # SEQUENTIAL -> PARALLEL (GPU ì‚¬ìš©ë¥  ê·¹ëŒ€í™”)
                    sess_options.intra_op_num_threads = 4  # 2 -> 4 (GPU ë³‘ë ¬ ì²˜ë¦¬ ì¦ê°€)
                    sess_options.inter_op_num_threads = 4  # 2 -> 4 (GPU ë³‘ë ¬ ì²˜ë¦¬ ì¦ê°€)
                else:
                    sess_options.intra_op_num_threads = 1
                    sess_options.inter_op_num_threads = 1
                    sess_options.execution_mode = onnxruntime.ExecutionMode.ORT_SEQUENTIAL
                # ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”
                sess_options.enable_mem_pattern = True
                sess_options.enable_cpu_mem_arena = True
                # ê·¸ë˜í”„ ìµœì í™” ë ˆë²¨ (ëª¨ë“  ìµœì í™” í™œì„±í™”)
                sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
                
                self.session = onnxruntime.InferenceSession(
                    model_path, 
                    providers=providers,
                    sess_options=sess_options
                )
                
                # ì‹¤ì œ ì‚¬ìš© ì¤‘ì¸ Provider í™•ì¸
                active_providers = self.session.get_providers()
                logging.info(f"âœ… FastIndustrialRecognizer: ì§ì ‘ ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({model_path})")
                logging.info(f"ğŸ” í™œì„±í™”ëœ ONNX Providers: {active_providers}")
                
                if 'CUDAExecutionProvider' in active_providers:
                    logging.info("âœ… CUDA ê°€ì† í™œì„±í™”ë¨")
                else:
                    logging.warning("âš ï¸ GPU Providerê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                
                # ì…ë ¥/ì¶œë ¥ ì´ë¦„ í™•ì¸
                inputs = self.session.get_inputs()
                outputs = self.session.get_outputs()
                self.input_name = inputs[0].name
                # AdaFace ONNX ëª¨ë¸ì€ ì¶œë ¥ì´ 2ê°œ (output, onnx::Div_704) - ì²« ë²ˆì§¸ ì¶œë ¥ë§Œ ì‚¬ìš©
                self.output_name = outputs[0].name
                
                # ì…ë ¥/ì¶œë ¥ í˜•ì‹ ë¡œê¹…
                logging.info(f"ğŸ” AdaFace ONNX ëª¨ë¸ ì •ë³´:")
                logging.info(f"   ì…ë ¥: {inputs[0].name}, shape={inputs[0].shape}, type={inputs[0].type}")
                logging.info(f"   ì¶œë ¥: {outputs[0].name}, shape={outputs[0].shape}, type={outputs[0].type}")
                if len(outputs) > 1:
                    logging.debug(f"   ì¶”ê°€ ì¶œë ¥ (ë¬´ì‹œ): {outputs[1].name}, shape={outputs[1].shape}")
                
                self.use_direct_onnx = True
                logging.info(f"âœ… FastIndustrialRecognizer: AdaFace ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({model_path})")
            except Exception as e:
                logging.warning(f"âš ï¸ ì§ì ‘ ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, InsightFace ì‚¬ìš©: {e}")
                self.use_direct_onnx = False
        else:
            logging.info("FastIndustrialRecognizer: InsightFace ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
    
    def _init_tensorrt(self, engine_path: str, ctx_id: int):
        """TensorRT ì—”ì§„ ì´ˆê¸°í™” (PyTorch CUDA ë©”ëª¨ë¦¬ ì‚¬ìš©)"""
        # GPU ë””ë°”ì´ìŠ¤ ì„¤ì • (TensorRT ì´ˆê¸°í™” ì „ì— ì„¤ì •!)
        device_id = ctx_id if ctx_id >= 0 else 0
        device = f'cuda:{device_id}'
        
        # CUDA ë””ë°”ì´ìŠ¤ ëª…ì‹œì  ì„¤ì • ë° ë™ê¸°í™”
        torch.cuda.set_device(device_id)
        torch.cuda.synchronize(device_id)
        
        logger = trt.Logger(trt.Logger.WARNING)
        with open(engine_path, 'rb') as f:
            engine_data = f.read()
        
        runtime = trt.Runtime(logger)
        engine = runtime.deserialize_cuda_engine(engine_data)
        
        if engine is None:
            raise RuntimeError("TensorRT ì—”ì§„ ë¡œë“œ ì‹¤íŒ¨")
        
        self.trt_context = engine.create_execution_context()
        self.trt_engine = engine  # ì—”ì§„ ì°¸ì¡° ìœ ì§€ (GC ë°©ì§€)
        self.trt_device_id = device_id
        
        # ë²„í¼ ë¯¸ë¦¬ í• ë‹¹ (ì¬ì‚¬ìš©ìœ¼ë¡œ ì†ë„ í–¥ìƒ)
        self.trt_input_tensor = torch.empty((1, 3, 112, 112), dtype=torch.float32, device=device).contiguous()
        self.trt_output_tensor = torch.empty((1, 512), dtype=torch.float32, device=device).contiguous()
        self.trt_output_div = torch.empty((1, 1), dtype=torch.float32, device=device).contiguous()
        self.trt_stream = torch.cuda.Stream(device=device)
        
        # ì´ˆê¸°í™” í›„ ë™ê¸°í™”
        torch.cuda.synchronize(device_id)
        
        self.use_tensorrt = True
        logging.info(f"âœ… AdaFace TensorRT ì—”ì§„ ë¡œë“œ ì™„ë£Œ: {engine_path} (GPU {device_id})")
        logging.info(f"   (ONNX ëŒ€ë¹„ 12ë°° ë¹ ë¦„: 27ms â†’ 2ms)")
    
    def get_embedding_fast(
        self, 
        frame: np.ndarray, 
        kps: np.ndarray,
        face_analyzer: Optional[Any] = None
    ) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """
        YOLO Faceì˜ ëœë“œë§ˆí¬(kps)ë¥¼ ì´ìš©í•´ ì¦‰ì‹œ ì •ë ¬ ë° ì„ë² ë”© ì¶”ì¶œ (Detection ìƒëµ)
        
        :param frame: ì›ë³¸ í”„ë ˆì„ (Cropëœ ì´ë¯¸ì§€ê°€ ì•„ë‹˜! ì›ë³¸ì—ì„œ ì¢Œí‘œë¡œ ìë¥´ëŠ”ê²Œ ë” ì •í™•í•¨)
        :param kps: YOLO Faceê°€ ë¦¬í„´í•œ 5ê°œ ëœë“œë§ˆí¬ ì¢Œí‘œ [[x1,y1], ... [x5,y5]]
                    í˜•ì‹: (5, 2) numpy array ë˜ëŠ” list of [x, y]
        :param face_analyzer: InsightFace ë¶„ì„ê¸° (ì§ì ‘ ONNXë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
        
        :return: (embedding, aligned_face) íŠœí”Œ
                 - embedding: 512ì°¨ì› ì„ë² ë”© ë²¡í„° (ì •ê·œí™”ë¨) ë˜ëŠ” None
                 - aligned_face: ì •ë ¬ëœ ì–¼êµ´ ì´ë¯¸ì§€ (112x112) ë˜ëŠ” None
        """
        try:
            # ëœë“œë§ˆí¬ í˜•ì‹ ë³€í™˜ ë° ê²€ì¦
            if kps is None or len(kps) < 5:
                return None, None
            
            # kpsë¥¼ numpy arrayë¡œ ë³€í™˜
            if isinstance(kps, list):
                kps = np.array(kps, dtype=np.float32)
            elif not isinstance(kps, np.ndarray):
                kps = np.array(kps, dtype=np.float32)
            
            # í˜•ì‹ ë³€í™˜: (5, 2) ë˜ëŠ” (10,) -> (5, 2)
            if kps.shape == (10,):
                kps = kps.reshape(5, 2)
            elif len(kps.shape) != 2 or kps.shape[1] != 2:
                logging.warning(f"ëœë“œë§ˆí¬ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {kps.shape}")
                return None, None
            
            # 5ê°œ ëœë“œë§ˆí¬ í™•ì¸ (ì™¼ìª½ ëˆˆ, ì˜¤ë¥¸ìª½ ëˆˆ, ì½”, ì™¼ìª½ ì…ê¼¬ë¦¬, ì˜¤ë¥¸ìª½ ì…ê¼¬ë¦¬)
            if kps.shape[0] < 5:
                return None, None
            
            # Face Alignment (Affine Transformation)
            # ìœ„ì—ì„œ 30ë„ ê°ë„ë¡œ ì´¬ì˜ëœ ì–¼êµ´ì„ ìœ„í•œ ê°œì„ ëœ ì •ë ¬
            # 1. ë¨¼ì € 2D ì •ë ¬ ìˆ˜í–‰ (í‰ë©´ íšŒì „ ë³´ì •)
            # 2. ìœ„ì—ì„œ ë³¸ ê°ë„(pitch) ë³´ì •ì„ ìœ„í•œ ì¶”ê°€ ì „ì²˜ë¦¬
            if not INSIGHTFACE_UTILS_AVAILABLE:
                # ê²½ê³ ëŠ” í•œ ë²ˆë§Œ ì¶œë ¥ (ë§¤ í”„ë ˆì„ë§ˆë‹¤ ì¶œë ¥ ë°©ì§€)
                if not getattr(self, '_insightface_warning_shown', False):
                    logging.info("â„¹ï¸ insightface.utils ì—†ìŒ â†’ ê¸°ë³¸ ì •ë ¬(_simple_align) ì‚¬ìš©")
                    self._insightface_warning_shown = True
                # ê¸°ë³¸ ì •ë ¬ (ê°„ë‹¨í•œ í¬ë¡­)
                aligned_face = self._simple_align(frame, kps)
            else:
                try:
                    # standard output size: 112x112
                    aligned_face = face_align.norm_crop(frame, kps)
                    
                    # ìœ„ì—ì„œ ë³¸ ê°ë„ ë³´ì •: ì–¼êµ´ì´ ìœ„ì—ì„œ 30ë„ ê°ë„ë¡œ ì´¬ì˜ëœ ê²½ìš°
                    # ì½”ì™€ ëˆˆì˜ ìˆ˜ì§ ìœ„ì¹˜ ì°¨ì´ë¥¼ ë¶„ì„í•˜ì—¬ pitch ê°ë„ ì¶”ì •
                    if aligned_face is not None and kps.shape[0] >= 3:
                        # ì½” ìœ„ì¹˜ (kps[2])
                        nose_y = kps[2][1]
                        # ëˆˆ ìœ„ì¹˜ í‰ê·  (kps[0], kps[1])
                        eye_y = (kps[0][1] + kps[1][1]) / 2.0
                        # ì½”ì™€ ëˆˆì˜ ìˆ˜ì§ ê±°ë¦¬
                        vertical_diff = nose_y - eye_y
                        
                        # ìœ„ì—ì„œ ë³¸ ê°ë„ê°€ í¬ë©´ (ì½”ê°€ ëˆˆë³´ë‹¤ ì•„ë˜ì— ìˆìœ¼ë©´) pitch ë³´ì • í•„ìš”
                        # ì¼ë°˜ì ìœ¼ë¡œ ì •ë©´ ì–¼êµ´ì—ì„œëŠ” ì½”ê°€ ëˆˆë³´ë‹¤ ì•½ê°„ ì•„ë˜ì— ìˆì§€ë§Œ,
                        # ìœ„ì—ì„œ ë³¸ ê°ë„ê°€ í¬ë©´ ì´ ì°¨ì´ê°€ ë” ì»¤ì§
                        if vertical_diff > 0:  # ì½”ê°€ ëˆˆë³´ë‹¤ ì•„ë˜ì— ìˆìŒ
                            # ìœ„ì—ì„œ ë³¸ ê°ë„ ë³´ì •: ì–¼êµ´ì„ ì•½ê°„ ìœ„ë¡œ íšŒì „ì‹œí‚¨ ê²ƒì²˜ëŸ¼ ë³´ì •
                            # perspective transformationì„ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ì„ ì •ë©´ìœ¼ë¡œ ë³´ì •
                            aligned_face = self._correct_pitch_angle(aligned_face, vertical_diff)
                            
                except Exception as e:
                    logging.warning(f"face_align.norm_crop ì‹¤íŒ¨, ê¸°ë³¸ ì •ë ¬ ì‚¬ìš©: {e}")
                    aligned_face = self._simple_align(frame, kps)
            
            if aligned_face is None or aligned_face.size == 0:
                return None, None
            
            # í™”ì§ˆ ê°œì„ : ëŒ€ë¹„ í–¥ìƒ (CLAHE) - build_databaseì™€ ë™ì¼í•œ YCrCb ìƒ‰ê³µê°„ ì‚¬ìš©
            try:
                face_size = max(aligned_face.shape[0], aligned_face.shape[1])
                if face_size >= 100:
                    # YCrCb ìƒ‰ê³µê°„ ì‚¬ìš© (build_databaseì™€ ë™ì¼)
                    ycrcb = cv2.cvtColor(aligned_face, cv2.COLOR_BGR2YCrCb)
                    y, cr, cb = cv2.split(ycrcb)
                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                    y = clahe.apply(y)
                    ycrcb = cv2.merge([y, cr, cb])
                    aligned_face = cv2.cvtColor(ycrcb, cv2.COLOR_YCrCb2BGR)
            except Exception:
                pass  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ì‚¬ìš©
            
            # ì„ë² ë”© ì¶”ì¶œ (TensorRT > ONNX > InsightFace ìˆœì„œ, ìë™ í´ë°±)
            embedding = None
            
            if self.use_tensorrt and self.trt_context is not None:
                embedding = self._get_embedding_from_tensorrt(aligned_face)
            
            # TensorRT ì‹¤íŒ¨ ì‹œ ONNXë¡œ í´ë°±
            if embedding is None and self.use_direct_onnx and self.session is not None:
                embedding = self._get_embedding_from_onnx(aligned_face)
            
            # ONNXë„ ì‹¤íŒ¨ ì‹œ InsightFaceë¡œ í´ë°±
            if embedding is None and face_analyzer is not None:
                embedding = self._get_embedding_from_insightface(aligned_face, face_analyzer)
            
            if embedding is None:
                return None, None
            
            if embedding is None:
                return None, None
            
            # Normalize Embedding (L2 Norm) - Cosine Similarityë¥¼ ìœ„í•´ í•„ìˆ˜
            norm_val = np.linalg.norm(embedding)
            if norm_val > 0:
                embedding = embedding / norm_val
            else:
                return None, None
            
            return embedding, aligned_face
            
        except Exception as e:
            logging.debug(f"get_embedding_fast ì˜¤ë¥˜: {e}")
            return None, None
    
    def _correct_pitch_angle(self, aligned_face: np.ndarray, vertical_diff: float) -> np.ndarray:
        """
        ìœ„ì—ì„œ ë³¸ ê°ë„(pitch) ë³´ì •: ì–¼êµ´ì„ ì •ë©´ìœ¼ë¡œ ë³´ì •
        :param aligned_face: ì •ë ¬ëœ ì–¼êµ´ ì´ë¯¸ì§€ (112x112)
        :param vertical_diff: ì½”ì™€ ëˆˆì˜ ìˆ˜ì§ ê±°ë¦¬ ì°¨ì´
        :return: ë³´ì •ëœ ì–¼êµ´ ì´ë¯¸ì§€
        """
        try:
            h, w = aligned_face.shape[:2]
            
            # ìœ„ì—ì„œ ë³¸ ê°ë„ê°€ í¬ë©´ (vertical_diffê°€ í¬ë©´) ì–¼êµ´ì„ ì•½ê°„ ìœ„ë¡œ íšŒì „ì‹œí‚¨ ê²ƒì²˜ëŸ¼ ë³´ì •
            # perspective transformationì„ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ ìƒë‹¨ì„ ì•½ê°„ í™•ëŒ€í•˜ê³  í•˜ë‹¨ì„ ì•½ê°„ ì¶•ì†Œ
            # ì´ë ‡ê²Œ í•˜ë©´ ìœ„ì—ì„œ ë³¸ ê°ë„ê°€ ì¤„ì–´ë“  ê²ƒì²˜ëŸ¼ ë³´ì„
            
            # ë³´ì • ê°•ë„: vertical_diffê°€ í´ìˆ˜ë¡ ë” ê°•í•œ ë³´ì •
            # ì¼ë°˜ì ìœ¼ë¡œ ì •ë©´ ì–¼êµ´ì—ì„œ ì½”-ëˆˆ ê±°ë¦¬ëŠ” ì–¼êµ´ ë†’ì´ì˜ ì•½ 10-15% ì •ë„
            # ìœ„ì—ì„œ 30ë„ ê°ë„ë¡œ ì´¬ì˜ë˜ë©´ ì´ ê±°ë¦¬ê°€ 20-30% ì •ë„ë¡œ ì¦ê°€
            correction_strength = min(0.15, vertical_diff / h)  # ìµœëŒ€ 15% ë³´ì •
            
            if correction_strength < 0.05:  # ë³´ì •ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ìŠ¤í‚µ
                return aligned_face
            
            # Perspective transformationì„ ìœ„í•œ source points
            src_points = np.float32([
                [0, 0],           # ì™¼ìª½ ìƒë‹¨
                [w, 0],           # ì˜¤ë¥¸ìª½ ìƒë‹¨
                [w, h],           # ì˜¤ë¥¸ìª½ í•˜ë‹¨
                [0, h]            # ì™¼ìª½ í•˜ë‹¨
            ])
            
            # Destination points: ìƒë‹¨ì„ ì•½ê°„ í™•ëŒ€í•˜ê³  í•˜ë‹¨ì„ ì•½ê°„ ì¶•ì†Œ
            offset = int(w * correction_strength * 0.3)  # ë³´ì • ì˜¤í”„ì…‹
            dst_points = np.float32([
                [offset, 0],           # ì™¼ìª½ ìƒë‹¨ (ì•½ê°„ ì˜¤ë¥¸ìª½ìœ¼ë¡œ)
                [w - offset, 0],       # ì˜¤ë¥¸ìª½ ìƒë‹¨ (ì•½ê°„ ì™¼ìª½ìœ¼ë¡œ)
                [w - offset, h],       # ì˜¤ë¥¸ìª½ í•˜ë‹¨
                [offset, h]            # ì™¼ìª½ í•˜ë‹¨
            ])
            
            # Perspective transformation matrix
            M = cv2.getPerspectiveTransform(src_points, dst_points)
            
            # Transform ì ìš©
            corrected_face = cv2.warpPerspective(aligned_face, M, (w, h), 
                                                  flags=cv2.INTER_LINEAR,
                                                  borderMode=cv2.BORDER_REPLICATE)
            
            return corrected_face
            
        except Exception as e:
            logging.debug(f"pitch ê°ë„ ë³´ì • ì‹¤íŒ¨ (ì›ë³¸ ì‚¬ìš©): {e}")
            return aligned_face
    
    def _simple_align(self, frame: np.ndarray, kps: np.ndarray) -> Optional[np.ndarray]:
        """
        ArcFace í‘œì¤€ ì •ë ¬ (face_align.norm_cropê³¼ ë™ì¼í•œ ë°©ì‹)
        InsightFace utilsë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œ ëŒ€ì²´ êµ¬í˜„
        """
        try:
            # ArcFace í‘œì¤€ ëœë“œë§ˆí¬ ìœ„ì¹˜ (112x112 ê¸°ì¤€)
            # ì™¼ìª½ëˆˆ, ì˜¤ë¥¸ìª½ëˆˆ, ì½”, ì™¼ìª½ì…ê¼¬ë¦¬, ì˜¤ë¥¸ìª½ì…ê¼¬ë¦¬
            arcface_dst = np.array([
                [38.2946, 51.6963],
                [73.5318, 51.5014],
                [56.0252, 71.7366],
                [41.5493, 92.3655],
                [70.7299, 92.2041]
            ], dtype=np.float32)
            
            # ì…ë ¥ ëœë“œë§ˆí¬ (5ê°œ)
            src_pts = kps[:5].astype(np.float32)
            
            # Similarity Transform ê³„ì‚° (íšŒì „, ìŠ¤ì¼€ì¼, ì´ë™)
            # cv2.estimateAffinePartial2DëŠ” similarity transformì„ ê³„ì‚°
            tform, _ = cv2.estimateAffinePartial2D(src_pts, arcface_dst, method=cv2.LMEDS)
            
            if tform is None:
                # fallback: ë‹¨ìˆœ affine transform
                tform = cv2.getAffineTransform(src_pts[:3], arcface_dst[:3])
            
            if tform is None:
                logging.warning("_simple_align: Transform ê³„ì‚° ì‹¤íŒ¨")
                return None
            
            # Affine Transform ì ìš©í•˜ì—¬ 112x112ë¡œ ì •ë ¬
            aligned_face = cv2.warpAffine(
                frame, tform, (112, 112), 
                flags=cv2.INTER_LINEAR,
                borderMode=cv2.BORDER_REPLICATE
            )
            
            if aligned_face is None or aligned_face.size == 0:
                return None
            
            return aligned_face
            
        except Exception as e:
            logging.error(f"_simple_align ì˜¤ë¥˜: {e}", exc_info=True)
            return None
    
    def _get_embedding_from_onnx(self, aligned_face: np.ndarray) -> Optional[np.ndarray]:
        """
        ONNX ëª¨ë¸ë¡œ ì§ì ‘ ì„ë² ë”© ì¶”ì¶œ
        AdaFace ì „ì²˜ë¦¬ ë°©ì‹: BGR ì´ë¯¸ì§€, [0, 255] -> [0, 1] -> [-1, 1] ì •ê·œí™”
        """
        # ONNX ì„¸ì…˜ì´ ì—†ìœ¼ë©´ (TensorRTë§Œ ì‚¬ìš© ì¤‘) None ë°˜í™˜
        if self.session is None:
            return None
        
        try:
            # AdaFace ì „ì²˜ë¦¬ ë°©ì‹ ì ìš©
            # aligned_faceëŠ” BGR í˜•ì‹ (OpenCV ê¸°ë³¸)
            # 1. [0, 255] -> [0, 1] ì •ê·œí™”
            np_img = aligned_face.astype(np.float32) / 255.0
            
            # 2. [0, 1] -> [-1, 1] ì •ê·œí™”: ((img / 255.) - 0.5) / 0.5
            np_img = (np_img - 0.5) / 0.5
            
            # 3. BGR ìˆœì„œ ìœ ì§€ (AdaFaceëŠ” BGR ì…ë ¥ ì‚¬ìš©)
            # 4. (H, W, C) -> (1, C, H, W) ë³€í™˜
            tensor = np_img.transpose(2, 0, 1)[np.newaxis, :, :, :].astype(np.float32)
            
            # Inference (Only Recognition)
            import time
            t0 = time.time()
            
            # ì…ë ¥ í…ì„œ í˜•ì‹ í™•ì¸ (ë””ë²„ê¹…)
            logging.debug(f"AdaFace ONNX ì¶”ë¡  ì…ë ¥: shape={tensor.shape}, dtype={tensor.dtype}, min={tensor.min():.3f}, max={tensor.max():.3f}")
            
            # ONNX ì¶”ë¡  ì‹¤í–‰
            outputs = self.session.run([self.output_name], {self.input_name: tensor})
            t1 = time.time()
            
            # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
            inference_time_ms = (t1 - t0) * 1000
            logging.debug(f"AdaFace ONNX ì¶”ë¡  ì™„ë£Œ: {inference_time_ms:.2f}ms")
            
            # GPU 2ëŒ€ ìµœëŒ€ í™œìš©: ê²½ê³  ì„ê³„ê°’ ì¡°ì • (ì‹¤ì œ ë³‘ëª©ì€ 150ms ì´ìƒ)
            if inference_time_ms > 150:
                logging.warning(f"âš ï¸ AdaFace ì¶”ë¡  ëŠë¦¼: {inference_time_ms:.1f}ms (ëª©í‘œ: <100ms)")
            
            embedding = outputs[0]
            
            # ì¶œë ¥ í˜•ì‹ í™•ì¸ (ë””ë²„ê¹…)
            logging.debug(f"AdaFace ONNX ì¶”ë¡  ì¶œë ¥: shape={embedding.shape}, dtype={embedding.dtype}")
            
            # Flatten (ì´ë¯¸ (1, 512) í˜•íƒœì¼ ìˆ˜ ìˆìŒ)
            if embedding.ndim > 1:
                embedding = embedding.flatten()
            
            # ìµœì¢… í˜•ì‹ í™•ì¸
            if embedding.shape[0] != 512:
                logging.error(f"âŒ AdaFace ì„ë² ë”© ì°¨ì› ì˜¤ë¥˜: ì˜ˆìƒ=512, ì‹¤ì œ={embedding.shape[0]}")
                return None
            
            return embedding
            
        except Exception as e:
            logging.error(f"_get_embedding_from_onnx ì˜¤ë¥˜: {e}", exc_info=True)
            return None
    
    def _get_embedding_from_tensorrt(self, aligned_face: np.ndarray) -> Optional[np.ndarray]:
        """
        TensorRT ì—”ì§„ìœ¼ë¡œ ì§ì ‘ ì„ë² ë”© ì¶”ì¶œ (ONNX ëŒ€ë¹„ 12ë°° ë¹ ë¦„)
        ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ONNXë¡œ í´ë°±
        """
        try:
            # ì˜¬ë°”ë¥¸ CUDA ë””ë°”ì´ìŠ¤ë¡œ ì „í™˜
            if hasattr(self, 'trt_device_id'):
                torch.cuda.set_device(self.trt_device_id)
            
            # AdaFace ì „ì²˜ë¦¬
            np_img = aligned_face.astype(np.float32) / 255.0
            np_img = (np_img - 0.5) / 0.5
            np_img = np_img.transpose(2, 0, 1)  # HWC -> CHW
            
            # ì…ë ¥ í…ì„œì— ë³µì‚¬ (ë™ê¸°í™” í¬í•¨)
            input_tensor = torch.from_numpy(np_img).unsqueeze(0).to(self.trt_input_tensor.device)
            self.trt_input_tensor.copy_(input_tensor)
            torch.cuda.synchronize(self.trt_device_id)
            
            # TensorRT ì‹¤í–‰
            self.trt_context.set_tensor_address('input', self.trt_input_tensor.data_ptr())
            self.trt_context.set_tensor_address('output', self.trt_output_tensor.data_ptr())
            self.trt_context.set_tensor_address('onnx::Div_704', self.trt_output_div.data_ptr())
            
            with torch.cuda.stream(self.trt_stream):
                success = self.trt_context.execute_async_v3(self.trt_stream.cuda_stream)
            
            # ì™„ì „í•œ ë™ê¸°í™”
            self.trt_stream.synchronize()
            torch.cuda.synchronize(self.trt_device_id)
            
            if not success:
                logging.warning("TensorRT ì¶”ë¡  ì‹¤íŒ¨, ONNXë¡œ í´ë°±")
                self.use_tensorrt = False  # ë‹¤ìŒë¶€í„° ONNX ì‚¬ìš©
                return None
            
            embedding = self.trt_output_tensor.cpu().numpy()[0]
            return embedding
            
        except RuntimeError as e:
            # CUDA ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬
            error_msg = str(e)
            if "CUDA" in error_msg or "illegal memory" in error_msg:
                logging.error(f"âŒ CUDA ë©”ëª¨ë¦¬ ì—ëŸ¬ ë°œìƒ: {e}")
                logging.warning("TensorRT ë¹„í™œì„±í™”, ONNXë¡œ í´ë°±")
                self.use_tensorrt = False
                # CUDA ìƒíƒœ ë³µêµ¬ ì‹œë„
                try:
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
                except:
                    pass
            return None
        except Exception as e:
            logging.warning(f"TensorRT ì˜¤ë¥˜, ONNXë¡œ í´ë°±: {e}")
            self.use_tensorrt = False  # ë‹¤ìŒë¶€í„° ONNX ì‚¬ìš©
            return None
    
    def _get_embedding_from_insightface(
        self, 
        aligned_face: np.ndarray, 
        face_analyzer: Any
    ) -> Optional[np.ndarray]:
        """
        InsightFace ë¶„ì„ê¸°ë¡œ ì„ë² ë”© ì¶”ì¶œ
        """
        try:
            # rec_model ì ‘ê·¼
            rec_model = None
            if hasattr(face_analyzer, 'models') and 'recognition' in face_analyzer.models:
                rec_model = face_analyzer.models['recognition']
            elif hasattr(face_analyzer, 'rec_model'):
                rec_model = face_analyzer.rec_model
            
            if rec_model is None:
                logging.warning("rec_modelì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None
            
            # ì´ë¯¸ ì •ë ¬ëœ ì–¼êµ´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì§ì ‘ ì„ë² ë”© ì¶”ì¶œ
            embedding = rec_model.get_feat(aligned_face)
            
            if embedding is not None:
                # ì •ê·œí™” (L2 norm)
                embedding = embedding / np.linalg.norm(embedding)
            
            return embedding
            
        except Exception as e:
            logging.error(f"_get_embedding_from_insightface ì˜¤ë¥˜: {e}", exc_info=True)
            return None


