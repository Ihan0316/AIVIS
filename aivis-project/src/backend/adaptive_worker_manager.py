"""
ì ì‘í˜• ì›Œì»¤ ê´€ë¦¬ ì‹œìŠ¤í…œ
GPU ì‚¬ìš©ë¥  ë° ì§€ì—° ì‹œê°„ì„ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ì›Œì»¤ ìˆ˜ë¥¼ ìë™ ì¡°ì •
"""
import time
import logging
import threading
from collections import deque
from typing import Dict, Tuple, Optional
from dataclasses import dataclass
import torch

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    avg_processing_time_ms: float = 0.0
    queue_size: int = 0
    gpu_utilization: float = 0.0  # ì¶”ì • GPU ì‚¬ìš©ë¥  (0-100)
    latency_ms: float = 0.0
    fps: float = 0.0


class AdaptiveWorkerManager:
    """GPU ì‚¬ìš©ë¥  ë° ì§€ì—° ì‹œê°„ ê¸°ë°˜ ì›Œì»¤ ìˆ˜ ìë™ ì¡°ì •"""
    
    def __init__(
        self,
        initial_face_workers: int = 8,
        initial_yolo_workers: int = 10,
        initial_danger_workers: int = 6,
        initial_frame_workers: int = 12,
        min_workers: int = 2,
        max_workers: int = 30,
        adjustment_interval: float = 10.0  # 10ì´ˆë§ˆë‹¤ ì¡°ì •
    ):
        self.current_face_workers = initial_face_workers
        self.current_yolo_workers = initial_yolo_workers
        self.current_danger_workers = initial_danger_workers
        self.current_frame_workers = initial_frame_workers
        
        self.min_workers = min_workers
        self.max_workers = max_workers
        
        self.adjustment_interval = adjustment_interval if adjustment_interval > 0 else 30.0
        self.last_adjustment_time = time.time()
        
        # ì›Œì»¤ ìˆ˜ ë³€ê²½ ì„ê³„ê°’ (ë„ˆë¬´ ìì£¼ ë³€ê²½ ë°©ì§€)
        self.min_change_threshold = 3  # ìµœì†Œ 3ê°œ ì´ìƒ ì°¨ì´ë‚  ë•Œë§Œ ë³€ê²½ (Executor êµì²´ ë¹ˆë„ ê°ì†Œ)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬
        self.processing_times: deque = deque(maxlen=100)
        self.queue_sizes: deque = deque(maxlen=100)
        self.latencies: deque = deque(maxlen=100)
        
        self.lock = threading.Lock()
        
        # MPS ê°ì§€
        self.is_mps = hasattr(torch, "backends") and torch.backends.mps.is_available()
        
        logger.info(f"âœ… ì ì‘í˜• ì›Œì»¤ ê´€ë¦¬ì ì´ˆê¸°í™”: Face={initial_face_workers}, YOLO={initial_yolo_workers}, Danger={initial_danger_workers}, Frame={initial_frame_workers}")
    
    def update_metrics(
        self,
        processing_time_ms: float,
        queue_size: int = 0,
        latency_ms: float = 0.0,
        fps: float = 0.0
    ):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        with self.lock:
            if processing_time_ms > 0:
                self.processing_times.append(processing_time_ms)
            if queue_size >= 0:
                self.queue_sizes.append(queue_size)
            if latency_ms > 0:
                self.latencies.append(latency_ms)
    
    def _estimate_gpu_utilization(self) -> float:
        """GPU ì‚¬ìš©ë¥  ì¶”ì • (MPSëŠ” ì²˜ë¦¬ ì‹œê°„ê³¼ í í¬ê¸°ë¡œ ì¶”ì •)"""
        if not self.processing_times:
            return 0.0
        
        avg_processing_time = sum(self.processing_times) / len(self.processing_times)
        avg_queue_size = sum(self.queue_sizes) / len(self.queue_sizes) if self.queue_sizes else 0
        
        # ì²˜ë¦¬ ì‹œê°„ì´ ì§§ê³  íê°€ ë¹„ì–´ìˆìœ¼ë©´ GPU ì‚¬ìš©ë¥  ë‚®ìŒ
        # ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ê³  íê°€ ìŒ“ì´ë©´ GPU ì‚¬ìš©ë¥  ë†’ìŒ (ë¶€í•˜)
        if avg_processing_time < 50:  # 50ms ë¯¸ë§Œ: ë¹ ë¥¸ ì²˜ë¦¬
            if avg_queue_size < 2:
                return 30.0  # GPU ì—¬ìœ 
            else:
                return 60.0  # ì¤‘ê°„ ë¶€í•˜
        elif avg_processing_time < 100:  # 100ms ë¯¸ë§Œ
            if avg_queue_size < 5:
                return 60.0  # ì¤‘ê°„ ë¶€í•˜
            else:
                return 80.0  # ë†’ì€ ë¶€í•˜
        else:  # 100ms ì´ìƒ: ëŠë¦° ì²˜ë¦¬
            if avg_queue_size > 5:
                return 95.0  # ë§¤ìš° ë†’ì€ ë¶€í•˜
            else:
                return 75.0  # ë†’ì€ ë¶€í•˜
    
    def _calculate_optimal_workers(
        self,
        current_workers: int,
        gpu_util: float,
        avg_latency_ms: float,
        avg_queue_size: float,
        target_latency_ms: float = 100.0
    ) -> int:
        """ìµœì  ì›Œì»¤ ìˆ˜ ê³„ì‚°"""
        # GPU ì‚¬ìš©ë¥ ì´ ë‚®ê³  ì§€ì—°ì´ ë†’ìœ¼ë©´ ì›Œì»¤ ì¦ê°€
        # GPU ì‚¬ìš©ë¥ ì´ ë†’ê³  ì§€ì—°ì´ ë‚®ìœ¼ë©´ ì›Œì»¤ ê°ì†Œ
        
        # ì›Œì»¤ ìˆ˜ ë³€ê²½ ì„ê³„ê°’ ì ìš© (ë„ˆë¬´ ìì£¼ ë³€ê²½ ë°©ì§€)
        if gpu_util < 50 and avg_latency_ms > target_latency_ms * 1.5:
            # GPU ì—¬ìœ  + ì§€ì—° ë†’ìŒ: ì›Œì»¤ ì¦ê°€
            new_workers = min(self.max_workers, current_workers + 2)
            # ìµœì†Œ ë³€ê²½ ì„ê³„ê°’ ì²´í¬
            if abs(new_workers - current_workers) >= self.min_change_threshold:
                logger.info(f"ğŸ“ˆ ì›Œì»¤ ì¦ê°€: {current_workers} -> {new_workers} (GPU ì‚¬ìš©ë¥ : {gpu_util:.1f}%, ì§€ì—°: {avg_latency_ms:.1f}ms)")
                return new_workers
        elif gpu_util < 50 and avg_queue_size > 5:  # í ì„ê³„ê°’ ì¦ê°€ (3 -> 5)
            # GPU ì—¬ìœ  + í ìŒ“ì„: ì›Œì»¤ ì¦ê°€
            new_workers = min(self.max_workers, current_workers + 1)
            # ìµœì†Œ ë³€ê²½ ì„ê³„ê°’ ì²´í¬
            if abs(new_workers - current_workers) >= self.min_change_threshold:
                logger.info(f"ğŸ“ˆ ì›Œì»¤ ì¦ê°€: {current_workers} -> {new_workers} (GPU ì‚¬ìš©ë¥ : {gpu_util:.1f}%, í í¬ê¸°: {avg_queue_size:.1f})")
                return new_workers
        elif gpu_util > 85 and avg_latency_ms < target_latency_ms * 0.8:
            # GPU ê³¼ë¶€í•˜ + ì§€ì—° ë‚®ìŒ: ì›Œì»¤ ê°ì†Œ
            new_workers = max(self.min_workers, current_workers - 1)
            # ìµœì†Œ ë³€ê²½ ì„ê³„ê°’ ì²´í¬
            if abs(new_workers - current_workers) >= self.min_change_threshold:
                logger.info(f"ğŸ“‰ ì›Œì»¤ ê°ì†Œ: {current_workers} -> {new_workers} (GPU ì‚¬ìš©ë¥ : {gpu_util:.1f}%, ì§€ì—°: {avg_latency_ms:.1f}ms)")
                return new_workers
        elif gpu_util > 90:
            # GPU ë§¤ìš° ê³¼ë¶€í•˜: ì›Œì»¤ ê°ì†Œ
            new_workers = max(self.min_workers, current_workers - 2)
            # ìµœì†Œ ë³€ê²½ ì„ê³„ê°’ ì²´í¬
            if abs(new_workers - current_workers) >= self.min_change_threshold:
                logger.info(f"ğŸ“‰ ì›Œì»¤ ê°ì†Œ: {current_workers} -> {new_workers} (GPU ì‚¬ìš©ë¥ : {gpu_util:.1f}%)")
                return new_workers
        
        # í˜„ì¬ ìƒíƒœ ìœ ì§€
        return current_workers
    
    def adjust_workers(self) -> Tuple[int, int, int, int]:
        """ì›Œì»¤ ìˆ˜ ìë™ ì¡°ì •"""
        current_time = time.time()
        
        # ì¡°ì • ê°„ê²© ì²´í¬
        if current_time - self.last_adjustment_time < self.adjustment_interval:
            return (
                self.current_face_workers,
                self.current_yolo_workers,
                self.current_danger_workers,
                self.current_frame_workers
            )
        
        with self.lock:
            if not self.processing_times:
                return (
                    self.current_face_workers,
                    self.current_yolo_workers,
                    self.current_danger_workers,
                    self.current_frame_workers
                )
            
            # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
            avg_processing_time = sum(self.processing_times) / len(self.processing_times)
            avg_queue_size = sum(self.queue_sizes) / len(self.queue_sizes) if self.queue_sizes else 0
            avg_latency = sum(self.latencies) / len(self.latencies) if self.latencies else avg_processing_time
            
            # GPU ì‚¬ìš©ë¥  ì¶”ì •
            gpu_util = self._estimate_gpu_utilization()
            
            # ê° ì›Œì»¤ íƒ€ì…ë³„ ì¡°ì •
            # YOLO ì›Œì»¤: GPU ë¶€í•˜ì— ê°€ì¥ ë¯¼ê°
            new_yolo_workers = self._calculate_optimal_workers(
                self.current_yolo_workers,
                gpu_util,
                avg_latency,
                avg_queue_size,
                target_latency_ms=100.0
            )
            
            # Face ì›Œì»¤: YOLO ì›Œì»¤ì˜ 70-80% ìˆ˜ì¤€
            new_face_workers = self._calculate_optimal_workers(
                self.current_face_workers,
                gpu_util * 0.8,  # FaceëŠ” GPU ë¶€í•˜ê°€ ì•½ê°„ ë‚®ìŒ
                avg_latency,
                avg_queue_size,
                target_latency_ms=150.0
            )
            
            # Danger ì›Œì»¤: YOLO ì›Œì»¤ì˜ 50-60% ìˆ˜ì¤€
            new_danger_workers = max(
                self.min_workers,
                int(new_yolo_workers * 0.6)
            )
            
            # Frame ì›Œì»¤: CPU ê¸°ë°˜ì´ë¯€ë¡œ í í¬ê¸°ì— ë”°ë¼ ì¡°ì •
            if avg_queue_size > 5:
                new_frame_workers = min(self.max_workers, self.current_frame_workers + 2)
            elif avg_queue_size < 2:
                new_frame_workers = max(self.min_workers, self.current_frame_workers - 1)
            else:
                new_frame_workers = self.current_frame_workers
            
            # ë³€ê²½ì‚¬í•­ ì ìš©
            changed = False
            if new_yolo_workers != self.current_yolo_workers:
                self.current_yolo_workers = new_yolo_workers
                changed = True
            if new_face_workers != self.current_face_workers:
                self.current_face_workers = new_face_workers
                changed = True
            if new_danger_workers != self.current_danger_workers:
                self.current_danger_workers = new_danger_workers
                changed = True
            if new_frame_workers != self.current_frame_workers:
                self.current_frame_workers = new_frame_workers
                changed = True
            
            if changed:
                logger.info(
                    f"ğŸ”„ ì›Œì»¤ ìˆ˜ ìë™ ì¡°ì • ì™„ë£Œ: "
                    f"Face={self.current_face_workers}, "
                    f"YOLO={self.current_yolo_workers}, "
                    f"Danger={self.current_danger_workers}, "
                    f"Frame={self.current_frame_workers} "
                    f"(GPU ì‚¬ìš©ë¥ : {gpu_util:.1f}%, ì§€ì—°: {avg_latency:.1f}ms, í: {avg_queue_size:.1f})"
                )
            
            self.last_adjustment_time = current_time
            
            return (
                self.current_face_workers,
                self.current_yolo_workers,
                self.current_danger_workers,
                self.current_frame_workers
            )
    
    def get_current_workers(self) -> Tuple[int, int, int, int]:
        """í˜„ì¬ ì›Œì»¤ ìˆ˜ ë°˜í™˜"""
        with self.lock:
            return (
                self.current_face_workers,
                self.current_yolo_workers,
                self.current_danger_workers,
                self.current_frame_workers
            )
    
    def get_metrics(self) -> PerformanceMetrics:
        """í˜„ì¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        with self.lock:
            avg_processing_time = sum(self.processing_times) / len(self.processing_times) if self.processing_times else 0.0
            avg_queue_size = sum(self.queue_sizes) / len(self.queue_sizes) if self.queue_sizes else 0
            avg_latency = sum(self.latencies) / len(self.latencies) if self.latencies else avg_processing_time
            gpu_util = self._estimate_gpu_utilization()
            
            return PerformanceMetrics(
                avg_processing_time_ms=avg_processing_time,
                queue_size=int(avg_queue_size),
                gpu_utilization=gpu_util,
                latency_ms=avg_latency,
                fps=1000.0 / avg_processing_time if avg_processing_time > 0 else 0.0
            )


# ì „ì—­ ì ì‘í˜• ì›Œì»¤ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤
_adaptive_worker_manager: Optional[AdaptiveWorkerManager] = None
_manager_lock = threading.Lock()


def get_adaptive_worker_manager() -> Optional[AdaptiveWorkerManager]:
    """ì ì‘í˜• ì›Œì»¤ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    return _adaptive_worker_manager


def initialize_adaptive_worker_manager(
    initial_face_workers: int = 8,
    initial_yolo_workers: int = 10,
    initial_danger_workers: int = 6,
    initial_frame_workers: int = 12
) -> AdaptiveWorkerManager:
    """ì ì‘í˜• ì›Œì»¤ ê´€ë¦¬ì ì´ˆê¸°í™”"""
    global _adaptive_worker_manager
    with _manager_lock:
        if _adaptive_worker_manager is None:
            _adaptive_worker_manager = AdaptiveWorkerManager(
                initial_face_workers=initial_face_workers,
                initial_yolo_workers=initial_yolo_workers,
                initial_danger_workers=initial_danger_workers,
                initial_frame_workers=initial_frame_workers
            )
        return _adaptive_worker_manager

