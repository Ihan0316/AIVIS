# utils_server.py - ì„œë²„ ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
"""
ì„œë²„ ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ëª¨ë“ˆ
GPU ëª¨ë‹ˆí„°ë§, ì‘ë‹µ ì••ì¶•, ë°ì´í„° í•„í„°ë§ ë“±
"""
import logging
import json
import gzip
from typing import Dict, Any

import torch
from aiohttp import web


def get_gpu_usage_stats() -> Dict[int, Dict[str, Any]]:
    """GPU ì‚¬ìš©ëŸ‰ í†µê³„ ê°€ì ¸ì˜¤ê¸°"""
    if not torch.cuda.is_available():
        return {}
    
    stats: Dict[int, Dict[str, Any]] = {}
    try:
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
            memory_reserved = torch.cuda.memory_reserved(i) / 1024**3  # GB
            memory_total = props.total_memory / 1024**3  # GB
            memory_free = memory_total - memory_reserved
            
            stats[i] = {
                "name": props.name,
                "memory_allocated_gb": memory_allocated,
                "memory_reserved_gb": memory_reserved,
                "memory_total_gb": memory_total,
                "memory_free_gb": memory_free,
                "memory_util_percent": (memory_reserved / memory_total) * 100 if memory_total > 0 else 0
            }
    except Exception as e:
        logging.warning(f"GPU ì‚¬ìš©ëŸ‰ ì²´í¬ ì‹¤íŒ¨: {e}")
    
    return stats


def log_gpu_optimization_recommendations(
    stats_lock: Any,
    system_stats: Dict[str, Any],
    default_face_workers: int,
    default_yolo_workers: int
) -> None:
    """GPU ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ìµœì í™” ê¶Œì¥ì‚¬í•­ ì¶œë ¥"""
    if not torch.cuda.is_available():
        return
    
    stats = get_gpu_usage_stats()
    if not stats:
        return
    
    logging.info("=" * 80)
    logging.info("ğŸ“Š GPU ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§")
    logging.info("=" * 80)
    
    for gpu_id, gpu_stat in stats.items():
        mem_util = gpu_stat["memory_util_percent"]
        mem_free = gpu_stat["memory_free_gb"]
        
        logging.info(f"GPU {gpu_id}: {gpu_stat['name']}")
        logging.info(f"  ë©”ëª¨ë¦¬: {gpu_stat['memory_reserved_gb']:.2f}GB / {gpu_stat['memory_total_gb']:.2f}GB ({mem_util:.1f}%)")
        logging.info(f"  ì—¬ìœ  ë©”ëª¨ë¦¬: {mem_free:.2f}GB")
        
        # ìµœì í™” ê¶Œì¥ì‚¬í•­
        if mem_util > 90:
            logging.warning(f"  âš ï¸ GPU {gpu_id} ë©”ëª¨ë¦¬ ë¶€ì¡±! ë°°ì¹˜ í¬ê¸° ê°ì†Œ ë˜ëŠ” ëª¨ë¸ ì…ë ¥ í¬ê¸° ê°ì†Œ ê³ ë ¤")
        elif mem_util < 50 and mem_free > 2:
            logging.info(f"  ğŸ’¡ GPU {gpu_id} ë©”ëª¨ë¦¬ ì—¬ìœ : ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê°€ëŠ¥")
        
        if gpu_id == 0:
            # GPU 0 (YOLO): ì›Œì»¤ ìˆ˜ ì²´í¬
            if mem_util > 85:
                logging.warning(f"  âš ï¸ GPU 0 ê³¼ë¶€í•˜: YOLO ì›Œì»¤ ìˆ˜ ê°ì†Œ ê³ ë ¤ (í˜„ì¬: {default_yolo_workers})")
        elif gpu_id == 1:
            # GPU 1 (Face): ì›Œì»¤ ìˆ˜ ì²´í¬
            if mem_util > 85:
                logging.warning(f"  âš ï¸ GPU 1 ê³¼ë¶€í•˜: Face ì›Œì»¤ ìˆ˜ ê°ì†Œ ê³ ë ¤ (í˜„ì¬: {default_face_workers})")
    
    # ë©€í‹° GPU ê· í˜• ì²´í¬
    if len(stats) >= 2:
        gpu0_util = stats[0]["memory_util_percent"]
        gpu1_util = stats[1]["memory_util_percent"]
        util_diff = abs(gpu0_util - gpu1_util)
        
        if util_diff > 30:
            logging.warning(f"  âš ï¸ GPU ì‚¬ìš©ë¥  ë¶ˆê· í˜•: GPU0={gpu0_util:.1f}%, GPU1={gpu1_util:.1f}% (ì°¨ì´: {util_diff:.1f}%)")
            logging.warning(f"     â†’ ì›Œì»¤ ìˆ˜ ì¬ì¡°ì • ê³ ë ¤ (í˜„ì¬: YOLO={default_yolo_workers}, Face={default_face_workers})")
        else:
            logging.info(f"  âœ… GPU ì‚¬ìš©ë¥  ê· í˜•: GPU0={gpu0_util:.1f}%, GPU1={gpu1_util:.1f}%")
        
        # GPU 1 ì‚¬ìš©ë¥ ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê²½ê³ 
        if gpu1_util < 15:
            logging.warning(f"  âš ï¸ GPU 1 (ì–¼êµ´ ì¸ì‹) ì‚¬ìš©ë¥ ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤: {gpu1_util:.1f}%")
            logging.warning(f"     â†’ ì–¼êµ´ ì¸ì‹ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            logging.warning(f"     â†’ Face ì›Œì»¤ ìˆ˜ ì¦ê°€ ê³ ë ¤ (í˜„ì¬: {default_face_workers})")
    
    logging.info("=" * 80)
    
    # í†µê³„ ì €ì¥
    with stats_lock:
        system_stats["gpu_stats"] = stats


def create_compressed_response(data: Dict[str, Any], content_type: str = 'application/json') -> web.Response:
    """gzip ì••ì¶•ëœ JSON ì‘ë‹µ ìƒì„±"""
    try:
        json_data = json.dumps(data, ensure_ascii=False).encode('utf-8')
        
        compressed_data = gzip.compress(json_data)
        
        response = web.Response(
            body=compressed_data,
            content_type=content_type,
            headers={
                'Content-Encoding': 'gzip',
                'Content-Length': str(len(compressed_data)),
                'Cache-Control': 'public, max-age=60',  # 1ë¶„ ìºì‹œ
                'Vary': 'Accept-Encoding'
            }
        )
        return response
    except Exception as e:
        logging.error(f"ì••ì¶• ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        # í´ë°±: ì¼ë°˜ JSON ì‘ë‹µ
        return web.json_response(data)


def filter_model_results(data: Dict[str, Any]) -> Dict[str, Any]:
    """ëª¨ë¸ ê²°ê³¼ ë°ì´í„° í•„í„°ë§ - í•„ìš”í•œ ë°ì´í„°ë§Œ ë°˜í™˜"""
    try:
        filtered_data = {
            "alerts": data.get("alerts", [])[-10:],  # ìµœê·¼ 10ê°œ ì•Œë¦¼ë§Œ
            "violations": data.get("violations", {}),
            "heatmap_counts": data.get("heatmap_counts", {}),
            "profile": data.get("profile", {}),
            "logs": data.get("logs", [])[-20:],  # ìµœê·¼ 20ê°œ ë¡œê·¸ë§Œ
            "kpi_data": data.get("kpi_data", {}),
            "detected_workers": data.get("detected_workers", {})
        }
        
        # ë¹ˆ ë°ì´í„° ì œê±°
        filtered_data = {k: v for k, v in filtered_data.items() if v}
        
        return filtered_data
    except Exception as e:
        logging.error(f"ëª¨ë¸ ê²°ê³¼ í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return data

