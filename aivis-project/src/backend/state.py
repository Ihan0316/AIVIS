# state.py - ì „ì—­ ìƒíƒœ ê´€ë¦¬
"""
ì „ì—­ ë³€ìˆ˜ ë° ìƒíƒœ ê´€ë¦¬ ëª¨ë“ˆ
ëª¨ë“  ì „ì—­ ìƒíƒœë¥¼ ì¤‘ì•™ì—ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""
import threading
import time
import queue
from typing import Dict, Set, Optional, Any
from collections import defaultdict

import os
import logging
import asyncio
from aiohttp import web
from cache_manager import IdentityCache, TTLCache
from concurrent.futures import ThreadPoolExecutor
import config
import torch
import threading

# í”„ë ˆì„ ì €ì¥
latest_frames: Dict[int, bytes] = {}  # ì²˜ë¦¬ëœ í”„ë ˆì„ ì €ì¥
latest_result_data: Dict[int, dict] = {}  # ìµœì‹  ê²°ê³¼ ë°ì´í„° ì €ì¥ (ëŒ€ì‹œë³´ë“œìš©)
frame_lock = threading.Lock()

# í”„ë ˆì„ ì²˜ë¦¬ ë™ì‹œì„± ì œì–´
processing_lock = threading.Lock()  # í”„ë ˆì„ ì²˜ë¦¬ ë™ì‹œì„± ì œì–´ìš©
processing_flags: Dict[int, bool] = {}  # cam_idë³„ ì²˜ë¦¬ ì¤‘ í”Œë˜ê·¸

# í”„ë ˆì„ í ì‹œìŠ¤í…œ (ìµœì‹  í”„ë ˆì„ ìš°ì„  ì²˜ë¦¬, ë”œë ˆì´ ìµœì†Œí™”)
frame_queues: Dict[int, queue.Queue] = {}  # cam_idë³„ í”„ë ˆì„ í
# í”„ë ˆì„ ìœ ì§€ìœ¨ ìµœëŒ€í™”: í í¬ê¸° ì¦ê°€ (MPS í™˜ê²½ ìµœì í™”: 10 -> 20, í”„ë ˆì„ ë“œë¡­ ë°©ì§€)
MAX_QUEUE_SIZE = int(os.getenv('MAX_QUEUE_SIZE', '20'))  # 10 -> 20 (MPS í™˜ê²½ í”„ë ˆì„ ìœ ì§€ìœ¨ í–¥ìƒ)
queue_lock = threading.Lock()

# í”„ë ˆì„ ê°„ê²© ì œì–´ (íŠ ë°©ì§€)
last_frame_processed_time: Dict[int, float] = {}  # cam_idë³„ ë§ˆì§€ë§‰ í”„ë ˆì„ ì²˜ë¦¬ ì‹œê°„
# í”„ë ˆì„ ìœ ì§€ìœ¨ ìµœëŒ€í™”: í”„ë ˆì„ ê°„ê²© ìµœì†Œí™” (ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬)
MIN_FRAME_INTERVAL = 1.0 / 30.0  # ìµœì†Œ í”„ë ˆì„ ê°„ê²© (30 FPS ê¸°ì¤€, ì•½ 33.33ms - í”„ë ˆì„ ë“œë¡­ ë°©ì§€)
frame_interval_lock = threading.Lock()

# WebSocket ì—°ê²° ê´€ë¦¬
connected_websockets: Set[web.WebSocketResponse] = set()
dashboard_websockets: Set[web.WebSocketResponse] = set()  # ëŒ€ì‹œë³´ë“œ ì „ìš© ì—°ê²°

# SafetySystem ë° StorageManager (ë‚˜ì¤‘ì— ì´ˆê¸°í™”ë¨)
safety_system_instance: Optional[Any] = None  # core.SafetySystem íƒ€ì…
safety_system_lock = threading.Lock()  # SafetySystem ì ‘ê·¼ìš© ë½ (ë©€í‹°ìŠ¤ë ˆë“œ ì•ˆì „ì„±)
storage_manager: Optional[Any] = None  # LocalStorageManager íƒ€ì…

# í”„ë ˆì„ ì²˜ë¦¬ íƒœìŠ¤í¬ ì¶”ì  (WebSocket ì—°ê²°ë³„)
processing_tasks: Dict[int, Dict[int, asyncio.Task]] = {}  # {cam_id: {client_id: task}}
processing_tasks_lock = asyncio.Lock()  # íƒœìŠ¤í¬ ì¶”ì ìš© ë½

# ëŒ€ì‹œë³´ë“œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì œì–´ (ê¹œë¹¡ê±°ë¦¼ ì™„í™”ìš©)
DASHBOARD_BROADCAST_INTERVAL = float(os.getenv("DASHBOARD_BROADCAST_INTERVAL", "0.5"))
last_dashboard_broadcast_ts = 0.0
last_dashboard_payload = ""
dashboard_broadcast_lock: Optional[asyncio.Lock] = None

# í”„ë ˆì„ ì¶”ì  í†µê³„ (ì‹¤ì‹œê°„ FPS ì¸¡ì •ìš©)
frame_stats: Dict[int, dict] = {}  # cam_idë³„ í”„ë ˆì„ í†µê³„
frame_stats_lock = threading.Lock()

# ì–¼êµ´ íƒì§€ ìµœì í™”ë¥¼ ìœ„í•œ í”„ë ˆì„ ì¶”ì  (CCTV íš¨ìœ¨ ì¸ì‹ ì„¤ì •)
last_face_detection_frame: Dict[int, int] = {}  # cam_idë³„ ë§ˆì§€ë§‰ ì–¼êµ´ íƒì§€ í”„ë ˆì„ ë²ˆí˜¸
face_detection_lock = threading.Lock()
face_recognition_cooldown_ts: Dict[int, float] = defaultdict(lambda: 0.0)

# ìµœê·¼ ì‹ë³„ ê²°ê³¼ ìºì‹œ (ë¼ë²¨ ì•ˆì •í™”) - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
# IdentityCache ì‚¬ìš©: ìë™ í¬ê¸° ì œí•œ ë° TTL ê´€ë¦¬
MAX_IDENTITY_CACHE_PER_CAM = 30  # 50 -> 30 (ë©€í‹°ìº  ë©”ëª¨ë¦¬ ìµœì í™”)
recent_identity_cache = IdentityCache(
    max_items_per_cam=MAX_IDENTITY_CACHE_PER_CAM,
    ttl=config.Thresholds.RECOGNITION_HOLD_SECONDS
)

# ë§ˆì§€ë§‰ ë Œë”ë§ëœ ë°•ìŠ¤/ë¼ë²¨ ìºì‹œ (ë Œë”ë§ ë³´ê°•)
# TTLCache ì‚¬ìš©: ìë™ ë§Œë£Œ ì²˜ë¦¬
# cam_id -> TTLCache['render' -> {'items': List[{box: (x1,y1,x2,y2), name: str}]}]
last_render_cache: Dict[int, TTLCache] = defaultdict(lambda: TTLCache(default_ttl=2.0))

# ì„¼íŠ¸ë¡œì´ë“œ ì„ë² ë”© ë²„í¼ (finalì˜ ê°œì„  ê¸°ë²• ë„ì…)
# cam_id -> person_box_key -> {'embeddings': [embedding1, ...], 'last_update': timestamp}
# person_box_keyëŠ” IoU ê¸°ë°˜ìœ¼ë¡œ ê°™ì€ ì‚¬ëŒì„ ì‹ë³„í•˜ëŠ” í‚¤
embedding_buffers: Dict[int, Dict[str, dict]] = defaultdict(dict)
EMBEDDING_BUFFER_SIZE = 5  # 3 -> 5ê°œ í”„ë ˆì„ í‰ê·  (ì •í™•ë„ í–¥ìƒ)
EMBEDDING_BUFFER_MIN_SIZE = 2  # ìµœì†Œ 2ê°œ ìˆì–´ì•¼ ì„¼íŠ¸ë¡œì´ë“œ ê³„ì‚° (ë¹ ë¥¸ ì¸ì‹ì„ ìœ„í•´ 2ë¡œ ì¡°ì •)
MAX_EMBEDDING_BUFFERS_PER_CAM = 20  # 50 -> 20 (ë©€í‹°ìº  ë©”ëª¨ë¦¬ ìµœì í™”)

# ë„˜ì–´ì§ ê°ì§€ ì‹œê°„ ì¶”ì  (finalì˜ ê°œì„  ê¸°ë²• ë„ì…)
# cam_id -> person_box_key -> fall_start_time
fall_start_times: Dict[int, Dict[str, float]] = defaultdict(dict)
FALL_DURATION_THRESHOLD = 0.5  # finalê³¼ ë™ì¼: 0.5ì´ˆ ì§€ì† ì‹œ ë„˜ì–´ì§ íŒì •

# ì„¼íŠ¸ë¡œì´ë“œ ê²°ê³¼ ìºì‹œ (ì¬ì‚¬ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ)
# TTLCache ì‚¬ìš©: ìë™ ë§Œë£Œ ì²˜ë¦¬
# cam_id -> TTLCache[person_box_key -> {'name': str, 'score': float}]
CENTROID_CACHE_TTL = 2.0  # 2ì´ˆê°„ ìºì‹œ ìœ ì§€
centroid_cache: Dict[int, TTLCache] = defaultdict(lambda: TTLCache(default_ttl=CENTROID_CACHE_TTL))

# ì–¼êµ´ ë°”ìš´ë”©ë°•ìŠ¤ ìºì‹œ (ê¹œë¹¡ì„ ë°©ì§€)
# TTLCache ì‚¬ìš©: ìë™ ë§Œë£Œ ì²˜ë¦¬
# cam_id -> TTLCache[person_box_key -> {'face_bbox': (x1,y1,x2,y2), 'person_box': (x1,y1,x2,y2)}]
FACE_BBOX_CACHE_TTL = 2.0  # 1.0 -> 2.0ì´ˆ (ë°”ìš´ë”© ë°•ìŠ¤ ì•ˆì •í™”, ê¹œë¹¡ì„ ë°©ì§€, í™€ë“œ ì‹œê°„ê³¼ í†µì¼)
face_bbox_cache: Dict[int, TTLCache] = defaultdict(lambda: TTLCache(default_ttl=FACE_BBOX_CACHE_TTL))

# ëª¨ë¸ ê²°ê³¼ ë°ì´í„° (finalê³¼ ë™ì¼í•œ êµ¬ì¡°)
model_results = {
    "alerts": [],
    "violations": {},
    "heatmap_counts": {"A-1": 0, "A-2": 0, "B-1": 0, "B-2": 0},
    "profile": {"name": "ì‹œìŠ¤í…œ", "status": "ì •ìƒ", "area": "ì „ì²´"},
    "logs": [],
    "kpi_data": {"totalWorkers": 0, "attendees": 0, "ppeRate": 0, "riskLevel": 0},
    "detected_workers": {}  # êµ¬ì—­ë³„ ê°ì§€ëœ ì‘ì—…ì ì •ë³´
}
results_lock = threading.Lock()

# ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€ë¥¼ ìœ„í•œ ìµœê·¼ ì•Œë¦¼ ì¶”ì 
# ì‹¬ê°í•œ ìœ„ë°˜(ë„˜ì–´ì§, ì‚¬ê³ )ì€ ì¦‰ì‹œ ì•Œë¦¼, PPE ìœ„ë°˜ì€ ì¿¨ë‹¤ìš´ ì ìš©
recent_alerts_cache: Dict[str, float] = {}  # key: "{worker}|{area}|{violation_types}", value: timestamp
ALERT_COOLDOWN_SECONDS = 30.0  # PPE ìœ„ë°˜ ì¿¨ë‹¤ìš´ (30ì´ˆ)
CRITICAL_VIOLATIONS = ["ë„˜ì–´ì§", "ì‚¬ê³ ", "FALL", "ACCIDENT"]  # ì¦‰ì‹œ ì•Œë¦¼ ìœ„ë°˜ (ì¿¨ë‹¤ìš´ ì—†ìŒ)

# ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§
system_stats = {
    "start_time": time.time(),
    "total_requests": 0,
    "error_count": 0,
    "last_health_check": time.time(),
    "memory_usage": 0,
    "cpu_usage": 0,
    "response_times": [],  # ì‘ë‹µ ì‹œê°„ ì¶”ì  (ìµœê·¼ 100ê°œ)
    "gpu_stats": {}  # GPU ì‚¬ìš©ëŸ‰ í†µê³„
}
stats_lock = threading.Lock()
MAX_RESPONSE_TIMES = 100  # ìµœê·¼ ì‘ë‹µ ì‹œê°„ ìµœëŒ€ ì €ì¥ ê°œìˆ˜

# í”„ë ˆì„ ì²˜ë¦¬ ìƒíƒœ ê´€ë¦¬ (cam_idë³„) - í•¨ìˆ˜ ì†ì„± ëŒ€ì‹  ì¤‘ì•™ ê´€ë¦¬
# ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ ë° ë©€í‹°ìŠ¤ë ˆë“œ ì•ˆì „ì„± í–¥ìƒ
frame_processing_state: Dict[int, Dict[str, Any]] = defaultdict(dict)
frame_processing_state_lock = threading.Lock()

def get_frame_processing_state(cam_id: int) -> Dict[str, Any]:
    """cam_idë³„ í”„ë ˆì„ ì²˜ë¦¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ì´ˆê¸°í™”)"""
    with frame_processing_state_lock:
        if cam_id not in frame_processing_state:
            frame_processing_state[cam_id] = {
                'frame_count': 0,
                'used_ppe_boxes': set(),
                'last_cleanup_time': 0.0,
                'perf_log_count': 0,
                'model_warmed_up': False
            }
        return frame_processing_state[cam_id]

def clear_frame_processing_state(cam_id: int) -> None:
    """cam_idë³„ í”„ë ˆì„ ì²˜ë¦¬ ìƒíƒœ ì´ˆê¸°í™”"""
    with frame_processing_state_lock:
        if cam_id in frame_processing_state:
            frame_processing_state[cam_id].clear()
            frame_processing_state[cam_id] = {
                'frame_count': 0,
                'used_ppe_boxes': set(),
                'last_cleanup_time': 0.0,
                'perf_log_count': 0,
                'model_warmed_up': False
            }

# ThreadPoolExecutor ì¸ìŠ¤í„´ìŠ¤ (AI ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰ìš©)

# GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì›Œì»¤ ìˆ˜ ê³„ì‚° (main.pyì™€ ë™ì¼í•œ ë¡œì§)
def _calculate_optimal_workers():
    """GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ìµœì ì˜ ì›Œì»¤ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    if torch.cuda.is_available():
        try:
            gpu_count = torch.cuda.device_count()
            
            if gpu_count >= 2:
                # ë©€í‹° GPU: ê° GPUì˜ ë©”ëª¨ë¦¬ë¥¼ í•©ì‚°í•˜ì—¬ ê³„ì‚°
                total_memory_gb = sum(torch.cuda.get_device_properties(i).total_memory / (1024**3) for i in range(gpu_count))
                avg_memory_gb = total_memory_gb / gpu_count
                
                # ë©€í‹° GPU: GPU ì—¬ìœ  í™œìš©í•˜ì—¬ ì„±ëŠ¥ ê°œì„ 
                # GPU ì‚¬ìš©ë¥ ì´ ë‚®ìœ¼ë¯€ë¡œ ì›Œì»¤ ìˆ˜ ëŒ€í­ ì¦ê°€ (GPU í™œìš©ë¥  í–¥ìƒ)
                # ì²˜ë¦¬ ì‹œê°„(97ms) > í”„ë ˆì„ ê°„ê²©(33ms)ì´ë¯€ë¡œ ë” ë§ì€ ì›Œì»¤ í•„ìš”
                if avg_memory_gb >= 10:
                    # ì›Œì»¤ ìˆ˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ CPU ì˜¤ë²„í—¤ë“œ(Context Switching) ë°œìƒ -> 8ê°œë¡œ ìµœì í™”
                    face_workers = 8   # 16 â†’ 8 (ìµœì í™”)
                    yolo_workers = 8   # 16 â†’ 8 (ìµœì í™”)
                    danger_workers = 6  # 10 â†’ 6
                    frame_workers = 16  # 24 â†’ 16
                else:
                    face_workers = 6   # 14 â†’ 6
                    yolo_workers = 6   # 14 â†’ 6
                    danger_workers = 6  # 10 â†’ 6
                    frame_workers = 12  # 26 â†’ 12
                
                logging.info(f"ë©€í‹° GPU ê°ì§€ ({gpu_count}ê°œ) - ì›Œì»¤ ìˆ˜: Face={face_workers}, YOLO={yolo_workers}, Danger={danger_workers}, Frame={frame_workers}")
                return face_workers, yolo_workers, danger_workers, frame_workers
            else:
                # ë‹¨ì¼ GPU
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                
                if gpu_memory_gb >= 16:
                    face_workers = 8
                    yolo_workers = 8
                    danger_workers = 6
                    frame_workers = 12
                elif gpu_memory_gb >= 10:
                    face_workers = 6
                    yolo_workers = 6
                    danger_workers = 4
                    frame_workers = 10
                else:
                    face_workers = 4
                    yolo_workers = 4
                    danger_workers = 3
                    frame_workers = 8
                
                logging.info(f"ë‹¨ì¼ GPU ê°ì§€ ({gpu_memory_gb:.1f}GB) - ì›Œì»¤ ìˆ˜: Face={face_workers}, YOLO={yolo_workers}, Danger={danger_workers}, Frame={frame_workers}")
                return face_workers, yolo_workers, danger_workers, frame_workers
        except Exception as e:
            logging.warning(f"GPU ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            return 4, 4, 3, 8
    elif hasattr(torch, "backends") and torch.backends.mps.is_available():
        # MPS (Apple Silicon) - M4 Pro ìµœì í™”
        try:
            import subprocess
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                   capture_output=True, text=True, timeout=1)
            cpu_info = result.stdout.strip() if result.returncode == 0 else ""
            is_m4_pro = 'M4' in cpu_info and 'Pro' in cpu_info
            
            if is_m4_pro:
                # M4 Pro: 20ì½”ì–´ GPU, 14ì½”ì–´ CPU
                # ì„±ëŠ¥ ìµœëŒ€í™”: ì›Œì»¤ ìˆ˜ ê·¹ëŒ€í™”ë¡œ GPU ë³‘ë ¬ ì²˜ë¦¬ ê·¹ëŒ€í™” (ì¸ì› ì¦ê°€ ëŒ€ì‘)
                face_workers = 18  # ì–¼êµ´ ì¸ì‹ ì›Œì»¤ (16 -> 18, ì¸ì› ì¦ê°€ ëŒ€ì‘)
                yolo_workers = 22  # YOLO ëª¨ë¸ ì›Œì»¤ (20 -> 22, GPU ì½”ì–´ ìˆ˜ í™œìš©)
                danger_workers = 14  # ìœ„í—˜ í–‰ë™ ê°ì§€ ì›Œì»¤ (12 -> 14)
                frame_workers = 24  # í”„ë ˆì„ ì²˜ë¦¬ ì›Œì»¤ (20 -> 24, CPU ì½”ì–´ + GPU ë³‘ë ¬ ì²˜ë¦¬)
                logging.info(f"M4 Pro ê°ì§€ (20ì½”ì–´ GPU, 14ì½”ì–´ CPU) - ì„±ëŠ¥ ìµœëŒ€í™”: Face={face_workers}, YOLO={yolo_workers}, Danger={danger_workers}, Frame={frame_workers}")
            else:
                # ë‹¤ë¥¸ Apple Silicon (M1/M2/M3 ë“±) - ì¸ì› ì¦ê°€ ëŒ€ì‘
                face_workers = 8  # 6 -> 8 (ì¸ì› ì¦ê°€ ëŒ€ì‘)
                yolo_workers = 8  # 6 -> 8
                danger_workers = 6  # 4 -> 6
                frame_workers = 12  # 8 -> 12
                logging.info(f"Apple Silicon ê°ì§€ - ì›Œì»¤ ìˆ˜: Face={face_workers}, YOLO={yolo_workers}, Danger={danger_workers}, Frame={frame_workers}")
            return face_workers, yolo_workers, danger_workers, frame_workers
        except Exception as e:
            logging.warning(f"Mac ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            # ê¸°ë³¸ê°’ (M4 Pro ê°€ì •)
            return 8, 10, 6, 12
        return 8, 3, 2, 4
    else:
        # CPU
        return 2, 2, 2, 3

# í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ë™ì  ê³„ì‚°ìœ¼ë¡œ ì›Œì»¤ ìˆ˜ ê²°ì •
_DEFAULT_FACE_WORKERS, _DEFAULT_YOLO_WORKERS, _DEFAULT_DANGER_WORKERS, _DEFAULT_FRAME_WORKERS = _calculate_optimal_workers()

# ì ì‘í˜• ì›Œì»¤ ê´€ë¦¬ì ì´ˆê¸°í™”
try:
    from adaptive_worker_manager import initialize_adaptive_worker_manager
    adaptive_worker_manager = initialize_adaptive_worker_manager(
        initial_face_workers=_DEFAULT_FACE_WORKERS,
        initial_yolo_workers=_DEFAULT_YOLO_WORKERS,
        initial_danger_workers=_DEFAULT_DANGER_WORKERS,
        initial_frame_workers=_DEFAULT_FRAME_WORKERS
    )
    logging.info("âœ… ì ì‘í˜• ì›Œì»¤ ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ (GPU ì‚¬ìš©ë¥  ë° ì§€ì—° ì‹œê°„ ê¸°ë°˜ ìë™ ì¡°ì •)")
except Exception as e:
    logging.warning(f"âš ï¸ ì ì‘í˜• ì›Œì»¤ ê´€ë¦¬ì ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
    adaptive_worker_manager = None

# MongoDB ë°°ì¹˜ ì €ì¥ ì‹œìŠ¤í…œ (DB ë¶€í•˜ ê°ì†Œ)
violation_batch_queue = queue.Queue()  # ìœ„ë°˜ ì‚¬í•­ ë°°ì¹˜ í
violation_batch_lock = threading.Lock()  # ë°°ì¹˜ í ì ‘ê·¼ìš© ë½
VIOLATION_BATCH_SIZE = int(os.getenv('VIOLATION_BATCH_SIZE', '20'))  # ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ 20ê°œ, ì¦ê°€)
VIOLATION_BATCH_INTERVAL = float(os.getenv('VIOLATION_BATCH_INTERVAL', '10.0'))  # ë°°ì¹˜ ê°„ê²© (ì´ˆ, ê¸°ë³¸ 10ì´ˆ, 5ì´ˆì—ì„œ ì¦ê°€)
VIOLATION_MIN_INTERVAL = float(os.getenv('VIOLATION_MIN_INTERVAL', '30.0'))  # ê°™ì€ ìœ„ë°˜ ìµœì†Œ ì €ì¥ ê°„ê²© (ì´ˆ, ê¸°ë³¸ 30ì´ˆ, 10ì´ˆì—ì„œ ì¦ê°€)
violation_last_saved: Dict[str, float] = {}  # ë§ˆì§€ë§‰ ì €ì¥ ì‹œê°„ ì¶”ì  (key: f"{worker_id}_{violation_type}_{cam_id}")
image_last_saved: Dict[str, float] = {}  # ì´ë¯¸ì§€ ì €ì¥ ì‹œê°„ ì¶”ì  (key: f"{worker_id}_{violation_type}_{cam_id}")
IMAGE_SAVE_MIN_INTERVAL = 1.0  # ì´ë¯¸ì§€ ì €ì¥ ìµœì†Œ ê°„ê²© (ì´ˆ, 1ì´ˆ)

# ThreadPoolExecutor ìƒì„± (ë™ì  ì¡°ì • ê°€ëŠ¥í•˜ë„ë¡ í•¨ìˆ˜ë¡œ ë˜í•‘)
def _create_executors():
    """ì›Œì»¤ ìˆ˜ì— ë”°ë¼ ThreadPoolExecutor ìƒì„±/ì—…ë°ì´íŠ¸"""
    if adaptive_worker_manager:
        face_w, yolo_w, danger_w, frame_w = adaptive_worker_manager.get_current_workers()
    else:
        face_w = int(os.getenv("FACE_RECOGNITION_WORKERS", str(_DEFAULT_FACE_WORKERS)))
        yolo_w = int(os.getenv("YOLO_WORKERS", str(_DEFAULT_YOLO_WORKERS)))
        danger_w = int(os.getenv("DANGEROUS_BEHAVIOR_WORKERS", str(_DEFAULT_DANGER_WORKERS)))
        frame_w = int(os.getenv("FRAME_PROCESSING_WORKERS", str(_DEFAULT_FRAME_WORKERS)))
    
    return (
        ThreadPoolExecutor(max_workers=face_w, thread_name_prefix="face_recognition"),
        ThreadPoolExecutor(max_workers=yolo_w, thread_name_prefix="yolo_inference"),
        ThreadPoolExecutor(max_workers=danger_w, thread_name_prefix="dangerous_behavior"),
        ThreadPoolExecutor(max_workers=frame_w, thread_name_prefix="frame_processing")
    )

# ì´ˆê¸° Executor ìƒì„±
face_recognition_executor, yolo_executor, dangerous_behavior_executor, frame_processing_executor = _create_executors()

# Executor ì—…ë°ì´íŠ¸ìš© ë½
_executor_update_lock = threading.Lock()

def update_worker_executors():
    """ì ì‘í˜• ì›Œì»¤ ê´€ë¦¬ìì— ë”°ë¼ Executor ì—…ë°ì´íŠ¸ (ì•ˆì „í•œ êµì²´)"""
    global face_recognition_executor, yolo_executor, dangerous_behavior_executor, frame_processing_executor
    
    # ë½ì„ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ì—…ë°ì´íŠ¸ ë°©ì§€
    with _executor_update_lock:
        if adaptive_worker_manager:
            face_w, yolo_w, danger_w, frame_w = adaptive_worker_manager.adjust_workers()
            
            # ì›Œì»¤ ìˆ˜ê°€ ë³€ê²½ë˜ì—ˆìœ¼ë©´ ìƒˆë¡œìš´ Executor ìƒì„±
            if (face_recognition_executor._max_workers != face_w or
                yolo_executor._max_workers != yolo_w or
                dangerous_behavior_executor._max_workers != danger_w or
                frame_processing_executor._max_workers != frame_w):
                
                # ìƒˆë¡œìš´ Executor ë¨¼ì € ìƒì„± (ê¸°ì¡´ Executor ì¢…ë£Œ ì „ì— ìƒì„±)
                new_face_executor, new_yolo_executor, new_danger_executor, new_frame_executor = _create_executors()
                
                # ê¸°ì¡´ Executorë¥¼ ì„ì‹œ ë³€ìˆ˜ì— ì €ì¥ (ì°¸ì¡° ìœ ì§€)
                old_face_executor = face_recognition_executor
                old_yolo_executor = yolo_executor
                old_danger_executor = dangerous_behavior_executor
                old_frame_executor = frame_processing_executor
                
                # ìƒˆë¡œìš´ Executorë¡œ ì¦‰ì‹œ êµì²´ (ìƒˆ ì‘ì—…ì€ ìƒˆ Executorë¡œ)
                face_recognition_executor = new_face_executor
                yolo_executor = new_yolo_executor
                dangerous_behavior_executor = new_danger_executor
                frame_processing_executor = new_frame_executor
                
                logging.info(f"ğŸ”„ Executor ì—…ë°ì´íŠ¸ ì™„ë£Œ: Face={face_w}, YOLO={yolo_w}, Danger={danger_w}, Frame={frame_w}")
                
                # ê¸°ì¡´ ExecutorëŠ” ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ (ê¸°ì¡´ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°)
                def shutdown_old_executor(old_exec, name):
                    try:
                        old_exec.shutdown(wait=True, timeout=10.0)
                    except Exception as e:
                        logging.warning(f"âš ï¸ {name} Executor ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
                
                # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ê¸°ì¡´ Executor ì¢…ë£Œ (ë¸”ë¡œí‚¹ ë°©ì§€)
                import threading
                threading.Thread(target=shutdown_old_executor, args=(old_face_executor, "Face"), daemon=True).start()
                threading.Thread(target=shutdown_old_executor, args=(old_yolo_executor, "YOLO"), daemon=True).start()
                threading.Thread(target=shutdown_old_executor, args=(old_danger_executor, "Danger"), daemon=True).start()
                threading.Thread(target=shutdown_old_executor, args=(old_frame_executor, "Frame"), daemon=True).start()

